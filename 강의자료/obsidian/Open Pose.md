# 오픈포즈 
- 카네기 멜론 대학교 저 차오 등이 2017년 발표한 논문 "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields"를 기반으로 함.
- 이미지 처리의 자세 추정 딥러닝 모델 


# 개요 
- 자세 추정(pose estimation)은 이미지에 포함된 여러 인물을 탐지하여 인체 각 부위의 위치를 식별하고 부위를 연결하는 선을 구현하는 기술 
![[Pasted image 20250123002326.png]]

![[Pasted image 20250123002542.png]]

# MSCOCO
- 학습을 위해서 MSCOCO 데이터셋을 사용 
- 자세 추정 전용 데이터셋은 아니지만 이미지 데이터를 사용하는 딥러닝을 위한 일반적인 데이터셋 
- 이미지 데이터와 함께 영상 분류, 물체 감지, 시맨틱 분할, 자세 추정 정답이 들어있는 어노테이션 데이터가 포함되어 있음 
- 이 데이터에는 이미지가 무엇을 나타내는지 설명하는 캡션 어노테이션 데이터도 있기 때문에 이미지 캡션 데이터를 생성하는 딥러닝 데이터셋으로도 활용 
- 이 중에서 인체의 부위와 링크의 어노테이션이 부여된 데이터를 사용 
- 해당 데이터는 자세 추정용으로 사용되고 COCO Keypoint Detection Task(COCO 키포인트 인식 작업)에 사용 
![[Pasted image 20250123002850.png]]![[Pasted image 20250123002857.png]]
- 이미지의 주요 인문 정보(joint_self)와 그 외 인물 정보(joint_others)로 나누어 저장
- MSCOCO 데이터셋은 동일한 화상의 주요 인물이 다른 어노테이션인 경우가 있음 
- 해당 어노테이션의 num_keypoints 라는 키 값으로 이미지의 인물 인체 분위가 몇 개로 저장되어 있으며 최대 17개 
	- 목에 대한 어노테이션은 없음 
- `joint_self`: 목 제외 17개 부위 x, y 좌표, 시인성(0, 1, 2) 정보 포함. 0은 좌표는 있으나 이미지에 부위 없음, 1은 좌표와 부위 모두 이미![[09_객체탐지/attachments/Pasted image 20250122135915.png]]pr![[09_객체탐지/attachments/Pasted image 20250122140103.png]]인지 나타냄.
- `joint_others`: 이미지 내 다른 인물 부위 정보 저장.

# 자세 추정 흐름 
- 오픈포즈를 활용한 자세 추정 3단계 흐름 설명.
- 오픈포즈는 카네기 멜런 대학교 Zhe Cao 등이 2017년 발표 논문 구현.
- 2018년 12월 최신 버전 arXiv에 발표.
- 2017년 초기 버전과 변경점 존재하나, 알고리즘 본질은 동일.
- 자세 추정은 시맨틱 분할과 유사.
- 시맨틱 분할은 픽셀 수준 물체 라벨 추정.
- 자세 추정도 왼쪽 팔꿈치, 손목 등 클래스 사용.

- 시맨틱 분할은 픽셀별 클래스 분류, 오픈포즈 자세 추정은 픽셀별 회귀 문제.
- 회귀 대상은 각 픽셀이 19개(신체 18개+기타) 물체일 확률.
- 예시로, 왼쪽 팔꿈치 확률이 가장 높은 픽셀을 해당 부위 좌표로 사용.
- 여러 사람 존재 시 신체 부위 연결 문제 발생 (링크 쌍 문제).
- 하향식(single-person) 및 상향식(multi-person) 접근 존재.
- 하향식은 물체 감지로 한 사람씩 잘라내어 자세 추정, 연결 문제 방지. 단점은 처리 시간 증가, 물체 검출 정밀도에 영향.
- 오픈포즈는 상향식 접근, PAFs(Part Affinity Fields) 개념으로 '부위 간 연결성'을 나타내 링크 쌍 문제 해결.
![[Pasted image 20250123003903.png]]
- 원본 이미지(왼쪽 상단)에서 타자와 포수 자세 추정.  그림 4-5는 추론 결과.
- 그림 4-5에서 픽셀별 회귀로 왼쪽 팔꿈치, 손목 픽셀 확률 구함(오른쪽 상단, 왼쪽 하단). 붉은 점은 해당 부위 좌표. 한 번 추론으로 모든 사람의 해당 부위 추정. 먼 곳 벤치 인물은 작아서 감지 안됨.
- 두 개![[09_객체탐지/attachments/Pasted image 20250122140429.png]]목 사이 픽셀' 클래스 사용. 해당 클래스 확률 픽셀별 계산. 이 클래스가 PAFs(오른쪽 하단). PAFs와 위치 정보로 연결 쌍 결정.
![[Pasted image 20250123004029.png]]
- 모든 부위, 링크에 적용해 다수 인원 자세 추정.
- 그림 4-6은 3단계 과정.
- 1단계: 전처리. 이미지 크기 368x368로 조정, 색상 표준화.
- 2단계: 신경망에 전처리 이미지 입력. 출력은 19x368x368(신체 부위), 38x368x368(PAFs) 배열. 19개 신체 부위(18개+기타), 38개 PAFs(19개 링크 x,y). 값은 픽셀별 신뢰도.
- 3단계: 부위별 좌표, PAFs로 링크. 이미지 크기 복원.
---
OpenPose에서 PAFs(Part Affinity Fields)는 **신체의 각 부분(관절) 간의 연결 관계를 표현하는 2D 벡터 필드**입니다. PAFs는 OpenPose의 핵심 구성 요소 중 하나로, **멀티-퍼슨 환경에서 각 사람의 관절을 정확하게 연결하여 골격(skeleton)을 구성하는 데 중요한 역할**을 합니다.
**1. PAFs의 역할:**
- **관절 간 연결![[09_객체탐지/attachments/Pasted image 20250122152120.png]]팔꿈치-손목, 어깨-팔꿈치) 간의 위치와 방향 정보를 인코딩**합니다.
- **멀티 퍼슨 골격 구성:** OpenPose는 이미지에서 여러 사람의 관절(키포인트)을 검출한 후, PAFs 정보를 이용하여 **동일한 사람에게 속하는 관절들을 연결**하여 각 사람의 골격을 구성합니다.

**2. PAFs의 원리:**
- **벡터 필드 표현:** PAFs는 이미지의 각 픽셀에 대해 **2D 벡터**를 할당합니다. 이 벡터는 해당 픽셀이 **특정 관절 쌍을 연결하는 선분(limb)의 일부일 경우, 그 방향과 크기를 나타냅니다.**
    - **방향:** 한 관절에서 다른 관절로 향하는 방향을 나타냅니다. (예: 팔꿈치에서 손목 방향)
    - **크기:** 해당 픽셀이 관절 쌍을 연결하는 선분의 일부일 가능성에 대한 신뢰도(confidence)를 나타냅니다.

# 데이터셋 
- 마스크 데이터 이해.
- 오픈포즈 Dataset 및 DataLoader 클래스 구현.
- 오픈포즈 전처리 및 데이터 확장 처리 이해.
## 마스크 데이터
- 이미지에 사람이 존재하나 자세 정보 어노테이션이 훈련 및 검증 데이터에 없는 경우 존재.
- 작은 크기, 어노테이션 누락 등 여러 이유 존재.
- 이미지 내 어노테이션 없는 인물은 불완전한 어노테이션 상태로 자세 추정 학습에 악영향.
- 악영향 방지 위해, 이미지 내 자세 어노테이션 없는 인물은 검은 마스크 적용.
- 이 데이터를 마스크 데이터라 지칭.
- 오픈포즈 학습은 손실 함수 계산 시 마스크 픽셀 무시.

## 데이터 증강
- `aug_scale` 클래스로 이미지 크기 0.5~1.1배 확대 및 축소.
- `aug_rotate` 클래스로 -40~40도 무작위 회전.
- `aug_croppad` 클래스로 `joint_self` 중심 364 픽셀 크기로 이미지 자름.
- `aug_flip` 클래스로 50% 확률 좌우 반전.
- `remove_illegal_joint` 클래스로 잘린 이미지 내 신체 부위 유무에 따라 어노테이션 수정. (`joint_self` 또는 `joint_others` 중 일부가 잘린 이미지에 없을 경우, 좌표 정보 '시인성'을 '이미지에 없고 어노테이션도 없음'으로 변경)
- `Normalize_Tensor` 클래스로 색상 정보 표준화, 넘파이 Array를 파이토치 텐서로 변환 (이번엔 확인 용이하도록 `no_Normalize_Tensor` 클래스 실행, 색상 표준화 생략).
```python
self.data_transform = {
		'train': Compose([
			get_anno(),  # JSON에서 어노테이션을 사전에 저장
			add_neck(),  # 어노테이션 데이터의 순서를 변경하고, 
							# 목의 어노테이션 데이터를 추가
			aug_scale(),  # 확대 축소
			aug_rotate(),  # 회전
			aug_croppad(),  # 자르기
			aug_flip(),  # 좌우 반전
			remove_illegal_joint(),  # 화상에서 밀려나온 어노테이션을 제거
			# Normalize_Tensor()  # 색상 정보의 표준화 및 텐서화
			no_Normalize_Tensor()  # 여기서는 색상 정보의 표준화를 생략
		]),
		'val': Compose([
			# 검증을 생략
		])
	}
```

## 어노테이션 작성 
- MSCOCO 어노테이션 기반으로 오픈포즈 학습 및 검증용 어노테이션 설명.
- MSCOCO는 신체 부위 정보를 픽셀 좌표로 표시, 1픽셀 오차도 허용 안됨.
- 어노테이션 담당자에 따라 동일 이미지라도 좌표 달라짐. 작은 오차 허용해야 함.
- MSCOCO 픽셀 좌표 중심으로 가우스 분포 적용, 주변 픽셀도 해당 부위 확률 높임. 이것이 히트맵.
- PAFs 어노테이션 작성. PAFs는 오픈포즈 고유 개념, MSCOCO에 없음. 기본적으로 부위 간 직선 상 픽셀은 1, 외는 0, 직사각형 모양. 실제로는 흐릿한 직사각형.

## 데이터셋 작성 
- 마스크 데이터는 RGB (255, 255, 255) 또는 (0, 0, 0)으로 표현.
- 마스크 부분은 (0, 0, 0).
- RGB 3차원 정보를 1차원으로 변환.
- 무시하고 싶은 부분은 0, 그렇지 않은 부분은 1.
- 마지막 차원에 있는 마스크 데이터 채널을 맨 앞으로 이동.

# 모델 구현 
## 구성 모듈 
- 오픈포즈는 7개 모듈 구성.
- Feature 모듈은 이미지 특징량 추출.
- Stage 모듈은 히트맵, PAFs 출력.
- Stage 모듈은 스테이지 1~6, 총 6개.

![[Pasted image 20250123010040.png]]
- 전처리된 이미지 -> Feature 모듈 -> 128채널 특징량 변환.
- VGG-19 사용, 출력 크기 1/8 (128x46x46).
- Feature 모듈 출력은 스테이지 1, 2~6로 전달.
- 스테이지 1은 두 서브 네트워크(블록 1_1, 1_2) 사용.
- 블록 1_1은 PAFs(38x46x46), 블록 1_2는 히트맵(19x46x46) 출력.
- 간단 추정은 블록 1_1, 1_2 출력 사용, 정밀도는 낮음.
- 더 정밀한 PAFs, 히트맵 위해 스테이지 1의 PAFs, 히트맵, Feature 모듈 출력 사용.
- 채널 방향 결합, 185x46x46 텐서 출력. (185 = 38 + 19 + 128)
- 스테이지 1 출력(185x46x46)은 스테이지 2의 블록 2_1, 2_2 입력.
- 블록 2_1, 2_2는 PAFs, 히트맵, Feature 모듈 출력 입력으로 사용, PAFs(38x46x46), 히트맵(19x46x46) 출력.
- 스테이지 2 출력은 스테이지 1보다 정밀.
- 스테이지 6까지 반복. 입력은 스테이지 5 PAFs, 히트맵, Feature 모듈 출력.
- 결합하여 185x46x46 텐서로 스테이지 6 입력.
- 블록 6_1은 PAFs, 6_2는 히트맵 출력.
- 스테이지 6 출력 PAFs, 히트맵으로 최종 자세 추정.
- 총 7개 모듈(Feature, 스테이지 1~6)로 오픈포즈 구성.
- 네, 알겠습니다. 전처리된 이미지 -> Feature 모듈 -> 128채널 특징량 변환되는 것부터, 스테이지 6에서 최종 자세 추정까지 오픈포즈 구조를 단계별로 자세히 설명해 드리겠습니다.
--- 
**1단계: Feature 모듈 - 이미지에서 특징 추출**
- **입력**: 전처리된 이미지 (크기: H x W x 3, H는 높이, W는 너비, 3은 RGB 채널)
- **역할**: 이미지에서 자세 추정에 필요한 핵심 특징 128채널을 추출합니다.
- **네트워크 구성**: VGG-19 기반, 출력 크기가 입력 이미지의 1/8 (46x46) 이 되도록 조정.
- **출력**: 128 x 46 x 46 크기의 특징 텐서. (128은 채널 수, 46 x 46은 특징 맵 크기)
- **추가 설명**:
    - VGG-19는 이미지 분류에 널리 사용되는 딥러닝 모델입니다.
    - 출력 크기가 1/8로 줄어든다는 것은 계산량을 줄이고, 큰 크기의 특징을 잡아내기 위함입니다.
    - 이 특징 텐서는 이미지 내의 각 부분이 어떤 신체 부위에 해당하는지, 그리고 그 부분들이 어떻게 연결되는지에 대한 정보를 담고 있습니다.

**2단계: 스테이지 1 - 초기 PAFs 및 히트맵 생성**
- **입력**: Feature 모듈 출력 (128 x 46 x 46)
- **역할**:
    - **블록 1_1**: 신체 부위 간 연결 관계를 나타내는 PAFs (Part Affinity Fields)를 추정합니다.
    - **블록 1_2**: 각 신체 부위의 위치를 나타내는 히트맵을 추정합니다.
- **네트워크 구성**: 두 개의 서브 네트워크 (블록 1_1, 블록 1_2)로 구성.
- **출력**:
    - 블록 1_1: 38 x 46 x 46 크기의 PAFs 텐서 (38 = 19개 링크 x 2 (x, y 방향))
    - 블록 1_2: 19 x 46 x 46 크기의 히트맵 텐서 (19 = 신체 부위 개수)
- **설명**:
    - 이 단계에서는 Feature 모듈에서 추출된 특징을 바탕으로 PAFs와 히트맵을 처음으로 생성합니다.
    - PAFs는 각 신체 부위 쌍 (예: 팔꿈치-손목) 간의 연결 강도와 방향을 나타내는 벡터 필드입니다.
    - 히트맵은 각 신체 부위 (예: 팔꿈치, 손목)가 이미지 내 어디에 위치하는지에 대한 확률 분포를 나타냅니다.
    - 이 단계의 PAFs와 히트맵은 아직 정밀도가 낮습니다.

**3단계: 스테이지 2 ~ 6 - PAFs 및 히트맵 정제**
- **입력**: 이전 스테이지 PAFs 및 히트맵, Feature 모듈 출력 (총 185 = 38 + 19 + 128 채널)
- **역할**: 이전 스테이지에서 생성된 PAFs와 히트맵을 더욱 정밀하게 개선합니다.
- **네트워크 구성**: 스테이지 2부터 6까지는 동일한 구조 (블록 n_1, 블록 n_2)가 반복됩니다.
- **출력**:
    - 블록 n_1: 38 x 46 x 46 크기의 PAFs 텐서
    - 블록 n_2: 19 x 46 x 46 크기의 히트맵 텐서
- **설명**:
    - 각 스테이지는 이전 스테이지에서 생성된 PAFs와 히트맵, 그리고 Feature 모듈에서 추출된 특징 텐서를 입력으로 받습니다.
    - 이전 스테이지의 PAFs와 히트맵은 현재 스테이지에서 더 정밀한 PAFs와 히트맵을 생성하기 위한 추가적인 문맥 정보를 제공합니다.
    - 스테이지를 반복할수록 PAFs와 히트맵은 점점 더 정밀해집니다.
    - 스테이지 6의 출력이 최종 PAFs와 히트맵으로 사용됩니다.

**4단계: 최종 자세 추정**

- **입력**: 스테이지 6 출력 (PAFs: 38 x 46 x 46, 히트맵: 19 x 46 x 46)
- **역할**: PAFs와 히트맵을 사용하여 이미지 내 사람들의 자세를 추정합니다.
- **과정**:
    1. 히트맵에서 각 신체 부위에 해당하는 픽셀(지역 최대값)을 찾습니다.
    2. PAFs를 따라 신체 부위들을 연결하여 골격을 구성합니다.
    3. 연결된 골격을 바탕으로 각 사람의 자세를 추정합니다.
- **출력**: 이미지 내 각 사람의 자세 정보 (각 신체 부위 좌표)

**요약**:

- 오픈포즈는 **7개의 모듈 (Feature 모듈 1개, 스테이지 모듈 6개)** 로 구성됩니다.
- **Feature 모듈**은 이미지에서 특징을 추출합니다.
- **스테이지 모듈**은 PAFs와 히트맵을 생성하고 정제합니다.
- **스테이지 1**은 초기 PAFs와 히트맵을 생성합니다.
- **스테이지 2~6**은 PAFs와 히트맵을 반복적으로 정제합니다.
- **마지막 스테이지(6) 출력**으로 최종 자세를 추정합니다.

```python
# Feature 모듈
self.model0 = OpenPose_Feature()

# Stage 모듈
# PAFs(Part Affinity Fields) 측
self.model1_1 = make_OpenPose_block('block1_1')
self.model2_1 = make_OpenPose_block('block2_1')
self.model3_1 = make_OpenPose_block('block3_1')
self.model4_1 = make_OpenPose_block('block4_1')
self.model5_1 = make_OpenPose_block('block5_1')
self.model6_1 = make_OpenPose_block('block6_1')

# confidence heatmap 측
self.model1_2 = make_OpenPose_block('block1_2')
self.model2_2 = make_OpenPose_block('block2_2')
self.model3_2 = make_OpenPose_block('block3_2')
self.model4_2 = make_OpenPose_block('block4_2')
self.model5_2 = make_OpenPose_block('block5_2')
self.model6_2 = make_OpenPose_block('block6_2')
```

## Feature 및  Stage
### Feature
- 오픈포즈의 Feature 모듈은 VGG-19를 활용 
![[Pasted image 20250123010851.png]]
- VGG-19의 10번째 합성곱 층까지 사용.
- ReLU, 최대 풀링 포함 0~22번째 층에 대응.
- 이후 합성곱 층 2개 + ReLU 추가.
- 출력 이미지는 입력의 1/8 크기, 46 픽셀(368/8).
- 출력 텐서 크기 128x46x46 (채널x높이x너비).
```python
class OpenPose_Feature(nn.Module):
    def __init__(self):
        super(OpenPose_Feature, self).__init__()

        # VGG-19의 최초 10개의 합성곱을 사용
        vgg19 = torchvision.models.vgg19(pretrained=True)
        model = {}
        model['block0'] = vgg19.features[0:23]  
        # VGG-19의 최초 10개의 합성곱 층까지

        # 나머지는 새로운 합성곱 층을 2개 준비
        model['block0'].add_module("23", torch.nn.Conv2d(
            512, 256, kernel_size=3, stride=1, padding=1))
        model['block0'].add_module("24", torch.nn.ReLU(inplace=True))
        model['block0'].add_module("25", torch.nn.Conv2d(
            256, 128, kernel_size=3, stride=1, padding=1))
        model['block0'].add_module("26", torch.nn.ReLU(inplace=True))

        self.model = model['block0']

    def forward(self, x):
        outputs = self.model(x)
        return outputs
```

### stage 
- 스테이지 1~6의 PAFs 및 히트맵을 출력하는 서브 네트워크 구성 및 구현 
![[Pasted image 20250123011140.png]]
- 스테이지 1은 Feature 모듈에서 128x46x46 텐서 입력.
- 스테이지 2~6은 Feature 모듈 출력 + 이전 스테이지 PAFs 및 히트맵, 185x46x46 텐서 입력.
- 각 스테이지 블록은 합성곱 층 + ReLU.
- 블록 1은 PAFs, 블록 2는 히트맵 출력.
- 블록 1(38), 블록 2(19) 출력 채널 수 제외 구성 동일.
- 스테이지 1, 스테이지 2~6은 합성곱 층 수, 종류 다름.
- `nn.Sequential()` 클래스로 네트워크 구현, `make_OpenPose_block` 함수 사용.
- `make_OpenPose_block` 함수 역할:

1. 구성(`configuration`) 설정: 딕셔너리, 합성곱 층 리스트. 전체 스테이지 및 블록 구성, `block_name` 설정 사용.
2. 합성곱 층, ReLU 생성, `layers` 리스트에 저장.
3. `layers` 정보로 `nn.Sequential()` 네트워크(`net`) 생성.
4. `net`의 합성곱 층 가중치 초기화.

이 과정을 통해 블록 생성.

```python
def make_OpenPose_block(block_name):
    """
    구성 변수에서 OpenPose의 Stage모듈의 block을 작성
    nn.Module이 아니라, nn.Sequential로 한다
    """

    # 1. 구성의 사전형 변수 blocks을 작성하여, 네트워크를 생성시킨다
    # 먼저 전 패턴의 사전을 준비하여, block_name 인수만을 생성한다
    blocks = {}
    # Stage 1
    blocks['block1_1'] = [{'conv5_1_CPM_L1': [128, 128, 3, 1, 1]},
                          {'conv5_2_CPM_L1': [128, 128, 3, 1, 1]},
                          {'conv5_3_CPM_L1': [128, 128, 3, 1, 1]},
                          {'conv5_4_CPM_L1': [128, 512, 1, 1, 0]},
                          {'conv5_5_CPM_L1': [512, 38, 1, 1, 0]}]

    blocks['block1_2'] = [{'conv5_1_CPM_L2': [128, 128, 3, 1, 1]},
                          {'conv5_2_CPM_L2': [128, 128, 3, 1, 1]},
                          {'conv5_3_CPM_L2': [128, 128, 3, 1, 1]},
                          {'conv5_4_CPM_L2': [128, 512, 1, 1, 0]},
                          {'conv5_5_CPM_L2': [512, 19, 1, 1, 0]}]

    # Stages 2 - 6
    for i in range(2, 7):
        blocks['block%d_1' % i] = [
            {'Mconv1_stage%d_L1' % i: [185, 128, 7, 1, 3]},
            {'Mconv2_stage%d_L1' % i: [128, 128, 7, 1, 3]},
            {'Mconv3_stage%d_L1' % i: [128, 128, 7, 1, 3]},
            {'Mconv4_stage%d_L1' % i: [128, 128, 7, 1, 3]},
            {'Mconv5_stage%d_L1' % i: [128, 128, 7, 1, 3]},
            {'Mconv6_stage%d_L1' % i: [128, 128, 1, 1, 0]},
            {'Mconv7_stage%d_L1' % i: [128, 38, 1, 1, 0]}
        ]

        blocks['block%d_2' % i] = [
            {'Mconv1_stage%d_L2' % i: [185, 128, 7, 1, 3]},
            {'Mconv2_stage%d_L2' % i: [128, 128, 7, 1, 3]},
            {'Mconv3_stage%d_L2' % i: [128, 128, 7, 1, 3]},
            {'Mconv4_stage%d_L2' % i: [128, 128, 7, 1, 3]},
            {'Mconv5_stage%d_L2' % i: [128, 128, 7, 1, 3]},
            {'Mconv6_stage%d_L2' % i: [128, 128, 1, 1, 0]},
            {'Mconv7_stage%d_L2' % i: [128, 19, 1, 1, 0]}
        ]

    # block_name 인수의 구성 사전을 꺼낸다
    cfg_dict = blocks[block_name]

    # 구성 내용을 리스트 변수 layers에 저장
    layers = []

    # 0번째부터 최후의 층까지 작성
    for i in range(len(cfg_dict)):
        for k, v in cfg_dict[i].items():
            if 'pool' in k:
                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1],
                                        padding=v[2])]
            else:
                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],
                                   kernel_size=v[2], stride=v[3],
                                   padding=v[4])
                layers += [conv2d, nn.ReLU(inplace=True)]

    # 3. layers를 Sequential로 한다
    # 단, 최후에 ReLU는 필요 없으므로 직전까지를 사용한다
    net = nn.Sequential(*layers[:-1])

    # 4. 초기화 함수를 설정하여, 합성곱 층을 초기화한다
    def _initialize_weights_norm(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.normal_(m.weight, std=0.01)
                if m.bias is not None:
                    init.constant_(m.bias, 0.0)

    net.apply(_initialize_weights_norm)

    return net

```

# 텐서보드X
- 텐서보드X는 딥러닝 패키지 텐서플로의 시각화 툴 텐서보드를 파이토치에서 사용하게 해주는 서드파티 패키지.
- 파이토치 모델을 ONNX 형식으로 변환 후 텐서보드에 넣음.
- 모든 파이토치 함수가 정확히 작동하지는 않지만, 모델 내 텐서 크기 변화를 파악하는데 유용.
- 텐서보드X를 사용하려면 텐서플로, 텐서보드X 설치 필요.

# Training 
## Loss function 
- 오픈포즈 손실함수는 히트맵, PAFs와 정답 어노테이션 데이터 사이 회귀 오차.
- 각 픽셀 값과 정답 데이터 값 차이 계산, 픽셀별 회귀.
- 시맨틱 분할은 픽셀별 클래스 분류.
- 오픈포즈는 픽셀별 값(예: 왼쪽 팔꿈치 정도) 회귀, 손실함수 다름.
- 평균 제곱 오차 함수(F.mse_loss()) 사용.
- 여섯 개 스테이지, 각 스테이지 출력별(히트맵, PAFs) 정답 데이터 오차 계산.
- 전체 네트워크 오차는 모든 스테이지 히트맵, PAFs 오차 합.
- 자세 어노테이션 없는 인물 부분은 손실 계산 제외.
- 어노테이션(히트맵, PAFs), 스테이지 추정(히트맵, PAFs)에 마스크(무시 0, 아니면 1) 곱함.
----
**1. 회귀 문제로서의 손실 함수**

- **분류 vs. 회귀**:  시맨틱 분할은 각 픽셀이 어떤 클래스(예: 고양이, 강아지, 사람)에 속하는지 분류하는 문제였습니다. 반면, 오픈포즈는 각 픽셀이 특정 신체 부위(예: 왼쪽 팔꿈치)일 확률, 또는 특정 신체 부위 간 연결(PAF) 강도를 나타내는 **연속적인 값**을 예측하는 **회귀(Regression)** 문제입니다.
- **픽셀별 회귀**: 오픈포즈는 각 픽셀에 대해 **두 가지 값**을 예측합니다.
    - **히트맵**: 각 신체 부위(예: 왼쪽 팔꿈치, 오른쪽 어깨 등)에 대한 히트맵은 해당 픽셀이 그 신체 부위일 확률을 0과 1 사이의 값으로 나타냅니다. (값이 클수록 해당 신체 부위일 확률 높음)
    - **PAFs**: 각 신체 부위 쌍(예: 왼쪽 팔꿈치-왼쪽 손목)에 대한 PAFs는 해당 픽셀이 그 부위들 사이를 연결하는 선의 일부일 확률 및 방향을 나타냅니다. (두 부위를 잇는 벡터 필드)
- **손실 함수의 역할**: 손실 함수는 예측된 히트맵 및 PAFs 값이 정답(Ground Truth) 어노테이션 데이터와 얼마나 차이가 나는지 측정합니다. 즉, 모델이 예측한 픽셀별 값들이 실제 값과 얼마나 가까운지 평가하는 척도입니다.

**2. 평균 제곱 오차 (Mean Squared Error, MSE) 사용**

- **MSE 정의**: MSE는 회귀 문제에서 널리 사용되는 손실 함수. 예측 값과 실제 값의 차이를 제곱하여 평균을 낸 값
    
    ```
    MSE = (1/N) * Σ (y_pred - y_true)^2
    ```
    
    - `N`: 데이터 개수 (여기서는 픽셀 개수)
    - `y_pred`: 예측 값 (여기서는 예측된 히트맵 또는 PAFs 값)
    - `y_true`: 실제 값 (여기서는 정답 어노테이션 히트맵 또는 PAFs 값)
- **오픈포즈에서의 MSE**: 오픈포즈는 `torch.nn.functional.mse_loss()` 함수를 사용하여 MSE를 계산합니다. 각 스테이지에서 생성된 히트맵과 PAFs에 대해 MSE를 계산하여, 이를 통해 모델의 예측 성능을 평가합니다.
    
- **왜 MSE를 사용하는가?**:
    
    - **미분 가능**: MSE는 미분 가능한 함수이기 때문에 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘을 사용하여 모델을 학습시킬 수 있습니다.
    - **직관적인 해석**: 오차의 제곱을 사용하기 때문에, 오차가 클수록 손실 값이 기하급수적으로 커집니다. 이를 통해 모델이 큰 오차를 더 잘 감지하고 수정하도록 유도할 수 있습니다.
    - **널리 사용됨**: 회귀 문제에서 널리 사용되는 표준적인 손실 함수입니다.

**3. 마스킹(Masking) 기법**

- **문제 상황**: 이미지에 사람이 있지만, 자세 어노테이션이 없는 경우가 존재합니다. (예: 너무 작게 찍힌 사람, 일부 신체 부위가 가려진 사람)
- **마스킹의 필요성**: 어노테이션이 없는 부분에 대해서는 손실을 계산하지 않아야 합니다. 만약 어노테이션이 없는 부분을 무시하지 않고 손실을 계산하면, 모델은 잘못된 정보를 학습하게 되어 성능이 저하될 수 있습니다.
- **마스킹 방법**: 마스크는 어노테이션이 없는 부분을 0으로, 있는 부분을 1로 표시한 이미지입니다. 손실을 계산할 때, 예측 값(히트맵, PAFs)과 정답 값(어노테이션)에 마스크를 곱하여, 어노테이션이 없는 부분은 손실 계산에서 제외합니다.
- **구현**: 오픈포즈에서는 손실을 계산하기 전에, 각 스테이지의 예측값 (히트맵, PAFs)과 정답 어노테이션(히트맵, PAFs)에 마스크를 곱합니다.

**4. 전체 손실 계산**

- **스테이지별 손실**: 오픈포즈는 6개의 스테이지로 구성되며, 각 스테이지에서 히트맵과 PAFs를 예측합니다. 각 스테이지의 예측 결과에 대해 MSE 손실을 계산합니다.
- **최종 손실**: 전체 네트워크의 손실은 각 스테이지의 히트맵 손실과 PAFs 손실을 모두 합한 값입니다.

**결론** 

1. **픽셀별 회귀**: 각 픽셀에 대해 히트맵과 PAFs 값을 예측하는 회귀 문제입니다.
2. **평균 제곱 오차(MSE)**: 예측 값과 실제 값의 차이를 제곱하여 평균을 낸 MSE를 손실 함수로 사용합니다.
3. **마스킹**: 어노테이션이 없는 부분은 손실 계산에서 제외하기 위해 마스킹 기법을 사용합니다.
4. **다중 스테이지 손실**: 6개 스테이지 각각에서 손실을 계산하고, 이를 모두 합하여 전체 손실을 구합니다.


```python
# 손실함수 설정
class OpenPoseLoss(nn.Module):
    """OpenPose의 손실함수 클래스"""

    def __init__(self):
        super(OpenPoseLoss, self).__init__()

    def forward(self, saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask):
        """
        손실함수 계산.

        Parameters
        ----------
        saved_for_loss : OpenPoseNet의 출력(리스트)

        heatmap_target : [num_batch, 19, 46, 46]
            정답 부위의 어노테이션 정보

        heatmap_mask : [num_batch, 19, 46, 46]
            heatmap 화상의 mask

        paf_target : [num_batch, 38, 46, 46]
            정답 PAF의 어노테이션 정보

        paf_mask : [num_batch, 38, 46, 46]
            PAF 화상의 mask

        Returns
        -------
        loss : 텐서
            손실값
        """

        total_loss = 0
        # 스테이지마다 계산합니다
        for j in range(6):

            # PAFs 및 heatmaps에서 마스크된 부분(paf_mask=0 등)은 무시
            # PAFs
            pred1 = saved_for_loss[2 * j] * paf_mask
            gt1 = paf_target.float() * paf_mask

            # heatmaps
            pred2 = saved_for_loss[2 * j + 1] * heat_mask
            gt2 = heatmap_target.float()*heat_mask

            total_loss += F.mse_loss(pred1, gt1, reduction='mean') + \
                F.mse_loss(pred2, gt2, reduction='mean')

        return total_loss

criterion = OpenPoseLoss()
```