# GAN
- Generative Adversarial Network 
- 2014년 이안 굿펠로우(Ian Goodfellow)와 동료들이 신경망을 사용해 새로운 이미지를 합성하는 방법으로 발표
- 생산적 적대 신경망을 사용하면 컴퓨터가 새로운 데이터를 생성할 수 있음 
- 처음 논문으로 나왔을 때 딥러닝 분야의 가장 중요한 혁신으로 간주될 정도로 이슈였음 

# 아이디어 
- GAN의 주요한 목적은 훈련 데이터셋과 동일한 분포를 가진 새로운 데이터를 합성하는 것 
- 그렇기 때문에 GAN이 원본 형태는 정답 데이터가 필요하지 않음 (비지도 학습)

# 활용
- Image to Image translation
	- 입력 이미지에서 출력 이미지로 매핑하는 방법
- Image super-resolution
	- 낮은 해상도의 이미지를 높은 해상도의 이미지로 변환 
- image inpainting
	- 이미지에서 누락된 부분을 재구성하는 방법을 학습 


# Autoencoder
- 훈련 데이터를 압축하고 해제할 수 있는 Autoencoder 구조 
- 오토인코더는 새로운 데이터를 생성할 수 없지만 GAN의 아이디어를 이해하는데 도움이 됨 

## 구조 

![[Pasted image 20250117134636.png]]
### 인코더 
- encoder 신경망은 샘플 x에 연관된 d차원의 입력 특성 벡터를 받아 p차원의 벡터 z로 인코딩 
- 인코딩 된 z 벡터는 잠재 벡터(latent vector) 또는 잠재 특성 표현이라고 부름 
- 일반적으로 잠재 벡터의 차원은 입력 샘플의 차원보다 작다. (p<d)
- 그렇기 때무에 인코더가 데이터 압축하는 기능을 한다고 이야기할 수 있음 
### 디코더 
- Decode는 저차원 잠재 벡터 z에서 $\hat{x}$ 를 압축 해제했다고 할 수 있음 
- 디코더를 함수 $\hat{x} = g(z)$ 로 표현

# 생성 모델 
- 오토인코더가 훈련 이후에 입력 x에 대해 저차원 공간의 압축된 버전에서 이 입력을 재구성할 수 있음 
- 따라서 압축된 표현을 변환하는 식으로 입력을 재구성하는 것을 넘어서 새로운 데이터를 생성할 수 없음 
- 생성 모델은 랜덤한 벡터 z에서 새로운 샘플 $\hat{x}$를 생성할 수 있음 
- 랜덤 벡터 z는 완벽하게 특징을 알고 있는 간단한 분포에서 만들어지기 때문에 쉽게 샘플링할 수 있음 


## KL-divergence
![[Pasted image 20250120090532.png]]



# 훈련 팁 
- GAN은 생성 모델링 분야의 커다란 혁신이지만 훈련이 어렵기로 유명함 
- GAN을 훈련할 때 자주 마주치게 되는 여러 가지 문제점을 살펴본다 
## 판별자 > 생성자 
- 판별자가 너무 강하면 손실 함수의 신호가 너무 약해져 생성자에서 의미 있는 향상을 도모할 수 없음 
- 최악의 시나리오에서는 아래 그림처럼 판별자가 진짜 이미지와 가짜 이미지를 구분하는 방법을 완벽하게 학습하고 그레디언트가 완전히 사라져 학습이 전혀 이루어지지 않게 됨 
![[Pasted image 20250120234537.png]]
### 해결책 
- 판별자에 있는 Dropout 층의 rate 매개변숫값을 증가시켜 네트워클르 통해 흐르는 정보의 양을 감소
- 판별자의 학습률을 줄임 
- 판별자의 합성곱 필터 수를 줄임 
- 판별자를 훈련할 때 레이블에 잡음을 추가함 
- 판별자를 훈련할 때 일부 이미지의 레이블을 무작위로 뒤집음 
## 생성자 > 판별자 
- 판별자가 강력하지 않으면 생성자가 거의 동일한 몇 개의 이미지로 판별자를 쉽게 속이는 방법을 찾음 
- 이를 모두붕괴(mode collapse)라고 부름 
- 판별자의 가중치를 업데이트하지 않고 몇 번의 배치를 하는 동안 생성자를 훈련한다고 가정해 보자 
- 생성자는 판별자를 항상 속이는 하나의 샘플(이를 mode라고 부름)을 찾으려는 경향이 있고 잠재 공간의 모든 포인트를 이 이미지에 매핑할 수 있음 
- 또한 손실 함수의 그레디언트가 0에 가까운 값으로 붕괴(collapse)하므로 이 상태로부터 벗어날 수 없게 됨 
![[Pasted image 20250120234930.png]]