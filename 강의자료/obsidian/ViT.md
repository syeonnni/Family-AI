# 개요 
- ViT(Vision Transformer)는 2020년 구글 논문에서 소개된 이미지 인식 딥러닝 모델. 이미지 자연어 처리 방식 분류 시도.
- ViT는 CNN 합성곱 계층 대신 트랜스포머 셀프 어텐션 적용. CNN은 지역 특징 추출, ViT는 셀프 어텐션으로 전체 이미지 한 번에 처리.
- ViT는 이미지 패치 순차 입력으로 2차원 이미지 특성 반영 한계. 이를 해결하기 위해 스윈 트랜스포머(Swin Transformer)와 CVT(Convolutional Vision Transformer) 제안.
- 
- 
- 스윈 트랜스포머는 로컬 윈도우(Local Window) 활용, 계층별 어텐션 패치 크기, 개수 다양화, 이미지 특징 학습.
- ViT와 비교 시, 셀프 어텐션을 로컬 윈도우 안에 대한 어텐션, 로컬 윈도우 간의 어텐션으로 수행,
- 이미지 특징 계층적 학습. 어텐션 함수에 상대적 위치 편향 반영, 위치 정보 포함.


- CvT는 기존 합성곱 연산 과정을 ViT에 적용한 모델.
- 저수준 특징(Low-level Feature)과 고수준 특징(High-level Feature)을 계층적으로 반영.
- 예: 사람 얼굴에서 저수준 특징은 눈, 코, 입, 고수준 특징은 전체 얼굴.
- 어텐션 연산 과정에서 쿼리(Q), 키(K), 값(V) 중 키와 값을 기존 특징 벡터보다 축소해 계산 복잡도 감소.


- ViT 계열 모델은 이미지 분류에 매우 효과적.
- 광범위한 다른 컴퓨터 비전 작업에도 적용 가능.

# ViT
- Vision Transoformers
- 자연어 처리 분야에서 트랜스포머 모델이 큰 성능 향상을 이뤄내면서 비전 분야의 모델에도 많은 영향을 미침 
- 이전 컴퓨터 비전 관련 연구는 합성곱 신경망에 트랜스포머 모델의 셀프 어텐션 모듈을 적용한 모델이 다수였지만, ViT는 트랜스포머 구조 자체를 컴퓨터비전 분야에 적용한 첫 번째 연구 
![[Pasted image 20250130230415.png]]
- 트랜스포머 모델의 구조를 그대로 사용하지만, 입력 데이터를 만드는 과정이 다름 
- ViT 모델은 이미지가 격자로 작은 단위의 이미지 패치로 나눠어 순차적으로 입력된다는 차이 
- 모델에 사용되는 입력 이미지 패치는 왼쪽에서 오른쪽, 위에서 아래로 표현된 시퀀셜 배열을 가정 


![[Pasted image 20250130230449.png]]
- 합성곱은 이미지 패치 중 일부만 선택하여 학습하며, 이를 통해 이미지 전체의 특징을 추출 
- 합성곱에서는 고양이의 왼쪽 눈의 특징을 추출하고자 한다면 합성곱 신경망은 이미지에서 해당 부분만 선택해 학습 
- ViT 임베딩은 이미지를 작은 패치로 나누고 각 패치 간의 상관관계를 학습 
- 이를 위해 셀프 어텐션 방법을 사용 
- 모든 이미지 패치가 서로에게 주는 영향을 고려해 이미지의 전체 특징을 추출 
- 그러므로 모든 이미지 패치가 학습에 관여하여 높은 수준의 이미지 표현을 제공할 수 있음 
- 좁은 수용 영역(Receptive Field, RF) 합성곱 신경망은 전체 이미지 정보 표현에 많은 계층 필요.
- 트랜스포머는 어텐션 거리 계산하여 오직 한 개 ViT 레이어로 전체 이미지 정보 표현.
- ViT는 픽셀 단위 처리 합성곱 모델과 달리 패치 단위 처리, 작은 모델로도 고성능.
- ViT는 입력 이미지 크기 고정, 크기 다르면 전처리 필요.
- 합성곱 신경망은 공간적 위치 정보 고려, ViT는 패치 간 상대적 위치만 고려, 이미지 변환에 취약.



## 구조 
- ViT 모델은 입력 이미지를 패치로 나누고, 벡터 형태로 변환하는 패치 임베딩과 패치 관계 학습하는 인코더 계층으로 구성.
- 패치 임베딩, 인코더 계층은 이미지 특징 추출, 분류, 회귀 등 작업에 맞는 출력값 변환.
![[Pasted image 20250130231541.png]]
## 패치 임베딩 
- 패치 임베딩(Patch Embedding)은 입력 이미지를 작은 패치로 분할하는 과정.
- 전처리로 이미지 크기 조정. 예: 640x480 -> 224x224(정방형).
- 전체 이미지를 패치 크기로 분할, 시퀀셜 배열 생성.
- 합성곱 신경망 계층 활용.
![[Pasted image 20250130231707.png]]
- kernel size와 stride를 설정해야 함 
- 위의 예제에서는 커널의 크기는 3, stride도 3으로 설정 
- 이미지의 가로 세로 크기가 9이므로 계산은 아래와 같다 

![[Pasted image 20250130233206.png]]
- 가로 패치3개, 세로 패치 3개로 총 9개의 패치가 생성 
- 분할된 이미지 배치들을 배열 형태로 나열할 때 왼쪽에서 오른쪽, 위에서 아래의 순서로 배열 
- 이 배열 가장 왼쪽에 분류 토큰(Special Classification Token, CLS)를 추가 
- 분류 토큰은 전체 이미지를 대표하는 벡터로 특정 문제를 예측하는 데 사용 
- 위치 임베딩(Position Embedding)을 사용하여 인접한 패치 간의 관계를 학습 
- 위치 임베딩은 패치의 위치 정보를 임베딩 벡터로 변환하고 기존 이미지 패치 벡터들과 더함 
- 마지막에 계층 정규화를 적용하면 패치 임베딩이 만들어짐 
- 이렇게 입력 이미지를 작은 패치로 나눠 처리하면 gpu 메모리의 한계를 극복하고 더 큰 이미지를 처리 
## 인코더 계층 
- ViT 모델은 이미지를 패치로 나누고 벡터로 변환, 인코더 계층에 입력.
- 다양한 쿼리, 키, 값 임베딩 관계 학습. (트랜스포머 멀티 헤드 어텐션과 동일)
- N개 인코더 레이어 반복 후, 마지막 레이어에서 분류 토큰(특수 패치) 특징 벡터 추출.
- 분류 토큰 벡터는 이미지 특징 대표, 이미지 분류 및 검색 문제 해결에 사용.
- 예: 고양이/비고양이 분류 시, ViT는 시퀀셜 산출물(Sequential Output) 대신 분류 토큰 벡터로 분류.
- 분류 토큰 벡터 -> 순방향 신경망(완전 연결 계층) -> 고양이/비고양이 분류, ViT 모델 학습 방식.



## 모델구현 

- 입력 데이터를 일정한 크기의 패치로 분할한 후 패치마다 특성을 추출하는 과정을 추가로 수행 
- 이를 위해 ViT 모델에는 패치 임베딩(Patch embeddings)이 추가 
- 패치 임베딩의 처리 방식을 확인하기 위해 배치 데이터 하나를 패치 임베딩 함수에 적용 



# Swin Transformer
- 스윈 트랜스포머(Swin Transformer)는 2021년 발표된 대규모 비전 인식 모델.
- 기존 트랜스포머는 고정 패치 크기로 인한 세밀한 예측 어려움, 2차 계산 복잡도로 인한 고해상도 이미지 처리 어려움.
- 스윈 트랜스포머는 계층적 특징 맵 구성, 1차 계산 복잡도 가짐.
- 이미지 분류, 객체 감지, 영상 분할 등 인식 작업에서 강력한 성능.
- 시프트 윈도우(Shifted Window) 기술로 입력 이미지를 패치로 분할, 패치를 윈도우로 구성, 윈도우 영역만 셀프 어텐션 계산.
- 패치 겹침, 계층적 어텐션 적용(패치 크기, 개수)으로 고해상도, 다양한 크기 객체 효율적 처리.
- 객체 탐지, 객체 분할 등 작업에서 백본 모델로 사용.

## 차이 
- **ViT와 스윈 트랜스포머 차이**
- **ViT**: 획일적 패치 크기 및 위치 제약, 다양한 이미지 크기 및 종횡비 데이터셋 적용 어려움. 패치 단위 입력으로 이미지 공간 정보 보존 어려움.
- 패치 수 증가는 학습 데이터 크기 증가, 계산량 증가, 대규모 데이터셋 학습 시 컴퓨팅 자원 소모 큼.
- **스윈 트랜스포머**: ViT의 한계 보완, **로컬 윈도(Local Window)** 활용, 물체 크기 및 해상도 계층적 학습. 고정 크기 작은 윈도로 입력 이미지 처리
- **시프트 윈도** 기술: 입력 이미지를 패치로 분할, 특징 맵 이동, 다음 계층에 사용, 패치 위치 유연.
- **장점**: ViT 대비 높은 정확도, 적은 매개변수, 넓은 범위 이미지 크기 및 종횡비.
![[Pasted image 20250131060112.png]]
- 회색 격자는 패치를 의미 
- 빨간색 격자는 로컬 윈도를 의미 
- Vit는 계층마다 전체 이미지 안에 어텐션이 수행되는 패치 크기와 개수가 동일 
- 스윈 트랜스포머는 로컬 윈도를 통해 계층마다 어텐션이 되는 패치의 크기와 개수를 계층적으로 다양하게 적용 
- 이러한 계층적 접근 방식은 로컬 윈도가 각 패치의 세부 정보에 집중하게 되고, 시프트 윈도를 통해 전역 특징을 확인할 수 있기 때문에 고해상도 이미지를 효율적으로 처리할 수 있음 
![[Pasted image 20250131060356.png]]
## 구조 
- 스윈 트랜스포머는 이미지 -> 패치 파티션 -> 선형 임베딩 -> 스윈 트랜스포머 블록 -> 패치 병합 순으로 반복 수행.
- 입력 이미지를 일정한 크기 패치로 분할, 각 패치에 선형 임베딩 수행.
- 각 패치는 고정 차원 벡터로 변환.
- 분할된 패치 기반, 스윈 트랜스포머 블록 구성. (어텐션 계층, 계층 정규화, 다층 퍼셉트론 등 포함, 패치 간 상호작용)
- 패치 병합, 전체 이미지 분류 수행. 패치 순차적 병합, 전체 정보 추출 방식.
![[Pasted image 20250131060554.png]]
- 각 스테이지는 서로 다른 해상도와 패치 크기를 가짐 
- 스테이지 1: 패치 -> 선형 임베딩 -> 스윈 트랜스포머 블록 전달.
- 스테이지 2, 3, 4: 패치 병합 -> 스윈 트랜스포머 블록 전달.
- 패치 병합: 이미지 텐서 `[C, H, W]` -> `[2C, H/2, W/2]` 저차원 임베딩, 매개변수 감소.
- 스테이지 1, 2, 3, 4 공통: 스윈 트랜스포머 블록 -> 윈도 격자 내/외부 관계 학습, 어텐션 모듈 2개.

### 패치 파티션 / 패티 병합
- 패치 파티션(Patch Partition)이란 입력 이미지를 작은 사각형 패치로 분할해 처리하는 방식 
- 분할된 패치는 트랜스포머 계층의 입력으로 사용되며, 입력 이미지의 공간 정보를 보존하고 트랜스포머 계층에 대한 계산 효율성을 높임 
![[Pasted image 20250131060842.png]]
- 일반적 이미지 텐서 구조는 `[C, H, W]` (채널, 높이, 너비).
- 패치 파티션으로 로컬 윈도 추가. (위 그림은 `M`은 로컬 윈도 크기)
- 이미지 텐서 차원이 `[3, 32, 32]`라면, 패치 가로($P_h$), 세로($P_w$)는 4.
- **패치 병합(Patch Merging)**: 인접 패치 정보 저차원 축소.
- 모델 매개변수 감소 위해 `[C, H, W]` -> `[2C, H/2, W/2]` 재정렬, `2C` 차원 저차원 임베딩.
- 예: `[3, 8, 8]` 텐서 -> `[2C, H/2, W/2]`에 따라 `[6, 4, 4]`로 재정렬 -> 3차원 임베딩, `[3, 4, 4]` 텐서 생성.
### 스윈 트랜스포머 블록 
- 스윈 트랜스포머 블록: 패치 파티션 후 4개 스테이지에서 반복.
- 선형 임베딩 또는 패치 병합 후 수행.
- 계층 정규화, W-MSA, MLP, SW-MSA 등으로 구성.
- 아래 그림은 스윈 트랜스포머 블록 구조.
![[Pasted image 20250131061147.png]]

- 스윈 트랜스포머 블록: MSA 대신 W-MSA, SW-MSA 순차 수행.
- W-MSA: 로컬 윈도 사용 멀티 헤드 셀프 어텐션. 입력 특징 맵을 겹치지 않게 나눠 각 윈도에서 독립적 셀프 어텐션 수행.
- W-MSA: 이미지에 일정한 크기 윈도 영역 설정, 영역 내 픽셀 간 어텐션 계산. 이미지 전체 어텐션, 특정 위치 정보 추출, 지역 특징 분석. 셀프 어텐션 비용 2차 -> 1차 계산 복잡도로 감소.
- W-MSA 한계: 윈도 내부 이미지 패치 영역만 셀프 어텐션 수행.
- SW-MSA: W-MSA를 보완하기 위해서 윈도 간 연결성 구축, 관계 정보 추출. 윈도 가로/세로 이동, 인접 윈도 간 셀프 어텐션 계산.
![[Pasted image 20250131061448.png]]
- W-MSA 방식은 로컬 윈도(빨간색 영역) 내 패치 간 셀프 어텐션 연산.
- 로컬 윈도 이동하며 각 윈도에서 연산, 계산량 증가되는 것처럼 보이지만 효율적인 배치 계산(Effieient Batchh Computation)으로 연산량 대폭 감소.
- 로컬 윈도 연산 과정에서 배치 축 사용, 연산 속도 향상되며, 자연어 처리 작업에서 높은 성능을 보임 
- SW-MSA는 로컬 윈도 간 셀프 어텐션, W-MSA보다 많은 윈도, 더 많은 셀프 어텐션 연산 필요.
- 순환 시프트(Cyclic Shift), 어텐션 마스크(Attention Mask)로 문제 해결.
![[Pasted image 20250131061810.png]]
- 순환 시프트: 로컬 윈도 이동 시, 이동 위치 정보 이전 위치에서 가져옴. 로컬 윈도 사이즈 M보다 작은 M/2만큼 이동.
- 어텐션 마스크: 이동 위치 연산 방지. 이동 위치 연산 차단.
- 역순환 시프트: 셀프 어텐션 수행 후, 원래 로컬 윈도 구조 복원.
- SW-MSA에서 로컬 윈도 간 셀프 어텐션 효율적 수행.
![[Pasted image 20250131062038.png]]
- SW-MSA는 순환 시프트, 어텐션 마스크로 로컬 윈도 간 효율적 셀프 어텐션 수행.
- 어텐션 마스크: 9개 윈도 파티션 대신 4개 윈도 파티션, 4개 셀프 어텐션 값으로 모든 로컬 윈도 간 셀프 어텐션 값 계산.
- 순환 시프트로 이동된 윈도 정보 보존, 불필요 연산 감소, 효율적 계산.
- 기존 ViT와 달리, 스윈 트랜스포머 셀프 어텐션은 상대적 위치 편향(Relative Position Bias) 고려.
- 상대적 위치 편향: 로컬 윈도 내 패치 간 상대적 거리 임베딩.
- 아래 그림은 상대적 위치 편향 계산 과정 예시.
![[Pasted image 20250131062206.png]]- 이미지 패치 1~4, 패치 간 상대적 거리 X축, Y축 두 가지 방식 표기.
- X축: 가로 방향 거리, Y축: 세로 방향 거리.
- X축 위치 행렬: 같은 X축 -> 상대적 거리 0, 아래 -> 1, 위 -> -1.
- Y축 위치 행렬: 같은 Y축 -> 상대적 거리 0, 오른쪽 -> -1, 왼쪽 -> 1.
