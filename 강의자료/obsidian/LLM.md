# LLM
- 2017년 Google Brain 팀의 Transformer라는 인공지능 AI 딥러닝 모델 소개 
- 이 이후 트랜스포머는 학계와 산업에서 다양한 자연어 처리(Natural Language Processing NLP) 작업의 표준이 됨 
- LLM(Large Language Model 대규모 언어 모델)은 대부분 트랜스포머 아키텍처에서 파생된 AI 모델임 
- 사람의 언어, 코드 등을 이해하고 생성하기 위해서 설계 
- 방대한 양의 텍스트 데이터를 학습, 사람 언어의 복잡성과 뉘앙스를 포착할 수 있음 
- LLM은 의료 산업에서 임상시험 매칭, 신약 발견, 사기 탐기, 금융 뉴스의 감정분석, 트레이딩 전략에 활용 

![[Pasted image 20250105233546.png]]
- 위의 그림은 NLP의 획을 긋는 역사적인 모델
- 트랜스포머는 시퀀스 내 각 단어가 다른 모든 단어에 '주의를 기울이게'하여 단어 간의 장거리 종속성(long range dependencies)과 문맥 관계를 포착할 수 있게 하는 특별한 종류의 어텐션 기법인 셀프 어텐션(Self-Attention)을 사용 
- Hugging Face와 같은 인기 있는 LLM 저장소가 등장하면서 대중들에게 오픈 소스 모델로 쉽게 접근 가능하게 만듬 

## 정의 
### 언어 모델링 
- 언어 모델링(Language Modeling)은 **단어 시퀀스(또는 문자 시퀀스)의 확률 분포를 모델링**
- 즉, 주어진 단어 시퀀스가 얼마나 그럴듯한지, 얼마나 자연스러운지 확률을 부여하는 모델을 만드는 것
- 지정된 어휘내의 토큰 시퀀스 가능성을 예측하기 위한 통계/딥러닝 모델의 생성
### 토큰
- **토큰(Token)** 은 **텍스트를 구성하는 기본 단위**를 의미
- 의미를 가지는 가장 작은 단위 
- 문장이나 텍스트를 작은 단위로 나눠 생성
- 일반적으로 **단어(word)** 와 유사하지만, 반드시 단어와 일치하는 것은 아님
- 토큰은 언어 모델의 종류, 훈련 데이터, 그리고 토큰화(tokenization) 방법에 따라 다르게 정의

### 자기회귀 언어 모델 
- 자기회귀 언어 모델(Autoregressive Language Model)은 문장에서 이전 토큰만을 기반으로 다음 토큰을 예측하도록 훈련 
- 트랜스포머 모델의 디코더 부분에 해당 
- 어텐션 헤드가 앞서 온 토큰만 볼 수 있도록 전체 문장에 마스크(Mask)가 적용 
- 자기 회귀 모델은 텍스트 생성에 이상적임 
- 대표적인 모델이 GPT
### 자동 인코딩 언어 모델 
- 자동 인코딩 언어 모델(Autoencoding Language Model)은 손상된 버전의 입력 내용으로부터 기존 문장을 재구성하도록 훈련 
- 트랜스포머 모델의 인코더에 해당 
- 마스크 없이 전체 입력에 접근할 수 있음 
- 자동 인코딩 모델은 전체 문장의 양방향 표현을 생성함 
- 텍스트 생성과 같은 다양한 작업에 파인튜닝될 수 있지만 주요 애플리케이션은 문장 분류 또는 토큰 분류 
- 대표적인 모델로는 BERT 

![[Pasted image 20250105234811.png]]

## 특징 
### Seq2Seq 
- 2017년에 고안된 모델 
- 2가지 중요한 특징이 있음 
	- 인코더(Encoder) : 원시 텍스트를 받아들여 핵심 구성 요소로 분리하고 해당 구성 요소를 벡터로 변환하는 업무  
	- 디코더 : 수정된 형식의 어텐션을 사용하여 다음에 올 최적의 토큰을 예측함으로써 텍스트 생성하는데 뛰어남 

### 트랜스포머 
![[Pasted image 20250105235144.png]]

#### 파생 모델 
- 인코더와 디코더를 모두 사용 
- 대표 모델 T5

- 인코더만 있는 모델 
	- BERT 계열 
	- 자동 인코딩 모델링 작업을 학습하고 실행할 수 있음 
	- 이해하는 작업에 뛰어남 

![[Pasted image 20250105235359.png]]
- 디코더만 있는 모델 
	- 자기회귀 모델링 작업을 학습하고 실행할 수 있음 
	- 생성하는 작업에 뛰어남 

![[Pasted image 20250105235433.png]]

## 작동원리 
### 사전 훈련 
- 이름이 붙여진 거의 모든 LLM은 대량의 텍스트 데이터로 특정 언어 모델링 관련 작업에 대해 사전 훈련(pre-training)되어 있음 
- 사전 훈련 중에 LLM은 일반적인 언어와 단어 간의 관계를 배우고 이해하려고 함 
- 모든 LLM은 서로 다른 말뭉치와 서로 다른 작업에 대해서 훈련되었음 
- BERT는 공개적으로 아래 두 개의 코퍼스를 사용 
	- 영어 위키백과 ( 약 25억 단어)
	- BookCorpus(약 8억 단어)
- BERT의 훈련 방식은 아래 두가지 방식 
![[Pasted image 20250105235823.png]]
- 마스크된 언어 모델링(Masked Language Modeling) 작업은 BERT가 하나의 문장안에서 토큰의 상호작용을 인식하도록 도와줌 
- 다음 문장 예측(Next Sentence Prediction NSP) 작업은 BERT가 문장들 사이에서 토큰이 어떻게 상호작용 하는지 이해하도록 도와줌 
- LLM 모델들은 기업의 요구에 따라서 독점적인 데이터 소스에서 훈련함

### 전이학습 
- 전이학습(Transfer Learing)은 머신러닝에서 한 작업에서 얻은 지식을 활용하여 다른 관련 작업의 성능을 향상시키는 기술 
- 기본 아이디어는 사전 훈련된 모델이 이미 특정 언어와 언어 내 단어 간의 관계에 대한 많은 정보를 학습하였으므로, 이 정보를 새로운 작업에서의 성능을 향상시키기 위한 시작점으로 사용할 수 있음 

![[Pasted image 20250106000318.png]]
### 파인튜닝 
- 사전 훈련된 LLM은 특정 작업을 위해 파인튜닝(Fine-Turing)이 될 수 있음 
- LLM을 작업에 특화된 상대적으로 작은 크기의 데이터셋에서 훈련시켜, 특정 작업을 위한 파라미터를 조정하는 것을 의미 
- 이를 통해 특정 작업의 정확도를 향상시킬 수 있음 
![[Pasted image 20250106000516.png]]
- Hugging Face의 트랜스포머 패키지는 LLM을 훈련하고 파인튜닝하기 위한 깔끔하고 명료한 API를 제공함 
1. 파인튜닝하려는 모델과 파라미터를 결정 
2. 학습 데이터를 수집 
3. 목적에 맞게 손실함수 및 옵티마이저를 선택 
4. 학습을 통한 파라미터 업데이트 


### 전이학습과 파인튜닝 차이점 
- **전이 학습(Transfer Learning)** 과 **파인 튜닝(Fine-tuning)** 은 둘 다 **사전 학습된 모델(Pre-trained Model)** 을 활용하여 새로운 작업(task)에 적용하는 방법
- 몇 가지 중요한 차이점이 있습니다.

**1. 전이 학습 (Transfer Learning):**
*   **개념:**  넓은 의미에서 **사전 학습된 모델의 지식을 새로운 작업에 전달하여 활용하는 모든 방법**을 포괄합니다.
*   **목표:** 사전 학습된 모델에서 **유용한 특성(feature)을 추출**하여 새로운 작업에 활용하거나, **모델의 일부를 재사용**하여 학습 시간과 데이터를 절약하고 성능을 향상시키는 것입니다.
*   **방법:**
    *   **Feature Extraction (특성 추출):**
        *   사전 학습된 모델을 **feature extractor**로 사용합니다.
        *   사전 학습된 모델의 가중치를 **고정(freeze)** 하고, 마지막 레이어(fully connected layer)를 새로운 작업에 맞게 교체합니다.
        *   새로운 데이터셋으로 마지막 레이어만 학습합니다.
        *   **예:** 이미지 분류에서 ImageNet으로 사전 학습된 CNN 모델을 사용하여 새로운 이미지 데이터셋을 분류할 때, CNN 부분은 feature extractor로 사용하고 마지막 fully connected layer만 새로운 데이터셋으로 학습합니다.
    *   **Fine-tuning (미세 조정):** (뒤에서 자세히 설명)
   *   **특징:**
    *   사전 학습된 모델의 가중치를 **대부분 고정하거나 일부만 미세 조정**합니다.
    *   새로운 작업에 대한 데이터가 적을 때 유용합니다.
    *   계산 비용이 적게 듭니다.

**2. 파인 튜닝 (Fine-tuning):**
*   **개념:** 전이 학습의 한 종류로, **사전 학습된 모델의 가중치를 새로운 작업에 맞게 미세 조정**하는 방법
*   **목표:** 사전 학습된 모델의 지식을 **유지하면서** 새로운 작업에 **최적화**하는 것입니다.
*   **방법:**
    *   사전 학습된 모델의 **모든 레이어 또는 일부 레이어의 가중치를 업데이트**합니다.
    *   일반적으로 **낮은 learning rate**를 사용하여 가중치를 조금씩 조정합니다.
    *   **Layer-wise learning rate decay**와 같은 기법을 사용하여, 앞쪽 레이어는 더 작은 learning rate로, 뒤쪽 레이어는 더 큰 learning rate로 학습할 수 있습니다.
*   **특징:**
    *   사전 학습된 모델의 가중치를 **초기값으로 사용**하고, 새로운 데이터셋으로 **전체 모델을 다시 학습**합니다.
    *   새로운 작업에 대한 데이터가 **어느 정도 충분히 있을 때** 효과적입니다.
    *   Feature extraction보다 일반적으로 **더 높은 성능**을 보입니다.
    *   Feature extraction보다 계산 비용이 더 많이 듭니다.

**핵심 차이점 요약:**

| 특징        | 전이 학습 (Transfer Learning)         | 파인 튜닝 (Fine-tuning)                    |
| :-------- | :-------------------------------- | :------------------------------------- |
| **범위**    | 사전 학습된 모델을 활용하는 **넓은 범위의 방법**을 포괄 | 전이 학습의 **한 종류**                        |
| **목표**    | 사전 학습된 모델의 지식 **전달**              | 사전 학습된 모델을 새로운 작업에 **최적화**             |
| **가중치**   | **일부 또는 전체를 고정**                  | **대부분 또는 전체를 업데이트**                    |
| **학습 정도** | **일부 레이어만 학습** (주로 마지막 레이어)       | **전체 모델을 다시 학습** (낮은 learning rate 사용) |
| **데이터 양** | **적은 양의 데이터**로도 가능                | **어느 정도 충분한 양의 데이터** 필요                |
| **계산 비용** | **적음**                            | **많음**                                 |
| **성능**    | Fine-tuning보다 일반적으로 **낮음**        | Fine-tuning이 일반적으로 **높음**              |

- 전이학습 예 
![[Pasted image 20250106001217.png]]
- 파인튜닝의 예 
![[Pasted image 20250106001320.png]]


### 어텐션 
- 어텐션(Attention)은 트랜스포머만 아니라 다양한 가중치를 입력의 다른 부분에 할당하는 딥러닝 모델에서 사용되는 메커니즘 
- 모델이 동적으로 입력의 다른 부분에 집중할 수 있게 하여, 성능 향상과 더 정확한 결과를 이끌어냄 
- 어텐션에 의존하는 현대의 LLM은 동적으로 입력 시퀀스의 중요한 부분에 집중할 수 있어, 각 부분의 중요성을 고려하여 예측함 
![[Pasted image 20250106001652.png]]
- LLM이 사전 훈련과 파인튜닝을 통해서 어떤 종류의 규칙들을 단순히 학습할 수 있는지를 탐구
- 하버드 대학의 연구자들이 주도한 일련의 테스트들에서 오델로 게임과 같은 복합적인 작업에 대해서 LLM이 규칙을 배울 능력이 있는지 연구 
- LLM이 이전에 실행된 게임 데이터에 대한 훈련만으로 게임의 규칙을 이해할 수 있다는 증거를 발견 
![[Pasted image 20250106001918.png]]
- 모든 LLM이 어떠한 규칙을 학습하기 위해서는 우리가 텍스트를 인지하는 것처럼 기계가 이해할 수 있는 것으로 변환해야 하는데 이를 임베딩(Embedding)이라고 함 

### 임베딩 
- 임베딩(Embedding)은 고차원 공간에서의 단어, 구절 또는 토큰의 수학적 표현 
- 여러 종류의 임베딩이 가능하며, 이 중 위치 임베딩은 문장에서 토큰의 위치를 인코딩하며, 토큰 임베딩은 토큰의 의미를 인코딩 
![[Pasted image 20250106002106.png]]

### 토큰화 
- 토큰화(Tokenization)은 텍스트를 가장 작은 이해 단위로 토큰으로 분해하는 과정 
- 이 토큰들은 의미를 내포한 정보 조각 
- 어텐션 계산에 입력으로 사용되어 LLM이 실제로 학습하고 작동하게 됨 
- 토큰은 LLM의 정적 어휘를 구성하며, 항상 전체 단어를 나타내는 것은 아님 
	- 토큰은 구두점, 개별 문자 또는 LLM이 알지 못하는 단어의 하위 단어를 나타낼 수 있음 
	- BERT는 특별한 CLS라는 토큰이 존재 
	- 모든 입력의 첫 번째 토큰으로 자동으로 삽입 
	- 전체 입력 시퀀스에 대한 인코딩된 의미를 나타내기 위한 것 

- LLM이 어휘 사전에 없는(Out Of Vocabulary OOV) 구문 처리를 위해서 서브워드 토큰화를 사용
	- **서브워드 토큰화(Subword Tokenization)** 는 텍스트를 단어(word)보다 작은 단위인 **서브워드(subword)** 로 분리하는 토큰화 방법
	- **Byte Pair Encoding (BPE), WordPiece, SentencePiece**는 대표적인 서브워드 토큰화
![[Pasted image 20250106003045.png]]
- LLM은 이전에 본적 없는 단어들을 처리해야 함. 여기서 토큰 ##an은 단어 an과 다른 토큰 

### 인간 피드백 기반 강화 학습 
- 인간 피드백 기반 강화학습(Reinforcement Learning from Human Feedback)은 사전 훈련된 LLM을 정렬하는데 인기 있는 방법 
- 사람의 피드백을 사용하여 성능을 향상 

## 대표적인 LLM 종류 
### BERT 
![[Pasted image 20250106003425.png]]
- 문장의 양방향 표현을 구성하기 위해 어텐션 메커니즘을 사용하는 자동 인코딩 모델 
- 이 접근법은 문장 분류와 토큰 분류 작업에 이상적 

### GPT-4와 ChatGPT
- 어텐션 메커니즘을 사용하여 이전 토큰을 기반으로 시퀀스에서 다음 토큰을 예측하는 자기회귀 모델 
- GPT 알고리즘 계열에는 ChatGPT와 GPT-4등이 있음 
- 주로 텍스트 생성에 사용 
- 사람이 쓴 것처럼 자연스러운 텍스트를 생성할 수 있는 능력이 있음 
![[Pasted image 20250106003757.png]]
## 다운스트림 태스크 
- **다운스트림 태스크 (Downstream Task):**
- 사전 학습된 모델(Pre-trained Model)의 출력을 기반으로 특정 문제를 해결하기 위해 미세 조정(fine-tuning)되는 작업.
### 텍스트 분류 
- 텍스트 분류 작업은 주어진 텍스트 조각에 레이블을 할당하는 것 
- 이 작업은 감정 분석에 흔히 사용
![[Pasted image 20250106004250.png]]
- 텍스트 분류는 가장 잘 알려져 있는 작업 중 하나 

### 번역 작업 
- 기계 번역으로 이작업의 목표는 의미와 맥락을 유지하면서 한 언어의 텍스트를 다른 언어로 번역하는 것 
- 최신 LLM은 사전 훈련과 효율적인 어텐션 계산 덕분에 이 작업을 더 쉽게 수행 
![[Pasted image 20250106004429.png]]


### SQL 생성 
- SQL을 언어로 간주하면 영어를 SQL로 변환하는 것은 번역하는 것과 다르지 않음 
- 최신 LLM은 이미 기본적으로 이를 즉석에서 수행할 수 있지만, 보다 정교하고 복잡한 SQL 쿼리에 대해서는 종종 파인튜닝이 필요 


# 의미 기반 검색 
- LLM의 강력한 응용 분야가 의미 기반 검색(Semantic Search)
- 텍스트 임베딩은 단어나 구문을 그들의 맥락적 의미를 기반으로 다차원 공간에서 기계가 읽을 수 있는 수치 벡터로 표현하는 방법 
- 기본적인 원리는 두 구문의 유사도를 측정 
- 이때 유클리드 거리와 같은 측정치를 사용할 수 있다 

## 임베딩 
### OpenAI
- 유사한 항목을 서로 가까이 배치하는 임베딩 메커니즘에 의존 
- 항목이 실제로 유사할 때 코사인 유사도가 크게 나옴 
- 여기서는 OpenAI의 임베딩 엔진을 이용해서 작업 

```python
import os
import openai

client = OpenAI(
    api_key=keys
)

ENGINE = 'text-embedding-ada-002'

# helper functions to get lists of embeddings from the OpenAI API
def get_embeddings(texts, engine=ENGINE):
    response = client.embeddings.create(
        input=texts,
        model=engine
    )

    return [d.embedding for d in list(response.data)]

def get_embedding(text, engine=ENGINE):
    return get_embeddings([text], engine)[0]

len(get_embedding('hi')), len(get_embeddings(['hi', 'hello']))
```


### 오픈소스 
- OpenAI와 같은 회사의 임베딩 API를 사용하면 비용이 추가 된다. 
- 이때 대안으로 오픈 소스를 사용할 수 있다 
- BERT를 이용한 바이 인코더(Bi encoder)
- Sentence Transformers 라이브러리는 다양한 자연어 처리 작업을 위해 사용할 수 있는 사전 훈련 모델을 제공 
- 바이-인코더는 두 개의 BERT모델을 훈련시키는 과정을 포함 
- 하나는 입력 텍스트를 인코딩
- 다른 하나는 출력 텍스트를 인코딩하기 위한 것 
- 이 두 모델은 입력과 출력 텍스트의 쌍이 서로가 최대한 유사하도록 큰 텍스트 데이터 말뭉치에서 동시에 훈련 
- 결과적으로 얻어진 임베딩들은 입력과 출력 텍스트 사이의 의미적 관계를 포착 
![[Pasted image 20250106012545.png]]
```python
from sentence_transformers import SentenceTransformer
from sentence_transformers import util
import numpy as np
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

sentences = [
    "이것은 예시 문장입니다.",
    "각 문장이 변환됩니다.",
    "이것은 또 다른 예시입니다."
]

embeddings = model.encode(sentences)
print(embeddings)


corpus = ['한 남자가 음식을 먹는다.',
 '한 남자가 빵 한 조각을 먹는다.',
 '그 여자가 아이를 돌본다.',
 '한 남자가 말을 탄다.',
 '한 여자가 바이올린을 연주한다.',
 '두 남자가 수레를 숲 속으로 밀었다.',
 '한 남자가 담으로 싸인 땅에서 백마를 타고 있다.',
 '원숭이 한 마리가 드럼을 연주한다.',
 '치타 한 마리가 먹이 뒤에서 달리고 있다.']

corpus_embeddings = model.encode(corpus, convert_to_tensor=True)

queries = ['한 남자가 파스타를 먹는다.',
  '고릴라 의상을 입은 누군가가 드럼을 연주하고 있다.',
  '치타가 들판을 가로 질러 먹이를 쫓는다.']

# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = 5
for query in queries:
    query_embedding = model.encode(query, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
    cos_scores = cos_scores.cpu()

 #We use np.argpartition, to only partially sort the top_k results
top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]

print("\n\n======================\n\n")
print("Query:", query)
print("\nTop 5 most similar sentences in corpus:")

for idx in top_results[0:top_k]:
    print(corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]))
```

## 문서 청킹 
- 텍스트 임베딩 엔진이 설정되면, 큰 문서를 임베딩하는 어려움이 생김 
- 특히 책이나 연구 논문과 같은 긴 문서를 다룰 때, 전체 문서를 단일 벡터로 임베딩하는 것은 종종 어려움을 겪게 됨 
- 이 문제에 대한 해결책으로 문서 청킹(Document Chunking)을 사용하는 것 
- 문서 청킹은 큰 문서를 임베딩하기 위해 더 작고 관리 가능한 청크로 나누는 것을 의미 

### 벡터 데이터베이스 
- 벡터 데이터베이스(Vector Database)는 벡터를 빠르게 저장하고 검색하기 위해 특별히 설계된 데이터 저장 시스템 
- 이러한 데이터베이스는 문서나 문서 일부의 의미를 인코딩하고 저장하는 LLM에 의해 생성된 임베딩을 저장하는데 유용함 
- 임베딩을 벡터 데이터베이스에 저장함으로써, 의미적으로 유사한 텍스트를 검색을 수행할 수 있음 
