{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e134be-b5f2-44c5-9fa1-c71ad7497571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f92b9d-0c0e-4919-95a8-f9d3395bb497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 19:12:22.853096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-01 19:12:23.041241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735726343.139277   20330 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735726343.169934   20330 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-01 19:12:23.362385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "#KB도 있고 대기업에서 배포한 학습 bert 모델이 많음 \n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5f6f13-edb9-4cb9-a56f-32420a45e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-2 \n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e359bfb-d81b-46e3-81e3-7521711ee490",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What is transformers models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa580a9d-4d25-4ef2-9deb-7e818a0d0d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 19081,  4275,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토크나이저에 이미 작업과정이 다 들어있음 / 파이토치 버전으로 넘겨줘 /  \n",
    "bert_tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6eb4249-4270-4f57-8d3b-6db8aab7cca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[2061,  318, 6121,  364, 4981,   30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644357c1-d9aa-45ff-a2a8-50585e4fa3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = bert_tokenizer(text, return_tensors='pt')\n",
    "encoded_text_gpt = gpt_tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b02dd4-8c70-43ee-9376-5c0010fed4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [2061, 318, 6121, 364, 4981, 30]\n",
      "attention_mask [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tmp_dict = {'input_ids': [2061,  318, 6121,  364, 4981,   30], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "#**은 어떤 형태인지 모르겠으나 모두 딕셔너리 형태로 넘겨줘 라는 뜻 / * 는 리스트 형태로 넘겨줘 \n",
    "def test(**args):\n",
    "    for key, val in args.items():\n",
    "        print(key, val)\n",
    "\n",
    "test(**tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc26ffb-97a0-4a48-8e55-2c4d1582431c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1417,  0.2247,  0.1933,  ..., -0.4819,  0.3708,  0.4487],\n",
       "         [ 0.1381, -0.0460,  0.0830,  ...,  0.1768,  0.6270, -0.1286],\n",
       "         [ 0.0999,  0.0931,  0.5192,  ..., -0.2497,  0.3004,  0.9019],\n",
       "         ...,\n",
       "         [ 0.7405,  0.2418,  0.0075,  ..., -0.4142, -0.0557, -0.0544],\n",
       "         [ 0.1038, -0.1234, -0.5671,  ..., -0.0185,  0.3046, -0.2642],\n",
       "         [ 0.7071,  0.0196, -0.0117,  ...,  0.3408, -0.4298, -0.2909]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9316, -0.4840, -0.8424,  0.8430,  0.5167, -0.3088,  0.9392,  0.4710,\n",
       "         -0.6954, -1.0000, -0.3170,  0.9067,  0.9851,  0.5453,  0.9490, -0.8589,\n",
       "         -0.4671, -0.6875,  0.3294, -0.7204,  0.6933,  1.0000,  0.1113,  0.4585,\n",
       "          0.5222,  0.9469, -0.8261,  0.9412,  0.9715,  0.8043, -0.8089,  0.3600,\n",
       "         -0.9891, -0.2619, -0.7908, -0.9923,  0.5010, -0.8049, -0.0522, -0.0602,\n",
       "         -0.8865,  0.3451,  1.0000, -0.5877,  0.3715, -0.4366, -1.0000,  0.3735,\n",
       "         -0.9085,  0.8017,  0.7727,  0.5914,  0.3281,  0.5548,  0.6078, -0.1821,\n",
       "         -0.0424,  0.1751, -0.3635, -0.6592, -0.7391,  0.5325, -0.7714, -0.9100,\n",
       "          0.8224,  0.7683, -0.2316, -0.4104, -0.1823,  0.0791,  0.9385,  0.2563,\n",
       "         -0.1892, -0.8815,  0.5184,  0.3388, -0.7084,  1.0000, -0.6677, -0.9778,\n",
       "          0.7052,  0.7985,  0.6625, -0.3100,  0.5973, -1.0000,  0.5269, -0.2109,\n",
       "         -0.9911,  0.3641,  0.5733, -0.3003,  0.6733,  0.6400, -0.7170, -0.4456,\n",
       "         -0.2933, -0.6972, -0.2544, -0.4086,  0.0929, -0.3396, -0.4803, -0.4394,\n",
       "          0.3891, -0.6160, -0.6223,  0.6395,  0.2973,  0.7414,  0.5265, -0.4209,\n",
       "          0.4730, -0.9616,  0.6874, -0.4678, -0.9895, -0.6488, -0.9891,  0.7409,\n",
       "         -0.3268, -0.3351,  0.9643, -0.0343,  0.5431, -0.2730, -0.7961, -1.0000,\n",
       "         -0.5900, -0.5681, -0.3863, -0.4150, -0.9802, -0.9706,  0.7179,  0.9660,\n",
       "          0.3663,  0.9999, -0.3282,  0.9509, -0.2074, -0.7050,  0.3195, -0.6022,\n",
       "          0.7896,  0.3791, -0.7363,  0.3471, -0.3678,  0.0679, -0.7729, -0.3251,\n",
       "         -0.7035, -0.9425, -0.4285,  0.9519, -0.4888, -0.9024, -0.0896, -0.3619,\n",
       "         -0.5267,  0.8868,  0.7154,  0.4497, -0.5255,  0.4734,  0.4211,  0.5397,\n",
       "         -0.9173,  0.0733,  0.5082, -0.4450, -0.8235, -0.9784, -0.4380,  0.6595,\n",
       "          0.9913,  0.8356,  0.3902,  0.6430, -0.4589,  0.4619, -0.9665,  0.9823,\n",
       "         -0.2038,  0.3664, -0.0842,  0.3726, -0.8869,  0.1203,  0.8888, -0.3729,\n",
       "         -0.8646, -0.1522, -0.5560, -0.5488, -0.7043,  0.5070, -0.3451, -0.4392,\n",
       "         -0.1115,  0.9311,  0.9871,  0.7999,  0.1588,  0.6806, -0.9028, -0.5901,\n",
       "          0.1643,  0.2812,  0.1597,  0.9939, -0.6042, -0.2385, -0.9357, -0.9860,\n",
       "          0.0762, -0.9043, -0.2703, -0.7294,  0.6841,  0.1167,  0.4696,  0.4781,\n",
       "         -0.9895, -0.8496,  0.4554, -0.5066,  0.5569, -0.3464,  0.7174,  0.8988,\n",
       "         -0.7463,  0.8710,  0.9403, -0.8199, -0.7531,  0.8876, -0.4245,  0.9228,\n",
       "         -0.7103,  0.9928,  0.8253,  0.7348, -0.9170, -0.7140, -0.9259, -0.6837,\n",
       "         -0.0689,  0.1960,  0.8655,  0.7064,  0.5096,  0.2299, -0.6993,  0.9986,\n",
       "         -0.8260, -0.9620, -0.0139, -0.2637, -0.9903,  0.7427,  0.3834,  0.1519,\n",
       "         -0.5210, -0.7816, -0.9698,  0.9051,  0.2240,  0.9935, -0.2700, -0.9425,\n",
       "         -0.6421, -0.9435, -0.0425, -0.3040, -0.2672, -0.0085, -0.9616,  0.6457,\n",
       "          0.6909,  0.5598, -0.7975,  0.9994,  1.0000,  0.9747,  0.9307,  0.9129,\n",
       "         -0.9996, -0.4793,  1.0000, -0.9818, -1.0000, -0.9499, -0.6792,  0.5216,\n",
       "         -1.0000, -0.4064, -0.1012, -0.9239,  0.6345,  0.9805,  0.9948, -1.0000,\n",
       "          0.8685,  0.9406, -0.7324,  0.9136, -0.5157,  0.9735,  0.6962,  0.5936,\n",
       "         -0.3540,  0.5454, -0.9447, -0.9069, -0.4870, -0.7351,  0.9947,  0.2882,\n",
       "         -0.8684, -0.9040,  0.3408, -0.2831, -0.2212, -0.9678, -0.3054,  0.3291,\n",
       "          0.7443,  0.2825,  0.4495, -0.7838,  0.3421,  0.0050,  0.4475,  0.7303,\n",
       "         -0.9520, -0.6420, -0.0375, -0.2564, -0.6470, -0.9744,  0.9640, -0.4397,\n",
       "          0.6932,  1.0000,  0.1080, -0.8834,  0.7003,  0.3421, -0.3953,  1.0000,\n",
       "          0.8310, -0.9816, -0.6325,  0.6507, -0.6489, -0.6699,  0.9995, -0.4148,\n",
       "         -0.6559, -0.3479,  0.9792, -0.9918,  0.9900, -0.9311, -0.9649,  0.9679,\n",
       "          0.9455, -0.6329, -0.7474,  0.2431, -0.7621,  0.3849, -0.9656,  0.7363,\n",
       "          0.5935, -0.2073,  0.8935, -0.8991, -0.6253,  0.4047, -0.7039,  0.0021,\n",
       "          0.8820,  0.5371, -0.3828,  0.2290, -0.3914, -0.4526, -0.9827,  0.3937,\n",
       "          1.0000, -0.2265,  0.4825, -0.5917, -0.1172, -0.0308,  0.6359,  0.6532,\n",
       "         -0.3656, -0.8687,  0.5903, -0.9748, -0.9895,  0.8058,  0.2942, -0.4615,\n",
       "          1.0000,  0.4394,  0.2917,  0.2674,  0.9567,  0.1334,  0.6781,  0.8587,\n",
       "          0.9836, -0.3880,  0.6910,  0.9040, -0.8472, -0.3873, -0.6944,  0.0832,\n",
       "         -0.9313,  0.0119, -0.9560,  0.9676,  0.8659,  0.4640,  0.3875,  0.4538,\n",
       "          1.0000, -0.4933,  0.7141, -0.4998,  0.8986, -0.9993, -0.8896, -0.5321,\n",
       "         -0.2093, -0.7646, -0.4137,  0.3677, -0.9735,  0.7436,  0.4400, -0.9917,\n",
       "         -0.9909,  0.0852,  0.9128,  0.1495, -0.9678, -0.7514, -0.6780,  0.5215,\n",
       "         -0.3821, -0.9484, -0.1015, -0.3826,  0.5518, -0.3420,  0.6240,  0.7918,\n",
       "          0.5633, -0.4357, -0.3695, -0.2258, -0.8735,  0.8954, -0.8784, -0.8215,\n",
       "         -0.3490,  1.0000, -0.6205,  0.8377,  0.8574,  0.7576, -0.3139,  0.3417,\n",
       "          0.8927,  0.3687, -0.8187, -0.7648, -0.7468, -0.4996,  0.7622,  0.3590,\n",
       "          0.7384,  0.7982,  0.7229,  0.2798, -0.2205,  0.1623,  0.9997, -0.1503,\n",
       "         -0.1776, -0.6647, -0.1240, -0.4730, -0.5992,  1.0000,  0.3917,  0.2954,\n",
       "         -0.9905, -0.7437, -0.9348,  1.0000,  0.8628, -0.8212,  0.7429,  0.4872,\n",
       "         -0.3161,  0.8201, -0.2783, -0.3728,  0.3653,  0.2230,  0.9604, -0.6378,\n",
       "         -0.9747, -0.7071,  0.4849, -0.9670,  0.9998, -0.6342, -0.3721, -0.4862,\n",
       "         -0.0044,  0.6817, -0.0475, -0.9867, -0.2686,  0.2494,  0.9598,  0.3164,\n",
       "         -0.6991, -0.9404,  0.6409,  0.7047, -0.8369, -0.9319,  0.9700, -0.9837,\n",
       "          0.6385,  1.0000,  0.5490, -0.1542,  0.2913, -0.6471,  0.4192, -0.1170,\n",
       "          0.7934, -0.9586, -0.4444, -0.2472,  0.3352, -0.2528, -0.1433,  0.6902,\n",
       "          0.3249, -0.6223, -0.7079, -0.1601,  0.5608,  0.8912, -0.3397, -0.2900,\n",
       "          0.2788, -0.2767, -0.9312, -0.4553, -0.4821, -1.0000,  0.8315, -1.0000,\n",
       "          0.2675,  0.1026, -0.3515,  0.8614,  0.4898,  0.5627, -0.7870, -0.8227,\n",
       "          0.4010,  0.7585, -0.3107, -0.2212, -0.7714,  0.3610, -0.1703,  0.1669,\n",
       "         -0.4848,  0.8529, -0.3464,  1.0000,  0.3123, -0.7055, -0.9832,  0.3576,\n",
       "         -0.3762,  1.0000, -0.9526, -0.9555,  0.4573, -0.7942, -0.8577,  0.4103,\n",
       "          0.2438, -0.7611, -0.8759,  0.9627,  0.9449, -0.6657,  0.4318, -0.4565,\n",
       "         -0.5670,  0.1047,  0.6954,  0.9870,  0.5192,  0.9344,  0.3377,  0.0342,\n",
       "          0.9747,  0.3527,  0.5981,  0.2196,  1.0000,  0.4188, -0.9295,  0.2067,\n",
       "         -0.9873, -0.3464, -0.9589,  0.3822,  0.3804,  0.9114, -0.2328,  0.9727,\n",
       "         -0.4768,  0.0300, -0.4853, -0.2519,  0.4864, -0.9274, -0.9890, -0.9877,\n",
       "          0.6631, -0.4990, -0.1408,  0.3325,  0.2302,  0.5257,  0.5080, -1.0000,\n",
       "          0.9309,  0.5213,  0.8856,  0.9626,  0.6639,  0.5381,  0.4060, -0.9883,\n",
       "         -0.9808, -0.4650, -0.2924,  0.7844,  0.7132,  0.9057,  0.4551, -0.6012,\n",
       "         -0.4845, -0.3941, -0.6465, -0.9939,  0.5182, -0.5272, -0.9756,  0.9640,\n",
       "         -0.0184, -0.1910, -0.0064, -0.7178,  0.9589,  0.8028,  0.4288,  0.2411,\n",
       "          0.5809,  0.9054,  0.9673,  0.9827, -0.8144,  0.8556, -0.5301,  0.4809,\n",
       "          0.7421, -0.9491,  0.3204,  0.4971, -0.5062,  0.3525, -0.3783, -0.9850,\n",
       "          0.5176, -0.4509,  0.6773, -0.4488, -0.0169, -0.4472, -0.3284, -0.7770,\n",
       "         -0.6889,  0.7040,  0.6111,  0.9135,  0.7675, -0.1700, -0.7077, -0.2782,\n",
       "         -0.7783, -0.9332,  0.9465, -0.2107, -0.3891,  0.6405, -0.0421,  0.7659,\n",
       "          0.2246, -0.4780, -0.4698, -0.8474,  0.8696, -0.4611, -0.6574, -0.6785,\n",
       "          0.6931,  0.4635,  1.0000, -0.7777, -0.8334, -0.3756, -0.3931,  0.4916,\n",
       "         -0.5146, -1.0000,  0.4627, -0.3954,  0.7107, -0.4836,  0.8229, -0.5929,\n",
       "         -0.9847, -0.3324,  0.4142,  0.6688, -0.5962, -0.5015,  0.6484,  0.2097,\n",
       "          0.9396,  0.8629, -0.3755, -0.0491,  0.7142, -0.7661, -0.7628,  0.9317]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(**encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "962e85fc-1b85-46b9-b21c-6e610f9b4b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.1643,  0.0957, -0.2844,  ..., -0.1632, -0.0774, -0.2154],\n",
       "         [-0.1234,  0.0442,  0.0240,  ..., -0.2752,  0.1524,  0.3333],\n",
       "         [-0.1695, -0.3943, -0.3917,  ..., -0.1077,  0.3173, -0.4133],\n",
       "         [-0.3354, -0.0618, -1.3236,  ..., -0.1970,  0.3384,  0.1069],\n",
       "         [-0.6139,  0.0136, -1.5310,  ..., -0.3137,  0.2608,  0.0270],\n",
       "         [-0.2761, -0.5130,  0.1960,  ...,  0.0550, -0.0251, -0.1793]]],\n",
       "       grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[-1.3190e+00,  1.8644e+00,  8.9757e-01,  ..., -1.4033e+00,\n",
       "           -2.3651e-01,  1.2896e+00],\n",
       "          [-1.8348e+00,  2.4955e+00,  1.7497e+00,  ..., -1.5397e+00,\n",
       "           -2.3685e+00,  2.4482e+00],\n",
       "          [-9.2793e-01,  1.7709e+00,  2.0284e+00,  ...,  2.7176e-01,\n",
       "           -1.2328e+00,  1.1275e+00],\n",
       "          [-2.5997e+00,  3.3536e+00,  2.3341e+00,  ...,  6.1344e-01,\n",
       "           -2.2147e+00,  2.4120e+00],\n",
       "          [-1.5183e+00,  3.0395e+00,  1.9572e+00,  ...,  1.0949e-01,\n",
       "           -1.7165e+00,  1.6786e+00],\n",
       "          [-2.4981e+00,  2.8332e+00,  1.9392e+00,  ..., -1.0660e+00,\n",
       "           -2.3585e+00,  1.9409e+00]],\n",
       "\n",
       "         [[-3.7719e-01,  4.4023e-01, -6.4755e-01,  ..., -3.9102e-01,\n",
       "            2.5417e+00,  1.0485e+00],\n",
       "          [ 6.7388e-01, -1.3429e+00, -1.0824e-01,  ..., -3.4649e+00,\n",
       "            3.4113e+00,  9.9918e-01],\n",
       "          [ 1.0763e+00, -2.3421e+00, -1.7250e+00,  ..., -4.3232e-01,\n",
       "            3.6918e+00,  2.0142e+00],\n",
       "          [-1.5648e+00,  8.2658e-01, -8.5370e-01,  ..., -1.3044e+00,\n",
       "            2.7591e+00,  6.1603e-01],\n",
       "          [ 1.6271e+00, -7.2923e-01,  2.4967e-01,  ..., -1.2446e+00,\n",
       "            2.7147e+00,  1.1312e+00],\n",
       "          [-3.4430e-01, -1.0480e+00, -1.0363e-01,  ..., -1.9122e+00,\n",
       "            2.5792e+00,  6.5190e-02]],\n",
       "\n",
       "         [[ 2.2048e-02, -7.5673e-02,  8.3024e-01,  ..., -1.4517e+00,\n",
       "           -1.6848e+00,  8.1629e-01],\n",
       "          [ 4.3450e-01,  2.0733e-01,  3.2424e-01,  ..., -2.4565e+00,\n",
       "            1.1946e-01,  1.9386e+00],\n",
       "          [ 9.8642e-01, -1.1411e+00,  1.0101e-01,  ..., -2.3615e+00,\n",
       "           -3.4432e-01,  3.1601e+00],\n",
       "          [-2.6659e-01, -1.7189e-01,  3.3439e-01,  ..., -2.5912e+00,\n",
       "            1.0748e-01,  9.5288e-01],\n",
       "          [ 1.6241e+00, -4.7254e-01, -9.8645e-02,  ..., -3.8823e+00,\n",
       "            7.9376e-02,  2.2926e+00],\n",
       "          [ 6.4428e-01,  6.9316e-01,  1.1326e+00,  ..., -2.0425e+00,\n",
       "            1.4424e+00,  1.8534e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3110e-01, -4.6927e-02, -1.3321e-01,  ...,  3.7510e-01,\n",
       "            6.8562e-01,  5.7607e-01],\n",
       "          [ 2.5121e-01,  1.1030e-01, -2.8209e-02,  ...,  8.2454e-01,\n",
       "            3.1058e-01,  5.9335e-01],\n",
       "          [-1.0394e+00,  3.2214e-01,  1.2598e-01,  ...,  9.1245e-01,\n",
       "            3.8059e-01, -6.9702e-01],\n",
       "          [ 6.8073e-03, -2.2922e-01,  5.0575e-01,  ...,  1.1443e+00,\n",
       "            5.3062e-01,  4.7216e-01],\n",
       "          [-3.5085e-02,  3.6524e-01,  1.1560e-01,  ...,  1.5125e+00,\n",
       "            2.2779e-01,  3.7555e-01],\n",
       "          [ 5.1079e-02, -1.5697e-04,  2.2588e-01,  ...,  1.0759e+00,\n",
       "            4.4574e-01,  4.6231e-01]],\n",
       "\n",
       "         [[ 1.4246e+00,  1.3487e+00, -2.4757e-01,  ..., -2.8181e-01,\n",
       "            9.5264e-01, -1.1325e+00],\n",
       "          [ 8.5059e-01,  6.2667e-01, -7.5365e-01,  ..., -9.0627e-01,\n",
       "            8.3284e-01, -5.7886e-01],\n",
       "          [ 1.3125e+00,  4.7276e-01, -6.7409e-01,  ..., -4.7092e-01,\n",
       "            6.2285e-01, -3.8107e-01],\n",
       "          [ 3.9066e-01,  2.3383e-01, -1.5592e+00,  ..., -1.3735e+00,\n",
       "            8.9532e-01, -7.7523e-01],\n",
       "          [ 1.7137e+00,  6.0465e-01, -3.8614e-01,  ..., -9.6389e-01,\n",
       "            4.0571e-01, -1.1798e-01],\n",
       "          [ 8.8399e-01,  1.3741e-01, -4.4718e-01,  ..., -6.9477e-01,\n",
       "            7.6977e-01,  3.6453e-01]],\n",
       "\n",
       "         [[ 4.9501e-01,  2.0561e-01, -7.4766e-02,  ..., -3.4694e-01,\n",
       "            1.5860e-01,  1.7998e+00],\n",
       "          [ 1.2434e+00,  6.7664e-02,  6.1694e-02,  ...,  7.0984e-01,\n",
       "            5.2591e-01,  1.7975e+00],\n",
       "          [-7.5917e-01, -4.5603e-01, -7.7850e-01,  ..., -2.9434e-01,\n",
       "            4.8637e-01,  1.9463e+00],\n",
       "          [-3.1555e-02,  6.5073e-01, -2.6542e-01,  ...,  2.3358e-01,\n",
       "            4.3566e-01,  9.1138e-01],\n",
       "          [-7.6558e-01, -3.9265e-01, -3.0463e-01,  ..., -2.2642e-01,\n",
       "            4.5706e-01,  1.4371e+00],\n",
       "          [-1.1231e-01,  1.0899e+00,  1.4645e+00,  ..., -7.6389e-01,\n",
       "            1.2188e-01,  1.0324e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 2.3089e-02,  8.2066e-02,  2.7945e-02,  ...,  2.6097e-02,\n",
       "           -2.3956e-02,  9.6702e-02],\n",
       "          [ 2.3196e-01, -2.3861e-01,  2.9457e-01,  ..., -5.8200e-02,\n",
       "            7.4808e-02, -8.7832e-02],\n",
       "          [ 3.2242e-02,  1.1287e-02, -1.6352e-01,  ..., -5.9199e-02,\n",
       "            3.2464e-01,  1.9572e-01],\n",
       "          [-1.7835e-01,  3.1739e-02, -1.3894e-01,  ...,  1.5822e-01,\n",
       "           -9.6225e-02, -2.5145e-01],\n",
       "          [ 1.3463e-02,  1.2617e-01, -1.5100e-01,  ..., -3.4030e-04,\n",
       "           -1.3269e-02, -1.9481e-02],\n",
       "          [-5.2062e-02,  6.0722e-02, -2.2644e-01,  ...,  1.5005e-01,\n",
       "           -2.8577e-01, -2.3280e-02]],\n",
       "\n",
       "         [[ 4.7284e-01,  1.1574e-01, -2.9770e-01,  ..., -6.3272e-01,\n",
       "           -2.1164e-01,  1.9056e-01],\n",
       "          [ 5.5125e-01,  7.0201e-02,  1.0743e-01,  ...,  1.1353e-02,\n",
       "            2.2987e-01, -7.8098e-02],\n",
       "          [ 3.5288e-01, -1.0815e-01, -2.0339e-02,  ...,  2.1925e-01,\n",
       "           -2.3392e-01, -2.3000e-01],\n",
       "          [ 4.1077e-01,  1.3323e-01,  1.4074e-01,  ...,  1.7415e-01,\n",
       "            4.5947e-01,  8.0482e-03],\n",
       "          [ 1.5297e-01,  5.6333e-02,  1.7624e-01,  ...,  1.8508e-01,\n",
       "           -3.8826e-02,  4.3533e-02],\n",
       "          [ 4.0171e-01,  3.0647e-02, -4.2317e-02,  ..., -1.8194e-01,\n",
       "            5.6371e-01, -2.4747e-01]],\n",
       "\n",
       "         [[ 7.3907e-02, -1.1804e-01,  8.6928e-02,  ..., -7.0653e-03,\n",
       "           -1.7060e-04, -6.2186e-02],\n",
       "          [ 9.5054e-02,  9.0989e-02,  5.3470e-02,  ..., -2.7077e-02,\n",
       "            9.1225e-02,  1.0601e-01],\n",
       "          [ 9.7621e-02,  4.2341e-03,  2.1298e-01,  ...,  2.3394e-01,\n",
       "           -9.5223e-02,  2.1792e-01],\n",
       "          [-8.9257e-02,  3.5644e-02, -3.4147e-01,  ..., -7.5399e-02,\n",
       "           -1.9381e-03, -3.4388e-02],\n",
       "          [ 1.9280e-01, -2.3668e-01, -4.3777e-02,  ..., -2.0150e-01,\n",
       "           -3.6350e-02,  2.4900e-01],\n",
       "          [-2.3146e-01,  2.0534e-01, -7.4261e-02,  ..., -9.0841e-03,\n",
       "           -6.5482e-02,  1.9889e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.8233e-02,  7.8248e-02,  8.7423e-02,  ..., -1.5830e-01,\n",
       "            8.4335e-02, -6.9996e-02],\n",
       "          [-4.7730e-01,  5.3618e-01,  1.1470e-01,  ...,  2.1382e-01,\n",
       "           -4.0779e-01,  1.0846e-01],\n",
       "          [-1.0855e-01, -1.2338e-01,  4.1710e-01,  ...,  2.6156e-01,\n",
       "            1.4752e-01, -1.6040e-01],\n",
       "          [-2.4685e-01,  8.7286e-02,  2.0852e-01,  ..., -2.2084e-01,\n",
       "            2.5562e-01,  3.6043e-01],\n",
       "          [-3.0849e-01,  4.1831e-01,  1.1109e-01,  ..., -3.2032e-02,\n",
       "            8.0740e-02,  3.4414e-02],\n",
       "          [ 5.2134e-03,  3.3071e-01,  9.9854e-02,  ..., -5.8137e-02,\n",
       "            2.2529e-02,  1.8651e-01]],\n",
       "\n",
       "         [[ 1.7370e-02, -8.3901e-02, -2.1068e-01,  ...,  1.3147e-01,\n",
       "            2.3579e-01, -4.4249e-02],\n",
       "          [-3.5043e-01,  7.0391e-02,  2.1141e-01,  ..., -6.4529e-01,\n",
       "           -1.4333e-01,  8.3923e-02],\n",
       "          [-2.3895e-01,  3.7611e-02,  1.1766e-02,  ...,  6.1418e-02,\n",
       "            9.9337e-03, -8.6962e-02],\n",
       "          [-1.1506e-01,  7.1640e-02, -1.4468e-01,  ..., -6.1278e-01,\n",
       "           -3.1185e-02, -1.6416e-01],\n",
       "          [-2.5117e-01, -2.7293e-01, -7.8139e-02,  ..., -4.6720e-01,\n",
       "            1.2360e-01,  4.0885e-02],\n",
       "          [-6.5717e-02,  1.2619e-01, -6.2645e-03,  ..., -2.7734e-02,\n",
       "            5.5060e-02, -8.9395e-02]],\n",
       "\n",
       "         [[ 7.5052e-02, -4.4469e-01,  1.6571e-01,  ...,  7.0882e-03,\n",
       "           -2.7171e-01, -8.7654e-02],\n",
       "          [-5.8485e-02, -1.2240e-02, -3.8542e-02,  ...,  1.7494e-01,\n",
       "            3.7288e-01,  1.2658e-01],\n",
       "          [-3.4592e-03, -3.3274e-02, -5.0681e-02,  ...,  3.0536e-01,\n",
       "            1.4533e-01,  1.9476e-01],\n",
       "          [ 2.3536e-02, -1.3128e-02, -2.0807e-01,  ...,  3.0906e-01,\n",
       "            2.6615e-01,  2.4545e-01],\n",
       "          [ 5.9430e-02,  2.1307e-02, -2.1710e-01,  ...,  1.3402e-01,\n",
       "           -2.8254e-01,  3.0093e-01],\n",
       "          [ 2.6312e-01, -4.0186e-02, -9.4522e-03,  ...,  5.5359e-02,\n",
       "            2.6957e-01, -2.4410e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.5472e-01,  1.6268e+00, -1.8836e+00,  ...,  1.4842e+00,\n",
       "           -1.3666e+00,  2.1387e-01],\n",
       "          [ 9.7149e-01,  2.6372e+00, -1.7462e+00,  ..., -6.0442e-01,\n",
       "           -2.1933e+00,  4.0203e-01],\n",
       "          [-1.3314e-01,  1.0952e+00, -1.5197e+00,  ..., -3.3945e-01,\n",
       "           -2.4303e-01,  2.7835e-01],\n",
       "          [ 1.4308e+00,  1.2376e+00, -1.5482e+00,  ..., -3.5584e-01,\n",
       "           -6.2627e-01,  2.9490e-01],\n",
       "          [ 1.2327e-01,  9.8066e-01, -1.1434e+00,  ..., -2.3300e-01,\n",
       "           -1.1804e+00, -9.7069e-01],\n",
       "          [ 4.3437e-01,  9.2002e-01, -1.0596e+00,  ...,  1.0448e-01,\n",
       "           -6.0085e-01,  7.5198e-02]],\n",
       "\n",
       "         [[-9.3697e-01, -2.7260e-01, -5.7192e-01,  ..., -9.0301e-02,\n",
       "            5.2307e-01, -8.0643e-01],\n",
       "          [-5.9683e-01,  3.3612e-01, -1.4138e+00,  ..., -7.0285e-01,\n",
       "            5.1227e-01, -3.1268e-01],\n",
       "          [-4.3387e-01, -4.8195e-02, -9.8209e-01,  ..., -1.1893e+00,\n",
       "           -3.0404e-01, -9.3066e-01],\n",
       "          [-4.5882e-01,  3.6432e-01, -1.5841e+00,  ..., -5.3187e-01,\n",
       "           -2.4932e-01, -5.3068e-01],\n",
       "          [-2.6594e-01, -3.0729e-01, -1.5162e+00,  ..., -9.9392e-01,\n",
       "           -4.5074e-01, -6.5368e-01],\n",
       "          [-6.3606e-01,  1.2306e+00, -1.4697e+00,  ..., -4.4699e-01,\n",
       "           -3.3400e-01, -1.1377e+00]],\n",
       "\n",
       "         [[ 5.1715e-01,  1.6527e-02, -6.9063e-02,  ..., -1.3421e+00,\n",
       "            9.8668e-02, -3.5192e-01],\n",
       "          [-1.0992e-01,  1.3660e-01, -1.2378e-01,  ..., -1.0395e+00,\n",
       "           -1.6754e-01,  3.4076e-01],\n",
       "          [-1.9296e-01,  4.5975e-01, -4.4008e-01,  ..., -1.2383e+00,\n",
       "           -1.8350e-01,  1.9633e-02],\n",
       "          [ 1.8864e-01, -3.2994e-02, -2.5112e-01,  ..., -8.1665e-01,\n",
       "            1.5091e-01,  4.6997e-02],\n",
       "          [ 1.8057e-01, -2.5239e-01, -1.9856e-01,  ..., -1.2601e+00,\n",
       "           -2.5061e-01,  1.6429e-01],\n",
       "          [-2.6022e-01, -4.3051e-02, -4.3405e-01,  ..., -1.2379e+00,\n",
       "            1.6925e-01,  3.0163e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2234e-02, -8.5425e-01, -7.2925e-01,  ..., -1.0152e+00,\n",
       "            7.2173e-01, -7.8648e-01],\n",
       "          [-1.8866e-01,  2.2690e+00,  1.4309e+00,  ..., -8.9005e-02,\n",
       "           -2.2363e-01, -5.9859e-03],\n",
       "          [-4.0663e+00,  1.4590e+00,  2.9781e+00,  ..., -1.0254e-01,\n",
       "           -2.2170e+00,  1.2114e+00],\n",
       "          [ 8.8726e-01,  8.2161e-01,  1.5758e+00,  ..., -3.9633e-01,\n",
       "           -3.3772e-01, -9.6067e-01],\n",
       "          [-1.0827e+00,  5.5131e-01,  2.1771e+00,  ..., -2.0071e-01,\n",
       "           -1.3019e+00, -7.0203e-01],\n",
       "          [-8.1438e-02,  1.8712e+00,  1.9375e+00,  ...,  1.5611e+00,\n",
       "           -1.6880e+00,  1.1348e+00]],\n",
       "\n",
       "         [[-1.1444e+00, -2.9872e+00,  1.7379e-01,  ...,  1.7589e+00,\n",
       "            1.6085e+00, -1.3629e+00],\n",
       "          [ 1.7109e-01,  9.8623e-01, -4.7831e-01,  ..., -9.1817e-01,\n",
       "            5.0882e-01, -4.7400e-01],\n",
       "          [ 2.7102e-01,  7.9745e-01, -5.2174e-01,  ..., -8.6263e-01,\n",
       "            6.5321e-01,  1.1420e-01],\n",
       "          [ 1.0701e-01,  7.3404e-01, -6.4824e-01,  ..., -4.5531e-01,\n",
       "            6.7286e-01, -1.6503e-01],\n",
       "          [ 1.7927e-02,  6.7554e-01, -5.7593e-01,  ..., -3.6039e-01,\n",
       "            7.8113e-01,  2.7157e-01],\n",
       "          [-1.5302e-01,  5.7646e-01, -6.4882e-01,  ..., -1.0491e-01,\n",
       "            6.5838e-01, -1.5423e-01]],\n",
       "\n",
       "         [[ 9.3066e-01,  2.5125e+00,  5.3014e-01,  ..., -6.1372e-01,\n",
       "           -5.9689e-01,  8.7409e-01],\n",
       "          [ 1.1523e+00,  1.9405e+00, -3.5638e-01,  ...,  1.1550e+00,\n",
       "            1.2631e-01, -1.1208e+00],\n",
       "          [-3.4954e-01,  2.0423e+00,  9.1586e-01,  ...,  4.5872e-01,\n",
       "           -1.5257e+00,  2.2547e-01],\n",
       "          [ 1.0186e-04,  1.4137e+00,  3.0313e-01,  ...,  5.5363e-01,\n",
       "           -7.0771e-01, -3.9553e-01],\n",
       "          [-1.0318e+00,  1.6335e+00,  7.0907e-01,  ...,  1.3139e+00,\n",
       "           -1.0500e+00, -9.6051e-01],\n",
       "          [ 1.8750e-01,  2.5195e+00,  3.3117e-01,  ..., -3.7295e-01,\n",
       "           -1.6397e-01,  7.7399e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.6888, -0.1220,  0.0583,  ..., -0.1614, -0.0519, -0.0682],\n",
       "          [ 0.1137,  0.1232, -0.1274,  ..., -0.2536, -0.3814, -0.0300],\n",
       "          [ 0.5965, -0.1115, -0.0157,  ...,  0.2252,  0.3615,  0.3515],\n",
       "          [ 0.0265, -0.4135,  0.1972,  ..., -0.3204,  0.2044,  0.4132],\n",
       "          [ 0.0998, -0.3044, -0.5739,  ..., -0.3834,  0.5481,  0.1083],\n",
       "          [ 0.7256, -0.0107,  0.0882,  ..., -0.3741, -0.0361, -0.0064]],\n",
       "\n",
       "         [[ 0.2817, -0.1418, -0.0332,  ..., -0.0043, -0.6268, -0.1629],\n",
       "          [ 0.0865,  0.4763,  0.8282,  ..., -0.3346,  0.1674,  0.3750],\n",
       "          [-0.3053, -0.3617,  0.0994,  ..., -0.2462,  0.5232, -0.4398],\n",
       "          [ 0.2658,  0.2496, -0.2445,  ..., -0.2105, -0.1172,  0.8804],\n",
       "          [ 0.1416,  0.5034,  0.3297,  ...,  0.1780,  0.4169,  0.2952],\n",
       "          [-0.2744, -0.2108,  0.4844,  ...,  0.4038,  0.2495, -0.1767]],\n",
       "\n",
       "         [[ 0.0756, -0.1996,  0.2204,  ..., -0.6018,  0.2539,  0.0377],\n",
       "          [ 0.4899,  0.2606,  0.4734,  ..., -0.7198,  0.0107,  0.0990],\n",
       "          [ 0.6116,  0.6925,  0.1729,  ..., -0.7169, -0.4260, -0.1345],\n",
       "          [ 0.6796,  0.0682, -0.1219,  ..., -0.6864,  0.0279, -0.2584],\n",
       "          [ 0.4267,  0.4582, -0.3418,  ..., -0.8409, -0.0274, -0.2747],\n",
       "          [ 0.6629, -0.0373,  0.4627,  ..., -0.4751,  0.3070, -0.2531]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3776,  0.7315, -0.1188,  ...,  0.0940, -1.0454, -0.1338],\n",
       "          [ 0.0328, -0.1539, -0.0585,  ..., -0.0356, -0.4006, -0.3786],\n",
       "          [-0.1157, -1.4314, -0.6729,  ..., -0.4139, -1.1189, -0.2123],\n",
       "          [-0.6002,  0.5058,  0.1804,  ...,  0.1189, -0.0896,  0.0463],\n",
       "          [ 0.7451, -0.1767,  0.0140,  ...,  0.4505, -0.3979, -0.0242],\n",
       "          [-0.1909, -0.9183,  0.3386,  ...,  0.1314, -0.8025, -0.3763]],\n",
       "\n",
       "         [[ 0.3376, -0.2081, -0.3444,  ...,  0.2393, -3.6649,  0.0240],\n",
       "          [ 0.0902, -0.1079,  0.2649,  ...,  0.5302,  0.1383, -0.0238],\n",
       "          [-0.2657,  0.2111, -0.1636,  ..., -0.2671, -0.2584,  0.0597],\n",
       "          [-0.0766,  0.3158, -0.3343,  ...,  0.2255,  0.0549,  0.3459],\n",
       "          [-0.0740,  0.1949, -0.1601,  ...,  0.2145, -0.1234,  0.0602],\n",
       "          [ 0.0977,  0.2125, -0.1099,  ..., -0.0145,  0.0252,  0.1644]],\n",
       "\n",
       "         [[ 0.0398, -0.1427, -0.0398,  ..., -0.1953,  0.2215, -0.0815],\n",
       "          [ 0.0916, -0.0555, -0.1958,  ...,  0.0073,  0.3752,  0.1137],\n",
       "          [-0.2821,  0.1940,  0.1472,  ...,  0.2909,  0.3102,  0.1354],\n",
       "          [ 0.0470,  0.1371, -0.0106,  ...,  0.0313,  0.0785, -0.0472],\n",
       "          [-0.1922,  0.4439,  0.3507,  ..., -0.1123,  0.3035,  0.0599],\n",
       "          [ 0.4182, -0.2686,  0.1924,  ..., -0.1562,  0.2410,  0.3107]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.3064e-01, -1.0979e+00,  3.2432e-01,  ..., -6.3491e-01,\n",
       "           -1.5534e-01, -3.2244e-02],\n",
       "          [ 9.1356e-01, -1.9732e+00, -1.5342e+00,  ..., -4.0318e-01,\n",
       "            6.9114e-01, -2.0584e+00],\n",
       "          [-1.6547e+00, -2.8462e+00,  3.2853e-01,  ..., -8.0076e-01,\n",
       "            4.7065e-01,  7.9458e-03],\n",
       "          [ 5.2406e-01, -3.6798e+00,  1.4178e-01,  ..., -1.8292e-01,\n",
       "            2.1573e-01, -4.9072e-02],\n",
       "          [ 8.4385e-01, -3.1234e+00,  2.5104e-01,  ..., -1.0628e+00,\n",
       "            3.6002e-01,  1.4938e-01],\n",
       "          [ 1.0584e-01, -3.1547e+00, -3.4994e-01,  ...,  1.8403e+00,\n",
       "            3.2077e-01, -5.0521e-01]],\n",
       "\n",
       "         [[-5.1276e-01,  2.7388e-01, -4.6331e-01,  ...,  1.2137e+00,\n",
       "           -5.0740e-01, -4.4837e-01],\n",
       "          [-1.1845e+00,  2.5824e-03, -2.0988e+00,  ...,  7.7741e-01,\n",
       "            1.1046e+00, -4.0853e-01],\n",
       "          [-1.8847e+00, -5.1615e-01, -1.1039e+00,  ..., -2.4228e-01,\n",
       "           -2.6114e-01,  1.2788e+00],\n",
       "          [-1.6678e+00,  2.3140e-02, -1.5047e+00,  ..., -2.1069e-01,\n",
       "            1.2064e+00,  3.7102e-01],\n",
       "          [-2.1296e+00, -4.5620e-01, -2.8393e-01,  ..., -5.6248e-02,\n",
       "            6.9919e-01,  1.0522e-01],\n",
       "          [-1.9136e+00, -1.1977e+00, -1.8097e+00,  ...,  6.9933e-02,\n",
       "            2.0639e+00, -2.0795e-01]],\n",
       "\n",
       "         [[ 1.2301e+00,  3.0167e+00,  3.7457e+00,  ...,  6.4829e-01,\n",
       "            1.6559e+00, -7.5835e-01],\n",
       "          [-3.5332e+00,  2.7201e+00, -3.1271e+00,  ..., -2.3347e+00,\n",
       "            3.7236e+00,  1.3975e+00],\n",
       "          [-2.4883e+00,  1.9978e+00, -3.6234e+00,  ..., -2.6884e+00,\n",
       "            2.9999e+00,  3.5145e-02],\n",
       "          [-4.0033e+00,  2.5719e-01, -4.9682e+00,  ..., -3.0074e+00,\n",
       "            3.1224e+00, -6.7949e-03],\n",
       "          [-2.7280e+00,  3.7905e-02, -3.3358e+00,  ..., -4.2488e+00,\n",
       "            2.6295e+00,  6.7595e-01],\n",
       "          [-2.6743e+00, -1.3148e+00, -3.6969e+00,  ..., -3.7045e+00,\n",
       "            2.6888e+00,  5.2857e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3653e+00, -2.7223e+00, -2.6976e+00,  ...,  9.1960e-01,\n",
       "            4.5927e-01,  2.6761e+00],\n",
       "          [-1.8721e+00,  2.2937e+00,  6.3503e-01,  ..., -5.0201e-01,\n",
       "           -2.4214e+00,  1.9656e-01],\n",
       "          [-3.1659e+00,  1.7722e+00,  1.1170e+00,  ..., -2.6509e-01,\n",
       "           -1.8794e+00, -5.1224e-01],\n",
       "          [-3.5464e+00,  2.8178e+00,  2.6299e+00,  ..., -1.0141e-01,\n",
       "           -2.5827e+00, -2.2415e+00],\n",
       "          [-2.5326e+00,  2.3707e+00,  1.3655e+00,  ..., -7.5593e-01,\n",
       "           -2.0564e+00, -1.6420e+00],\n",
       "          [-3.3097e+00,  3.2133e+00,  9.5118e-01,  ..., -2.0901e-01,\n",
       "           -2.1269e+00, -4.4490e-01]],\n",
       "\n",
       "         [[ 1.7415e+00,  4.4209e-01,  9.3136e-01,  ..., -2.7852e-03,\n",
       "           -9.8478e-01, -3.0201e-01],\n",
       "          [ 1.9936e+00,  1.1219e+00,  6.8835e-01,  ...,  3.0822e-02,\n",
       "           -1.7473e+00, -1.4582e+00],\n",
       "          [ 2.3752e+00,  9.4328e-02,  1.6681e+00,  ..., -7.6727e-02,\n",
       "           -1.1702e+00, -1.2695e+00],\n",
       "          [ 2.8562e+00,  4.1311e-01,  1.4480e+00,  ...,  4.1253e-03,\n",
       "           -1.4706e+00, -1.4856e+00],\n",
       "          [ 2.8780e+00,  3.6897e-01,  1.2427e+00,  ...,  1.3662e-01,\n",
       "           -1.0466e+00, -1.2149e+00],\n",
       "          [ 1.6901e+00,  4.0759e-01,  8.9699e-01,  ..., -2.2744e-01,\n",
       "           -1.7088e+00, -6.9268e-01]],\n",
       "\n",
       "         [[-2.6354e-01,  1.7693e-01, -5.4590e-01,  ...,  2.5397e-01,\n",
       "            2.7925e-01,  1.6319e-01],\n",
       "          [-9.4149e-01,  7.3126e-01,  1.2778e-01,  ..., -3.4195e-02,\n",
       "            7.7102e-01, -2.5772e-01],\n",
       "          [-8.1389e-01,  5.7675e-01, -4.6410e-01,  ..., -1.8066e-02,\n",
       "            5.8138e-01, -7.0576e-01],\n",
       "          [ 1.0448e-01,  2.2127e-01,  1.6736e-02,  ...,  9.1541e-01,\n",
       "            1.2172e+00,  1.9267e-01],\n",
       "          [-2.2847e-01,  6.8503e-01, -1.7095e-01,  ...,  3.1876e-01,\n",
       "            5.5788e-01, -2.2934e-01],\n",
       "          [-8.5155e-01,  4.0834e-01, -6.5728e-01,  ...,  1.7498e-01,\n",
       "            7.3826e-01,  1.1471e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-5.0390e-04,  4.8796e-03, -1.4501e-01,  ..., -7.6290e-03,\n",
       "           -2.8119e-02, -5.4427e-01],\n",
       "          [ 4.1592e-01, -1.6358e-01, -6.1516e-01,  ..., -1.1610e+00,\n",
       "           -3.3906e-01,  7.3473e-01],\n",
       "          [ 6.2024e-01, -6.7862e-01,  5.2781e-01,  ...,  1.8616e-01,\n",
       "           -8.3010e-01,  1.4301e+00],\n",
       "          [ 9.5459e-02, -6.7075e-01, -2.9875e-01,  ..., -2.6957e-01,\n",
       "            5.8320e-01,  2.3786e-01],\n",
       "          [ 5.9103e-01, -1.0900e+00, -2.9615e-01,  ..., -1.6540e-01,\n",
       "            2.0155e-02,  7.0819e-01],\n",
       "          [-2.4473e-02,  9.3317e-01, -1.8469e+00,  ...,  4.6240e-01,\n",
       "            2.7402e-01, -7.0872e-01]],\n",
       "\n",
       "         [[ 3.9716e-02,  1.0030e-02,  3.6718e-02,  ..., -4.8265e-02,\n",
       "           -3.5170e-03,  1.6775e-02],\n",
       "          [-1.1896e-01,  1.7423e-01,  1.9564e-02,  ..., -6.3828e-01,\n",
       "            1.1594e-01, -1.3074e-01],\n",
       "          [-1.9169e-01, -1.5611e-01,  1.2617e+00,  ...,  1.7678e-01,\n",
       "           -7.0348e-01, -4.7612e-01],\n",
       "          [-2.6220e-01,  7.5956e-02,  2.5373e-01,  ...,  2.8098e-01,\n",
       "            1.1321e-02, -2.2686e-01],\n",
       "          [-3.8030e-02,  3.6989e-01,  5.7103e-01,  ...,  6.7692e-01,\n",
       "           -5.5844e-01, -5.1019e-01],\n",
       "          [-1.6954e-01,  8.5752e-01, -3.7571e-01,  ..., -4.4276e-01,\n",
       "            1.2596e-01, -5.5268e-01]],\n",
       "\n",
       "         [[ 3.3389e-02, -7.5867e-01, -7.8789e-02,  ...,  5.0789e-02,\n",
       "            6.6967e-03, -5.6933e-02],\n",
       "          [ 1.8743e-01, -8.2430e-01,  1.6054e-01,  ...,  5.8465e-01,\n",
       "           -3.7226e-01,  4.5724e-01],\n",
       "          [-1.1302e-02, -1.7664e+00,  2.4033e-01,  ...,  6.5743e-01,\n",
       "           -5.4831e-02,  3.2146e-01],\n",
       "          [ 4.3034e-01, -1.1711e+00,  6.5769e-01,  ..., -3.1693e-02,\n",
       "            1.9255e-01, -4.4432e-01],\n",
       "          [ 2.8044e-01, -1.8505e+00,  3.5811e-01,  ..., -8.7326e-02,\n",
       "           -8.8667e-02, -7.8624e-01],\n",
       "          [ 5.8303e-01, -1.3734e+00,  1.5944e-01,  ..., -3.6586e-01,\n",
       "            5.8466e-02,  1.4301e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.6744e-02, -7.3889e-02,  1.3169e+00,  ..., -4.2897e-02,\n",
       "            1.9233e-01, -2.8341e-02],\n",
       "          [ 1.1231e-01, -5.2662e-01,  2.1252e+00,  ...,  4.1341e-01,\n",
       "            1.5472e-01,  4.6660e-01],\n",
       "          [-2.7699e-01, -1.7122e-01,  1.2199e+00,  ...,  4.3662e-01,\n",
       "            3.3560e-01, -2.7934e-01],\n",
       "          [-3.5777e-01, -5.5691e-01,  1.5952e+00,  ...,  1.0421e-01,\n",
       "            1.9397e-01,  3.1288e-01],\n",
       "          [ 4.0839e-02, -6.2763e-01,  1.3509e+00,  ...,  6.4152e-03,\n",
       "            6.2372e-02, -3.7363e-01],\n",
       "          [ 8.8768e-02, -6.5062e-01,  2.1222e+00,  ...,  5.1447e-01,\n",
       "           -1.7809e-01,  6.9206e-01]],\n",
       "\n",
       "         [[ 3.3704e-03, -1.1483e-01, -1.7243e-01,  ...,  1.5501e-01,\n",
       "            1.0659e-01,  1.5490e-01],\n",
       "          [ 7.2209e-01, -7.6462e-02, -4.1955e-01,  ...,  8.6717e-02,\n",
       "           -1.2081e+00, -7.5173e-01],\n",
       "          [-3.5769e-01, -9.3502e-01, -4.3178e-01,  ..., -5.2291e-02,\n",
       "           -6.1368e-01, -1.0596e-01],\n",
       "          [-2.2646e-01, -1.1651e-01, -8.0530e-02,  ...,  4.7340e-01,\n",
       "           -3.1638e-01, -3.5428e-01],\n",
       "          [-1.7844e-01, -6.5304e-01,  2.5734e-01,  ..., -6.7539e-02,\n",
       "           -6.0155e-01, -3.7664e-01],\n",
       "          [ 1.4303e+00, -2.3827e-01,  3.3473e-01,  ..., -5.0163e-01,\n",
       "           -8.7916e-01,  3.6498e-01]],\n",
       "\n",
       "         [[ 1.4329e-02,  2.1596e-02,  2.4439e-02,  ..., -6.4790e-02,\n",
       "            2.3542e-01,  4.6180e-03],\n",
       "          [-1.7783e-01, -5.5750e-01,  5.4701e-01,  ..., -7.5747e-01,\n",
       "           -1.9474e+00,  4.7547e-01],\n",
       "          [-1.1995e-01, -1.1207e-01, -8.6461e-02,  ..., -3.7288e-01,\n",
       "           -1.7793e+00, -2.0060e-01],\n",
       "          [ 3.4220e-02, -4.9380e-01, -3.5336e-01,  ..., -4.8544e-02,\n",
       "           -2.0096e+00, -2.2352e-01],\n",
       "          [-8.2879e-02, -5.9375e-01, -3.4359e-01,  ...,  3.3547e-01,\n",
       "           -1.8084e+00, -2.6900e-02],\n",
       "          [ 5.7030e-01, -3.4966e-01, -6.0843e-02,  ...,  1.3368e-01,\n",
       "           -1.1015e+00, -1.1745e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0278, -0.2285,  0.1759,  ..., -0.8720,  0.7339, -1.2083],\n",
       "          [-0.3331, -0.2673, -1.0965,  ...,  0.0762, -0.3753,  1.0802],\n",
       "          [-2.5590,  1.1161, -1.7027,  ...,  2.4981,  0.8645,  1.2272],\n",
       "          [-4.4928,  1.1419, -3.4351,  ..., -0.3177, -0.3557,  0.6577],\n",
       "          [-1.0369, -0.7761, -1.7631,  ...,  0.4129, -0.9025,  0.1944],\n",
       "          [-0.4675, -1.2673, -0.1179,  ...,  1.9035,  1.0250,  0.6972]],\n",
       "\n",
       "         [[ 0.7876,  0.2094,  0.0446,  ..., -0.1404, -1.0861, -0.1898],\n",
       "          [ 0.4443, -1.7008,  2.0655,  ...,  2.3038,  4.3517,  1.6300],\n",
       "          [ 0.3390, -1.5468,  0.3962,  ...,  0.6721,  5.2610,  1.2981],\n",
       "          [ 0.5788, -1.1720, -0.4110,  ...,  0.2269,  6.3949,  2.5675],\n",
       "          [ 0.6144, -0.8388, -0.7355,  ...,  0.8730,  5.3230,  2.2772],\n",
       "          [-2.1655, -0.2015,  0.0374,  ...,  0.2088,  4.5625,  1.6469]],\n",
       "\n",
       "         [[ 0.3675, -0.3501, -0.3418,  ...,  0.3457,  1.4536,  0.2457],\n",
       "          [ 0.8278, -5.4502, -1.4256,  ..., -3.0112, -1.8108, -5.9609],\n",
       "          [-0.7946, -5.8592, -2.0036,  ..., -2.8538, -2.9881, -6.9288],\n",
       "          [-0.7333, -7.3860, -1.1620,  ..., -4.7276, -1.9267, -8.5722],\n",
       "          [-1.2301, -6.1414, -0.8410,  ..., -5.3266, -2.8350, -5.4595],\n",
       "          [-1.4321, -6.4486, -2.0762,  ..., -3.1923, -1.2639, -5.4101]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2211,  1.7470,  0.5168,  ...,  0.2433,  0.4647, -1.6733],\n",
       "          [ 0.7547, -5.9362,  1.2807,  ..., -2.8052, -1.8204,  4.9073],\n",
       "          [ 2.2641, -5.3128,  1.9287,  ..., -2.5532, -0.6950,  6.5598],\n",
       "          [ 2.1684, -6.1221,  2.0379,  ..., -2.5539, -1.5841,  7.2189],\n",
       "          [ 0.1892, -5.5400,  1.6499,  ..., -3.1225, -2.0369,  6.2002],\n",
       "          [ 4.1612, -7.7373,  2.6797,  ..., -0.9817, -1.6717,  3.9505]],\n",
       "\n",
       "         [[ 0.0437, -0.0386,  0.1539,  ..., -0.1031, -0.0938, -0.1531],\n",
       "          [-1.5399, -1.4477, -0.0901,  ..., -1.8788,  0.1700, -1.3554],\n",
       "          [ 0.9560, -1.5641, -0.4879,  ..., -0.7166, -0.1774,  0.1505],\n",
       "          [ 0.4604, -0.6800, -0.6644,  ..., -1.2832, -1.0391, -0.1829],\n",
       "          [ 0.3427,  0.6657, -1.4619,  ..., -1.7414, -0.7250, -0.4485],\n",
       "          [-0.1315, -0.1621, -0.3967,  ..., -1.9611,  0.6277, -0.5944]],\n",
       "\n",
       "         [[ 0.4139, -0.0507,  1.8796,  ..., -0.2293, -0.1941, -1.0033],\n",
       "          [ 3.9689,  2.8258, -2.5711,  ...,  1.5037,  1.1603,  2.3677],\n",
       "          [ 3.4320,  1.5884, -2.1755,  ...,  2.0773, -0.2010,  6.0371],\n",
       "          [ 3.1715,  1.9318, -2.1000,  ...,  0.5385,  1.3558,  5.5863],\n",
       "          [ 2.5406,  1.1928, -1.7169,  ...,  0.8803,  0.3720,  5.1319],\n",
       "          [ 3.1278,  2.0482, -2.7541,  ...,  0.4907,  0.9622,  2.8476]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 3.9259e-02,  6.3955e-02, -4.0454e-03,  ...,  1.4827e-02,\n",
       "            1.0029e-01,  3.6431e-02],\n",
       "          [ 1.8991e-01, -1.2731e+00, -9.2660e-02,  ...,  4.1799e-01,\n",
       "           -1.1752e+00, -7.5972e-01],\n",
       "          [ 7.8742e-01, -1.3769e+00, -3.5817e-01,  ...,  6.1639e-02,\n",
       "           -9.6911e-01, -2.2850e+00],\n",
       "          [ 8.1955e-01, -8.0699e-01,  1.3534e-01,  ...,  4.1130e-01,\n",
       "           -1.0649e+00, -5.5523e-01],\n",
       "          [ 4.7345e-02, -7.1251e-01, -3.0951e-01,  ...,  3.6738e-01,\n",
       "           -1.1856e+00, -1.1522e+00],\n",
       "          [ 2.2304e-01, -5.7109e-01,  4.5297e-01,  ...,  2.7217e-04,\n",
       "           -1.2737e+00,  9.6880e-02]],\n",
       "\n",
       "         [[-3.9613e-02,  4.5612e-03,  8.0164e-02,  ..., -3.2315e-02,\n",
       "           -3.2279e-02, -4.7883e-02],\n",
       "          [ 4.5341e-01,  3.3753e-01,  3.4445e-02,  ..., -1.3454e-01,\n",
       "            7.3806e-02,  3.3579e-01],\n",
       "          [-3.3340e-01,  4.7654e-01, -6.6384e-01,  ..., -8.9859e-01,\n",
       "            8.9253e-01, -5.0896e-01],\n",
       "          [-1.5307e-01, -3.7818e-01, -3.5343e-01,  ...,  2.4622e-01,\n",
       "            1.3188e+00, -3.4875e-02],\n",
       "          [ 6.4462e-01,  1.1696e-01, -3.8616e-01,  ...,  2.7082e-01,\n",
       "           -3.4976e-02,  2.8384e-01],\n",
       "          [-5.8611e-02,  4.8391e-02,  1.3635e-01,  ...,  4.8457e-01,\n",
       "            5.5350e-01,  3.2094e-01]],\n",
       "\n",
       "         [[ 3.9839e-02, -1.0285e-01, -5.5149e-02,  ..., -2.7857e-02,\n",
       "            9.4383e-02, -1.5435e-01],\n",
       "          [-5.1908e-01, -1.9460e-01, -9.5136e-01,  ...,  2.8379e-01,\n",
       "           -1.7938e-01,  1.8253e-02],\n",
       "          [-4.6930e-02,  3.3352e-03, -6.2649e-02,  ..., -2.4447e-01,\n",
       "           -1.4365e+00, -3.3413e-01],\n",
       "          [-3.5188e-01, -4.0034e-02,  1.9160e-01,  ..., -1.3271e-01,\n",
       "           -5.6871e-01,  2.4549e-01],\n",
       "          [-3.1665e-02,  5.7820e-02,  1.5965e-01,  ..., -7.1699e-02,\n",
       "           -2.3608e-01,  1.2293e-01],\n",
       "          [ 2.3567e-01,  3.4344e-01, -4.0453e-01,  ..., -2.1092e-01,\n",
       "           -1.5545e-01,  1.8644e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9853e-02,  1.1956e-01,  4.3179e-03,  ..., -2.8044e-02,\n",
       "            7.0337e-02, -3.9722e-02],\n",
       "          [ 3.7397e-01, -5.7253e-01,  5.4016e-01,  ...,  3.3539e-01,\n",
       "           -2.7830e-01,  1.1768e-01],\n",
       "          [-5.4333e-01, -4.9339e-01, -7.4159e-01,  ...,  8.4516e-01,\n",
       "            1.9418e-01,  8.3214e-01],\n",
       "          [ 2.9885e-02,  7.6332e-02, -2.4620e-01,  ...,  3.1905e-01,\n",
       "            1.0337e-01, -1.8722e-01],\n",
       "          [ 6.0703e-01,  4.8446e-01, -4.3490e-01,  ..., -5.7642e-02,\n",
       "            1.2394e-01, -1.3965e-02],\n",
       "          [ 8.5518e-01, -6.2102e-01,  8.8462e-01,  ...,  1.0584e+00,\n",
       "           -6.0929e-02, -7.8893e-01]],\n",
       "\n",
       "         [[-1.6496e-01, -1.2725e-01, -9.1060e-02,  ..., -2.3799e-01,\n",
       "           -2.3474e-02, -4.3854e-02],\n",
       "          [ 2.4232e-01, -1.9817e-01, -3.7978e-01,  ...,  1.2084e+00,\n",
       "            1.2818e-01,  4.9704e-01],\n",
       "          [ 6.6212e-01, -6.3784e-01,  4.2212e-01,  ...,  5.7191e-02,\n",
       "           -3.4505e-01, -5.4202e-01],\n",
       "          [ 6.2087e-01, -1.6857e-01, -6.1306e-01,  ...,  5.1969e-01,\n",
       "            8.3028e-01,  8.0251e-01],\n",
       "          [ 8.4099e-01, -8.7853e-01,  2.8977e-01,  ..., -6.8800e-02,\n",
       "            3.5918e-01, -4.7324e-01],\n",
       "          [ 4.4478e-01,  4.9492e-01,  5.3217e-02,  ...,  4.3785e-01,\n",
       "           -4.2686e-01,  9.8263e-02]],\n",
       "\n",
       "         [[ 1.1979e-01, -6.6158e-02, -2.8034e-02,  ..., -1.4966e-02,\n",
       "           -8.9915e-02, -1.0581e-01],\n",
       "          [ 8.6998e-02,  7.0718e-01, -6.7852e-02,  ..., -2.2823e-01,\n",
       "           -5.9419e-01,  2.2255e-01],\n",
       "          [-2.4154e-01,  9.3151e-01,  9.7524e-01,  ...,  8.9035e-01,\n",
       "            5.2347e-01,  3.6025e-01],\n",
       "          [ 1.1113e-01,  6.2136e-01,  7.1452e-01,  ...,  6.0798e-01,\n",
       "           -8.2364e-02,  3.8202e-01],\n",
       "          [ 1.9849e-01,  4.1939e-01,  4.1173e-01,  ...,  5.4255e-01,\n",
       "           -5.2421e-01, -5.2228e-01],\n",
       "          [ 5.6982e-01,  1.3721e-02, -8.7658e-02,  ..., -5.3452e-02,\n",
       "           -7.6089e-02,  6.0818e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-8.7030e-01, -1.3107e-01,  3.3352e-01,  ..., -9.5781e-01,\n",
       "            2.2612e-02, -2.9464e+00],\n",
       "          [ 8.7902e-01,  8.4963e-01, -2.3842e+00,  ..., -1.0832e+00,\n",
       "           -3.3621e+00,  7.3047e+00],\n",
       "          [ 2.5570e+00, -1.9292e-01, -3.6957e+00,  ..., -1.7653e+00,\n",
       "           -2.9821e+00,  5.7028e+00],\n",
       "          [ 3.2000e+00,  5.5784e-01, -3.8734e+00,  ..., -2.8891e+00,\n",
       "           -2.4369e+00,  8.8679e+00],\n",
       "          [ 2.6192e+00, -8.6866e-01, -2.5924e+00,  ..., -4.9153e-01,\n",
       "           -1.8699e+00,  7.7619e+00],\n",
       "          [ 1.4101e+00, -1.6581e+00, -2.3390e+00,  ..., -1.8627e+00,\n",
       "           -1.3419e+00,  7.6398e+00]],\n",
       "\n",
       "         [[ 3.9207e-01, -8.7198e-02,  4.5695e-01,  ..., -1.4772e-01,\n",
       "           -5.9226e-02, -2.2404e+00],\n",
       "          [-9.2611e-01, -1.0826e+00,  2.9565e+00,  ..., -3.3355e-01,\n",
       "            6.7192e-02,  5.2791e+00],\n",
       "          [-2.1616e+00,  1.9853e-01,  4.6245e+00,  ..., -9.6174e-01,\n",
       "           -9.8421e-02,  5.8884e+00],\n",
       "          [-9.1392e-01,  9.6418e-01,  3.2869e+00,  ..., -3.2166e-01,\n",
       "            1.2705e-01,  5.8059e+00],\n",
       "          [-1.6106e+00,  3.5298e-01,  2.4765e+00,  ..., -1.1065e+00,\n",
       "           -6.0882e-01,  6.1943e+00],\n",
       "          [-1.0136e+00,  5.6778e-01,  3.9922e+00,  ..., -8.6237e-02,\n",
       "            3.4959e-01,  5.8089e+00]],\n",
       "\n",
       "         [[ 1.3705e-01, -6.6146e-01, -2.0491e-01,  ...,  1.4911e-01,\n",
       "            2.5535e-01, -1.7442e-01],\n",
       "          [ 9.6288e-01,  7.6793e-01,  1.0388e+00,  ...,  1.1536e+00,\n",
       "            6.9593e-01,  6.7704e-02],\n",
       "          [-1.4728e+00,  1.7598e+00,  5.5673e-01,  ...,  1.0653e-01,\n",
       "            4.2899e-01, -1.1887e+00],\n",
       "          [-5.3811e-01,  2.2843e+00,  9.2828e-01,  ...,  3.4505e-01,\n",
       "            2.8535e-01, -1.3492e+00],\n",
       "          [-9.5339e-01,  2.3791e+00, -5.3819e-01,  ...,  8.0040e-01,\n",
       "            3.3090e-01, -4.1655e-01],\n",
       "          [-3.7801e-01,  4.0328e+00,  1.8050e+00,  ..., -1.9449e+00,\n",
       "            5.8701e-01,  4.6330e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.7748e-01,  2.7054e-02, -1.0139e-02,  ...,  1.2542e+00,\n",
       "            6.6676e-02,  1.7786e+00],\n",
       "          [ 8.4195e-01,  3.3218e-01, -3.7188e-01,  ..., -2.4526e+00,\n",
       "           -1.0317e+00, -1.7486e+00],\n",
       "          [ 1.7698e-01, -1.0607e+00,  9.9471e-01,  ..., -3.0419e+00,\n",
       "           -2.4236e+00, -8.3646e-01],\n",
       "          [-5.7982e-02,  5.8938e-01,  1.0797e+00,  ..., -3.3608e+00,\n",
       "           -1.4379e+00, -4.7567e-01],\n",
       "          [ 7.0047e-01, -7.8186e-02,  3.5363e-01,  ..., -2.8939e+00,\n",
       "            3.4033e-02, -5.1257e-01],\n",
       "          [-3.7622e-02, -1.6520e+00, -2.2487e+00,  ..., -2.5476e+00,\n",
       "           -1.0566e+00,  2.4898e-01]],\n",
       "\n",
       "         [[-3.3143e-01, -1.4473e-01,  2.1405e-01,  ...,  2.5461e-01,\n",
       "           -2.4395e-02,  9.5475e-03],\n",
       "          [ 1.0049e+00, -3.7631e-02,  1.0678e+00,  ...,  8.0685e-01,\n",
       "            1.1032e+00, -6.8820e-01],\n",
       "          [ 8.1661e-01, -9.2361e-01,  9.9671e-01,  ...,  1.8909e-01,\n",
       "           -7.0582e-01,  5.0928e-01],\n",
       "          [-1.2870e+00, -1.5890e+00,  3.0541e-01,  ...,  9.1633e-01,\n",
       "            2.4561e-01,  5.8785e-01],\n",
       "          [-9.2093e-02,  8.4265e-01,  6.6369e-01,  ..., -1.2663e+00,\n",
       "            7.9312e-01,  5.0476e-01],\n",
       "          [-1.5674e-01, -6.8587e-01,  1.3185e+00,  ...,  4.3263e-01,\n",
       "           -6.7191e-01,  5.8436e-01]],\n",
       "\n",
       "         [[ 3.4086e+00,  2.1957e+00, -2.0634e+00,  ..., -2.8445e+00,\n",
       "           -3.8668e+00, -1.2118e+00],\n",
       "          [-3.9493e+00, -9.6458e-02,  6.3140e+00,  ..., -1.6963e+00,\n",
       "            9.7586e+00,  6.5454e-01],\n",
       "          [-5.6729e+00,  6.9446e-01,  4.3606e+00,  ..., -6.1152e-01,\n",
       "            7.5625e+00, -3.6090e+00],\n",
       "          [-5.6329e+00, -1.4410e-01,  6.4518e+00,  ..., -2.0334e+00,\n",
       "            9.0970e+00, -5.8691e+00],\n",
       "          [-6.4725e+00, -1.8927e+00,  5.7072e+00,  ..., -1.9997e+00,\n",
       "            8.6948e+00, -4.3690e-01],\n",
       "          [-6.3106e+00, -2.8265e+00,  7.9032e+00,  ...,  8.4011e-01,\n",
       "            8.6328e+00, -4.5446e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-8.1242e-03, -4.6875e-02,  2.8140e-02,  ...,  6.3446e-02,\n",
       "            3.7524e-02,  6.2208e-02],\n",
       "          [ 3.6318e-01,  4.0679e-01, -8.8457e-01,  ..., -8.7599e-01,\n",
       "           -6.2197e-01, -2.3575e-01],\n",
       "          [ 8.5350e-01, -5.2839e-02, -2.3945e-01,  ..., -1.4663e+00,\n",
       "            2.8418e-01, -5.2442e-01],\n",
       "          [ 2.2829e-01, -1.5523e-01,  4.1957e-01,  ...,  2.4479e-02,\n",
       "            1.0040e+00,  9.7725e-02],\n",
       "          [ 3.7979e-01, -1.4993e-01,  5.6792e-01,  ..., -4.7770e-01,\n",
       "            8.8467e-01, -4.2463e-01],\n",
       "          [-4.8506e-01,  9.6477e-01, -1.9094e-01,  ...,  8.7791e-01,\n",
       "           -1.7944e-01, -9.8096e-02]],\n",
       "\n",
       "         [[-7.9340e-02, -1.9839e-02, -1.3748e-01,  ..., -4.5362e-02,\n",
       "            4.7141e-02, -2.1514e-02],\n",
       "          [ 3.8464e-01,  4.4843e-01,  2.5690e-01,  ...,  6.0012e-02,\n",
       "           -7.9366e-01,  1.7406e-02],\n",
       "          [ 6.1341e-01,  2.5119e-01, -3.4738e-02,  ...,  1.4562e+00,\n",
       "            2.0921e-01,  5.9288e-01],\n",
       "          [-5.5456e-02,  8.3802e-01,  5.9265e-01,  ...,  1.1479e+00,\n",
       "           -2.0239e-01,  6.2810e-01],\n",
       "          [-8.0788e-02,  3.8712e-01, -2.4502e-01,  ...,  1.2002e+00,\n",
       "            8.4951e-01,  4.2897e-01],\n",
       "          [-7.5956e-01,  6.1994e-03,  1.5961e-01,  ..., -3.3697e-01,\n",
       "           -1.2363e-01,  6.2667e-02]],\n",
       "\n",
       "         [[ 6.4877e-02,  1.0147e-01,  9.0731e-02,  ...,  2.4853e-02,\n",
       "           -7.0408e-02,  2.1704e-03],\n",
       "          [-2.8727e-01,  1.0639e+00, -7.6741e-01,  ..., -1.6286e-01,\n",
       "           -2.7386e-02,  2.1888e-01],\n",
       "          [-3.1855e-01, -4.8112e-01, -9.9339e-01,  ...,  1.0322e+00,\n",
       "            1.3897e-01, -1.0586e+00],\n",
       "          [-1.8267e-01, -2.0410e-02,  4.1620e-02,  ...,  4.0875e-01,\n",
       "            6.2781e-01, -3.7567e-01],\n",
       "          [-9.0675e-02, -6.6817e-01, -2.4202e-01,  ...,  3.2046e-01,\n",
       "            1.2854e-01,  8.5499e-01],\n",
       "          [ 9.8853e-01,  3.5046e-01, -4.4697e-01,  ...,  4.6178e-01,\n",
       "            2.1417e-01,  1.0984e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.9640e-04,  8.0578e-02, -8.2013e-02,  ...,  4.4803e-02,\n",
       "            3.8873e-02, -1.2982e-01],\n",
       "          [ 9.7045e-01,  7.6598e-01,  2.0732e-01,  ..., -2.7961e-02,\n",
       "            3.3511e-01,  4.8941e-01],\n",
       "          [-5.9063e-01, -1.0315e-02, -1.2458e+00,  ..., -1.4574e-02,\n",
       "           -3.7085e-01, -2.2188e-01],\n",
       "          [-7.3626e-01, -5.7646e-01, -7.8813e-02,  ..., -3.4866e-01,\n",
       "            5.8743e-01,  2.7205e-01],\n",
       "          [-6.1321e-01,  4.9728e-01, -1.5303e-01,  ..., -9.1797e-02,\n",
       "            4.2255e-01,  2.3398e-01],\n",
       "          [-1.1490e-01,  2.6956e-01, -2.0141e-01,  ...,  2.1418e-01,\n",
       "           -8.1769e-01, -7.1658e-01]],\n",
       "\n",
       "         [[-1.2790e-01, -5.0633e-02,  1.0964e-01,  ..., -7.0735e-02,\n",
       "            5.3396e-02, -2.0372e-02],\n",
       "          [-3.0156e-01, -9.6380e-01, -5.5456e-02,  ...,  3.8136e-02,\n",
       "           -1.4608e-01,  3.0372e-01],\n",
       "          [ 1.3876e+00, -1.3314e-01, -9.1365e-01,  ...,  5.6340e-02,\n",
       "           -5.8987e-01, -3.5692e-01],\n",
       "          [ 5.3175e-01,  1.0018e+00, -1.0359e+00,  ...,  9.7843e-01,\n",
       "           -2.8411e-03, -1.7513e+00],\n",
       "          [-8.1012e-02, -2.9661e-01, -7.7939e-01,  ..., -4.2685e-01,\n",
       "           -1.1530e-01, -1.0762e+00],\n",
       "          [ 8.1398e-02,  4.0110e-02, -2.0533e-01,  ..., -5.6400e-01,\n",
       "           -7.5740e-01,  2.4241e-01]],\n",
       "\n",
       "         [[-1.7268e-02, -1.3456e-02, -2.0527e-02,  ..., -2.6838e-02,\n",
       "            9.7050e-03, -1.7938e-02],\n",
       "          [-7.7853e-01, -6.9514e-01,  2.7593e-02,  ...,  9.3046e-03,\n",
       "           -2.1480e-01,  2.6232e-02],\n",
       "          [-3.7959e-01, -4.0076e-01, -3.2661e-01,  ...,  6.0946e-04,\n",
       "            2.9713e-01,  2.3125e-01],\n",
       "          [-2.4309e-01, -6.0262e-01,  7.9452e-02,  ..., -8.4320e-02,\n",
       "            9.1148e-01,  3.8133e-01],\n",
       "          [ 5.7028e-01, -1.2057e-01,  2.6616e-01,  ...,  9.3208e-02,\n",
       "            3.8069e-01,  3.5125e-01],\n",
       "          [ 5.2606e-01, -9.5531e-02,  5.3116e-02,  ..., -3.7371e-01,\n",
       "            3.1919e-01,  2.9988e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.4765e-02, -2.8574e-01,  2.1615e-01,  ...,  1.6883e+00,\n",
       "           -2.1490e-01, -7.0574e-02],\n",
       "          [ 4.3964e-01,  2.0785e+00,  7.6777e-01,  ..., -4.4799e+00,\n",
       "            5.3344e-01, -9.9032e-01],\n",
       "          [-3.4987e-01,  2.1746e+00,  3.2859e-02,  ..., -3.6666e+00,\n",
       "            5.2859e-01, -1.2421e+00],\n",
       "          [-2.1154e-01,  5.0151e-01,  4.2212e-01,  ..., -3.9096e+00,\n",
       "           -2.1529e-01, -2.1130e+00],\n",
       "          [-5.0979e-01, -1.6848e+00, -3.7626e-01,  ..., -4.6883e+00,\n",
       "           -8.5417e-01, -1.4293e+00],\n",
       "          [-9.8255e-02, -2.2478e-01, -1.2809e+00,  ..., -4.7693e+00,\n",
       "           -2.5728e-01,  7.2117e-02]],\n",
       "\n",
       "         [[ 1.8058e-01,  9.8178e-01, -1.4240e+00,  ..., -1.2947e-01,\n",
       "            2.6640e-01,  9.1793e-01],\n",
       "          [-7.9507e-01, -5.2798e+00,  6.3241e-01,  ..., -1.5922e+00,\n",
       "            1.8192e+00, -1.4376e+00],\n",
       "          [ 3.1608e+00, -3.7142e+00,  1.2277e+00,  ...,  6.5908e-01,\n",
       "           -2.5099e-01, -7.1254e-01],\n",
       "          [ 9.9432e-01, -5.9269e+00,  5.9468e-01,  ...,  6.6274e-01,\n",
       "           -2.0753e-01, -1.4604e+00],\n",
       "          [-6.0658e-01, -2.9856e+00,  2.3118e+00,  ..., -5.9418e-01,\n",
       "           -1.3308e+00, -1.5515e+00],\n",
       "          [ 1.0982e+00, -3.4825e+00,  3.9355e+00,  ..., -8.0240e-01,\n",
       "           -1.3086e-01, -2.3725e+00]],\n",
       "\n",
       "         [[-6.6871e-01,  2.4777e-01, -4.1925e-02,  ...,  1.8080e-01,\n",
       "            4.8689e-02, -2.9203e-01],\n",
       "          [ 1.5419e+00, -1.1425e+00, -5.7651e-01,  ..., -1.4225e+00,\n",
       "           -1.7641e-01, -5.7116e-01],\n",
       "          [ 2.3449e+00, -1.4212e+00,  5.3825e-01,  ..., -6.4237e-02,\n",
       "            2.0660e-02, -1.7395e-01],\n",
       "          [ 2.2528e+00, -6.6243e-01, -6.3819e-01,  ..., -4.9181e-01,\n",
       "           -4.3109e-01, -1.3337e+00],\n",
       "          [ 2.6142e+00,  9.6379e-01, -6.4184e-01,  ..., -8.1082e-01,\n",
       "            1.0874e+00, -5.2358e-01],\n",
       "          [ 1.2401e+00, -2.3107e-01, -7.5833e-01,  ..., -1.0387e+00,\n",
       "           -1.0980e+00,  6.5876e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0074e-02,  1.1457e-01,  1.4191e-01,  ..., -9.7227e-02,\n",
       "            2.0769e-02,  1.5848e-01],\n",
       "          [-1.8722e-01,  1.7946e-01, -5.2563e-01,  ...,  2.2159e+00,\n",
       "           -1.6230e-01, -2.3987e-01],\n",
       "          [ 9.6092e-01, -7.9333e-01,  3.6926e-02,  ...,  1.1065e+00,\n",
       "           -1.0840e+00,  5.6770e-01],\n",
       "          [ 2.2618e+00,  4.5567e-02, -2.4633e-01,  ...,  7.6091e-01,\n",
       "            1.6837e-01,  1.0982e+00],\n",
       "          [ 2.5069e+00, -9.2461e-01, -1.9281e-01,  ...,  3.9250e-01,\n",
       "            2.2067e-01,  1.3920e+00],\n",
       "          [ 2.0683e+00, -1.4330e+00, -1.5020e-01,  ...,  4.5042e-01,\n",
       "           -2.9368e-02,  3.4225e-01]],\n",
       "\n",
       "         [[-3.0022e+00,  3.9174e-01, -1.1223e-02,  ..., -4.7967e-01,\n",
       "           -3.3196e-01,  1.2360e+00],\n",
       "          [ 5.4688e+00,  7.2843e-01, -3.0647e-01,  ..., -6.9659e-01,\n",
       "            2.0842e+00, -3.1210e-01],\n",
       "          [ 5.2985e+00,  9.3475e-01, -7.7734e-01,  ...,  4.9336e-01,\n",
       "           -2.4204e-02, -4.2473e-01],\n",
       "          [ 5.0319e+00,  3.5821e-01, -2.0218e+00,  ...,  3.5053e-02,\n",
       "            2.2460e-01, -8.7618e-01],\n",
       "          [ 4.9458e+00,  2.1131e-01,  6.0538e-01,  ..., -1.7904e+00,\n",
       "            1.1083e-01, -7.1199e-01],\n",
       "          [ 5.0568e+00,  1.2188e+00, -1.4366e+00,  ..., -1.5999e+00,\n",
       "            1.3549e-01, -7.7553e-01]],\n",
       "\n",
       "         [[-7.1125e-04, -2.4458e-01,  2.4069e-03,  ..., -1.7966e-01,\n",
       "            3.2623e-01,  7.5991e-02],\n",
       "          [ 4.7459e-01, -2.1584e+00,  5.7037e-01,  ..., -1.8233e+00,\n",
       "            1.2086e+00, -1.3540e+00],\n",
       "          [-2.0294e-01,  1.5010e-01, -1.8428e-01,  ..., -2.6651e+00,\n",
       "            3.9298e-01, -5.6551e-01],\n",
       "          [-2.3340e-01, -2.8843e-01,  8.1386e-01,  ...,  3.4288e-01,\n",
       "            1.0935e+00,  1.0450e-01],\n",
       "          [-6.5075e-01, -4.1424e-01,  8.8851e-01,  ...,  1.8196e-01,\n",
       "           -5.1766e-01,  8.6558e-01],\n",
       "          [ 5.1395e-01, -5.1572e-01, -1.4793e-02,  ...,  7.4257e-01,\n",
       "           -2.1675e-01, -1.0222e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.6154e-02, -2.4324e-02,  4.0457e-03,  ..., -5.0684e-03,\n",
       "           -4.3276e-02,  3.5572e-01],\n",
       "          [ 2.1075e+00,  3.3239e-01,  7.0590e-01,  ..., -6.4442e-01,\n",
       "            2.0325e-01, -1.1933e+00],\n",
       "          [ 2.8132e-01,  7.4110e-01, -5.7849e-01,  ..., -1.3484e+00,\n",
       "            8.1796e-01, -8.5377e-01],\n",
       "          [ 9.4673e-01, -2.0756e-01, -2.5264e-01,  ..., -2.9507e-01,\n",
       "            3.2167e-01,  4.0561e-01],\n",
       "          [ 1.2094e+00, -8.8893e-02,  2.3706e-01,  ...,  2.9906e-01,\n",
       "           -4.0728e-02, -9.2554e-01],\n",
       "          [ 1.3204e-01,  1.0660e+00, -9.9149e-01,  ...,  9.6109e-01,\n",
       "            1.5956e-01, -7.6533e-01]],\n",
       "\n",
       "         [[ 2.8516e-03, -2.1652e-02,  2.0662e-02,  ..., -1.2961e-02,\n",
       "            2.0125e-02, -4.7159e-03],\n",
       "          [ 1.0375e+00, -2.5208e-01,  1.1858e+00,  ..., -1.1227e-01,\n",
       "            1.6027e+00, -5.9035e-01],\n",
       "          [ 1.4201e+00, -7.5806e-01,  1.2572e+00,  ..., -5.0802e-01,\n",
       "            1.9032e+00, -4.0960e-01],\n",
       "          [ 1.0985e+00,  5.2307e-01, -9.2598e-01,  ...,  6.0425e-01,\n",
       "            1.1726e+00, -8.9815e-01],\n",
       "          [ 1.6638e+00, -9.4679e-01, -1.1589e+00,  ..., -1.2936e+00,\n",
       "            9.3144e-01,  2.0764e-01],\n",
       "          [-6.3733e-01, -8.2326e-02,  4.8729e-01,  ..., -1.0549e+00,\n",
       "            1.3904e+00, -4.2085e-01]],\n",
       "\n",
       "         [[-5.5537e-02,  4.8931e-03, -4.0918e-02,  ..., -3.8937e-02,\n",
       "            3.6256e-04, -8.0482e-02],\n",
       "          [-1.2110e+00, -5.9457e-01, -7.2941e-01,  ..., -6.5566e-01,\n",
       "            1.4824e-01, -1.2141e+00],\n",
       "          [-1.7088e+00,  6.0949e-01,  5.0842e-01,  ...,  8.7223e-02,\n",
       "            7.1390e-01,  1.1513e-01],\n",
       "          [-1.0148e+00, -8.6062e-02, -1.1542e-01,  ...,  8.5153e-01,\n",
       "           -6.6463e-01,  3.2984e-01],\n",
       "          [-2.6595e-01,  7.5610e-02,  5.1988e-01,  ...,  2.1095e+00,\n",
       "           -5.4382e-01, -2.6248e-01],\n",
       "          [-4.6993e-02,  4.0632e-01,  8.2436e-01,  ...,  9.5018e-01,\n",
       "           -2.2719e-01, -7.9116e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.2976e-01, -2.0310e-01, -7.0779e-02,  ..., -4.7672e-01,\n",
       "            2.2137e-01,  9.6389e-02],\n",
       "          [ 1.2666e+00, -1.3649e+00, -9.1768e-01,  ...,  3.2598e+00,\n",
       "            1.7509e-01,  4.1349e-02],\n",
       "          [ 2.3996e+00, -8.3760e-01, -5.3198e-01,  ...,  2.0222e+00,\n",
       "           -3.9857e-01,  1.4109e+00],\n",
       "          [ 6.9429e-01, -1.3008e+00,  2.3784e-01,  ...,  1.2453e+00,\n",
       "           -5.0874e-01, -1.7000e+00],\n",
       "          [ 1.5194e+00, -9.9902e-01,  3.8541e-02,  ...,  1.1380e+00,\n",
       "           -1.2560e+00, -2.0283e+00],\n",
       "          [ 2.7812e+00, -6.6949e-01, -1.0711e+00,  ...,  9.5535e-01,\n",
       "            5.5079e-01, -1.2989e+00]],\n",
       "\n",
       "         [[-8.1667e-02, -1.3310e-01, -4.5393e-02,  ..., -1.8094e-01,\n",
       "           -1.4460e-01,  1.1990e-01],\n",
       "          [ 1.9767e-01, -2.0686e-02,  7.5460e-03,  ..., -7.3771e-01,\n",
       "           -3.5278e-01, -1.8229e-01],\n",
       "          [-2.5061e-01,  8.3434e-01, -3.6206e-01,  ...,  3.2021e-01,\n",
       "           -5.0111e-02, -6.7287e-01],\n",
       "          [ 1.3885e-02, -5.5523e-01,  2.4228e-01,  ...,  7.5830e-03,\n",
       "           -4.7637e-01, -7.6021e-02],\n",
       "          [ 1.4443e-01,  4.5993e-01,  3.5707e-01,  ..., -8.1641e-01,\n",
       "            1.3231e+00,  1.8180e-01],\n",
       "          [-3.8470e-01, -1.9261e-02, -2.6191e-01,  ..., -2.1379e-02,\n",
       "           -6.0301e-02, -7.0865e-01]],\n",
       "\n",
       "         [[-3.1941e-02, -3.7102e-02,  8.7104e-02,  ...,  8.3825e-02,\n",
       "           -3.5994e-02,  2.2552e-02],\n",
       "          [-6.7529e-02,  7.2829e-01, -6.5636e-01,  ..., -1.1950e+00,\n",
       "           -3.0989e-01,  5.2998e-01],\n",
       "          [-1.6261e+00,  1.1138e+00, -7.8954e-01,  ..., -9.0982e-01,\n",
       "           -1.1568e+00,  1.3140e+00],\n",
       "          [ 1.4231e-01,  4.0466e-01, -5.0768e-01,  ...,  1.1708e-01,\n",
       "            6.6870e-01,  3.8124e-01],\n",
       "          [-1.1858e+00, -1.4331e-02, -6.7513e-01,  ...,  1.0442e+00,\n",
       "           -7.1258e-01,  2.4312e+00],\n",
       "          [-2.0989e+00, -1.3711e-01, -1.1852e+00,  ...,  7.2935e-01,\n",
       "            1.2567e-01,  3.2500e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.3348,  0.8583, -0.1585,  ...,  1.1293, -0.1629,  0.1378],\n",
       "          [-0.7339, -5.1293, -0.1342,  ..., -3.2350,  1.4706,  0.7144],\n",
       "          [ 0.5189, -3.2634,  0.8173,  ..., -3.3869,  0.6157,  1.2919],\n",
       "          [ 1.0170, -4.7487, -0.0811,  ..., -4.0765,  0.5272,  1.0693],\n",
       "          [ 0.1749, -4.4977,  0.1465,  ..., -4.5542,  0.7752,  0.5193],\n",
       "          [ 1.1501, -4.6922,  0.1796,  ..., -4.1334,  1.6590,  0.4772]],\n",
       "\n",
       "         [[ 0.0474,  0.8748, -0.6412,  ..., -0.0219,  0.2836,  0.0141],\n",
       "          [ 0.6127,  1.2520,  1.3016,  ...,  1.1617, -1.7080, -1.0072],\n",
       "          [-0.7112,  1.7044,  0.4178,  ...,  0.6778, -2.0701,  0.1313],\n",
       "          [-0.9872,  0.5684, -0.3798,  ...,  1.0324, -0.4697, -0.3029],\n",
       "          [-2.9806,  0.1110,  0.0705,  ...,  0.2435, -0.4895,  0.1770],\n",
       "          [-0.1473,  0.0788,  1.5045,  ...,  0.9007, -0.1450,  0.0879]],\n",
       "\n",
       "         [[-0.3118,  0.1428, -0.9919,  ..., -0.3561, -0.0489, -0.1319],\n",
       "          [-0.6023, -0.6286,  3.5674,  ..., -0.4814, -0.1837, -0.0893],\n",
       "          [ 0.1045,  0.1955,  3.6964,  ..., -0.2413,  0.2496, -0.2054],\n",
       "          [-0.5830,  0.0809,  2.4515,  ..., -0.2014, -0.4712,  0.1340],\n",
       "          [-0.9307,  0.5467,  2.8446,  ...,  0.2757,  0.4073, -0.8951],\n",
       "          [-0.2409,  0.8343,  2.2040,  ...,  0.5675,  0.5279, -0.1536]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3822,  0.0742, -0.0555,  ..., -0.0494,  0.2271,  0.0165],\n",
       "          [-0.0731, -0.8080,  1.4979,  ..., -0.3655,  1.4223, -0.3932],\n",
       "          [-0.4804, -1.9854,  0.2530,  ..., -0.6215,  2.0017, -0.5603],\n",
       "          [ 2.2720,  0.2035, -1.2692,  ...,  0.4753, -0.7717, -0.6821],\n",
       "          [ 2.5066, -0.9447, -0.9222,  ...,  1.6307, -0.1653, -1.3509],\n",
       "          [-0.8396,  0.1639, -0.8464,  ...,  0.3971,  0.3916,  0.5922]],\n",
       "\n",
       "         [[ 0.2081,  0.0614,  0.3197,  ...,  0.4175,  0.0096,  0.2301],\n",
       "          [ 0.1414,  0.6425,  0.6252,  ..., -1.5353,  0.1810, -0.3889],\n",
       "          [ 0.5764,  0.9186,  0.5785,  ..., -0.7435, -1.1918,  0.9236],\n",
       "          [ 1.1701,  1.5857,  0.9781,  ..., -1.8112, -0.2944,  0.5584],\n",
       "          [ 1.9596,  0.4805,  1.3256,  ..., -1.4809,  0.0808,  0.8787],\n",
       "          [ 1.7699, -0.4296,  0.6798,  ..., -0.8266, -0.0288,  0.5023]],\n",
       "\n",
       "         [[-3.0148,  0.5458,  0.5576,  ..., -0.9447,  0.3317,  0.1998],\n",
       "          [ 7.4210,  1.1603, -2.3412,  ...,  0.7059, -0.6426,  0.9819],\n",
       "          [ 6.6739,  0.3216, -3.0027,  ...,  1.1754,  0.0464,  0.1345],\n",
       "          [ 7.1160, -0.6402, -2.8950,  ...,  2.0299, -0.2694,  0.2465],\n",
       "          [ 6.8688, -0.1610, -2.7440,  ...,  2.4761, -0.6373, -0.0114],\n",
       "          [ 8.2566, -0.9423, -1.6011,  ...,  3.0561, -1.3424, -0.3850]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 3.6623e-02, -5.2531e-02,  1.9742e-02,  ..., -7.7511e-02,\n",
       "            7.2724e-04, -8.8710e-02],\n",
       "          [-9.7288e-01,  9.7550e-02, -1.2793e-01,  ..., -8.6592e-01,\n",
       "            9.6055e-01,  9.4705e-01],\n",
       "          [-5.9580e-02, -2.4644e-01,  4.6786e-01,  ..., -3.9647e-01,\n",
       "           -1.6205e-01, -6.8616e-02],\n",
       "          [ 2.0609e-01, -5.8221e-01,  2.9551e-01,  ...,  2.8551e-01,\n",
       "           -2.8799e-01,  1.5502e-01],\n",
       "          [ 6.7438e-01, -1.7384e-01,  8.2083e-01,  ...,  4.2029e-01,\n",
       "           -2.1918e-01,  4.3584e-01],\n",
       "          [ 3.6693e-01,  1.1228e-01, -2.8574e-01,  ...,  6.4347e-02,\n",
       "            2.7151e-01, -1.1678e+00]],\n",
       "\n",
       "         [[ 6.4450e-02,  2.1530e-02, -2.1496e-02,  ..., -2.4472e-02,\n",
       "            1.2992e-02, -4.4305e-03],\n",
       "          [ 2.3571e-01,  2.0767e-02, -1.3801e+00,  ...,  1.3692e+00,\n",
       "            4.9315e-01,  9.9846e-01],\n",
       "          [ 8.8027e-01,  1.5313e-01, -4.3236e-01,  ...,  1.1984e+00,\n",
       "            3.5652e-01, -1.0329e+00],\n",
       "          [ 1.2982e+00, -8.6139e-01, -8.1887e-01,  ...,  2.7215e-01,\n",
       "           -9.6900e-01,  5.6205e-01],\n",
       "          [ 7.6074e-01, -1.0951e+00, -9.5581e-01,  ..., -2.5352e-02,\n",
       "            3.3574e-01, -4.3918e-01],\n",
       "          [ 4.9171e-01,  1.0917e-02, -6.3083e-01,  ..., -2.2951e-01,\n",
       "           -2.8120e-01,  2.4871e-01]],\n",
       "\n",
       "         [[ 7.8684e-02,  1.9472e-02,  6.3902e-03,  ...,  1.7769e-02,\n",
       "           -6.6467e-02, -6.4331e-02],\n",
       "          [ 6.6447e-01,  1.5695e-01, -3.0897e-01,  ..., -2.5935e-02,\n",
       "            7.7874e-02,  7.9969e-01],\n",
       "          [ 1.0287e+00,  4.5207e-01,  3.8813e-01,  ..., -1.8722e+00,\n",
       "            1.2269e+00,  1.2773e+00],\n",
       "          [ 5.9027e-01, -5.7818e-02,  6.1746e-01,  ...,  4.7602e-01,\n",
       "            3.8448e-01,  6.5019e-01],\n",
       "          [ 2.5357e-01,  2.6608e-02,  1.6584e+00,  ...,  5.5198e-01,\n",
       "            3.8640e-01, -5.4974e-02],\n",
       "          [ 7.4704e-01,  7.7158e-01,  7.4647e-01,  ...,  6.2141e-01,\n",
       "           -2.9371e-01,  7.7368e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.2003e-03,  2.2984e-02,  2.6918e-02,  ..., -8.5574e-02,\n",
       "           -2.0333e-02,  2.4534e-02],\n",
       "          [-6.2101e-01, -3.4177e-01, -2.3836e-01,  ...,  1.5253e+00,\n",
       "           -5.2830e-01,  6.2624e-01],\n",
       "          [-3.1030e-01, -1.8390e+00,  5.1169e-01,  ...,  1.8316e+00,\n",
       "            3.3975e-01,  2.2786e-01],\n",
       "          [-8.3141e-01, -6.6812e-01, -3.8697e-01,  ...,  1.8976e+00,\n",
       "           -7.8898e-01, -2.5232e-01],\n",
       "          [ 3.5364e-02, -1.2332e+00, -1.1671e+00,  ...,  1.3737e+00,\n",
       "            4.1626e-01,  6.1326e-01],\n",
       "          [-8.7697e-01, -2.0272e+00, -4.6374e-01,  ...,  6.2547e-03,\n",
       "            1.9354e-01,  1.5355e+00]],\n",
       "\n",
       "         [[ 3.6519e-02, -1.5498e-02,  2.4342e-02,  ...,  3.0825e-02,\n",
       "           -7.8301e-03, -2.5399e-03],\n",
       "          [ 2.1977e-02,  5.4030e-01, -1.2397e+00,  ..., -5.6330e-02,\n",
       "            8.2130e-01, -8.4140e-01],\n",
       "          [-8.3100e-01,  6.3825e-01,  7.1269e-02,  ...,  8.6748e-01,\n",
       "           -8.4123e-01,  1.4391e+00],\n",
       "          [-1.3294e+00,  9.1927e-01, -1.1354e-01,  ...,  2.0057e-01,\n",
       "            7.9387e-01,  4.3085e-01],\n",
       "          [ 3.9758e-02,  8.9483e-01, -1.3220e-01,  ...,  1.2604e+00,\n",
       "            4.0333e-01,  5.0423e-01],\n",
       "          [-1.1594e+00,  1.0752e-01, -1.7591e+00,  ..., -9.9493e-02,\n",
       "            1.1894e+00, -8.6113e-01]],\n",
       "\n",
       "         [[ 7.5280e-02, -1.9492e-01, -7.6429e-02,  ..., -1.8599e-02,\n",
       "            1.9642e-01, -4.4872e-02],\n",
       "          [-5.2066e-01, -4.2676e-01, -1.4494e+00,  ...,  5.5262e-02,\n",
       "           -7.3229e-01,  3.5040e-01],\n",
       "          [ 5.8666e-01, -4.6790e-01,  3.0742e-01,  ...,  9.5054e-01,\n",
       "           -2.0766e-01,  2.5639e-02],\n",
       "          [ 9.9127e-01, -4.8155e-01,  3.8296e-01,  ...,  2.3706e-01,\n",
       "           -1.8117e-01,  1.0043e-01],\n",
       "          [-2.0021e-01,  9.6648e-01, -6.4060e-01,  ...,  5.0927e-02,\n",
       "            4.2149e-01,  3.4190e-01],\n",
       "          [-8.6246e-01, -9.6492e-01, -5.9173e-01,  ..., -2.5312e-01,\n",
       "           -3.2248e-02,  3.5306e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0622, -0.2432, -0.1308,  ...,  0.6292,  0.7119, -0.2988],\n",
       "          [-3.5855, -2.5353,  0.7704,  ...,  0.8290, -5.8205,  0.8902],\n",
       "          [-4.5516, -1.6983,  1.6707,  ..., -1.5326, -4.7720,  0.5554],\n",
       "          [-4.1969, -2.0093,  1.4361,  ..., -1.6766, -4.5112,  0.4021],\n",
       "          [-3.4159, -2.8172,  1.1218,  ..., -1.5955, -5.3064, -0.2075],\n",
       "          [-3.5702, -1.1972,  0.5770,  ..., -0.7356, -4.3539, -0.2407]],\n",
       "\n",
       "         [[-0.1469, -0.0684,  0.1629,  ..., -0.0474, -0.8726, -0.2045],\n",
       "          [ 0.1480, -0.2014, -0.4688,  ..., -0.0923, -0.2896, -1.5943],\n",
       "          [ 0.2454,  1.2025,  0.2665,  ...,  0.1631,  0.2424,  1.7197],\n",
       "          [ 0.3403,  0.4259,  1.0321,  ...,  0.0331, -0.4136,  1.9445],\n",
       "          [ 0.7645, -0.2740,  1.4969,  ..., -1.1928, -0.5547, -0.1132],\n",
       "          [ 1.2435,  0.3519,  1.6623,  ..., -0.8713, -0.7069, -1.0017]],\n",
       "\n",
       "         [[ 0.1925,  0.2996,  1.1201,  ..., -0.4592,  0.4389, -0.5034],\n",
       "          [-1.1269, -1.0395, -2.0093,  ..., -1.8079, -1.2817, -0.1274],\n",
       "          [-0.3585, -1.4230, -0.9391,  ..., -0.6311, -2.4615,  3.0645],\n",
       "          [-0.5267, -0.8266, -3.4025,  ...,  1.0848, -4.2802,  2.1817],\n",
       "          [-0.4130, -0.6084, -3.0601,  ...,  0.8434, -2.1143,  0.9375],\n",
       "          [ 1.6413, -0.3087, -3.0605,  ..., -0.1879, -2.0151,  1.8870]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1420,  0.0799, -0.2224,  ...,  0.0068,  0.1565,  0.0186],\n",
       "          [-2.2436, -0.3818, -1.8959,  ...,  1.4808, -1.6620, -1.2042],\n",
       "          [-1.0974, -1.2807, -2.0326,  ...,  0.5641, -0.3800, -0.3301],\n",
       "          [-2.0350, -2.2599, -0.3341,  ...,  0.8501, -0.2771, -1.2310],\n",
       "          [-1.7662, -1.9401,  0.4593,  ...,  0.7611, -0.6171, -0.5208],\n",
       "          [-1.3506,  0.9918, -0.7079,  ..., -0.8326, -1.0266, -1.5014]],\n",
       "\n",
       "         [[-0.3561, -2.1814,  0.1236,  ..., -0.0884, -0.0492,  0.9104],\n",
       "          [ 1.0831,  3.0001,  0.0141,  ..., -0.6536,  0.2618, -1.4213],\n",
       "          [ 0.3000,  1.6895,  0.0176,  ...,  0.3691, -1.0537,  1.1324],\n",
       "          [ 0.2500,  3.1956,  2.5603,  ...,  1.2410, -0.6513,  2.3614],\n",
       "          [ 0.7589,  2.4922,  2.0786,  ...,  0.9712, -1.6077,  0.6318],\n",
       "          [ 1.6294,  1.4620,  0.0352,  ...,  1.4463, -2.4197,  0.4050]],\n",
       "\n",
       "         [[ 0.3734,  0.0663, -0.1479,  ...,  0.6412,  0.1363,  0.2640],\n",
       "          [-0.9330, -1.3735, -0.6337,  ..., -1.2171,  1.0816, -1.2383],\n",
       "          [ 0.1042, -0.4612,  1.5085,  ..., -0.3796, -0.1181,  0.1064],\n",
       "          [-0.2165,  0.3483, -0.0445,  ...,  1.3134,  2.0872, -0.1321],\n",
       "          [-1.3408,  1.2955,  0.6045,  ..., -0.2379,  0.9021,  1.0555],\n",
       "          [-0.8258,  2.5911,  0.3349,  ..., -1.2408,  0.8646, -0.3570]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.4548e-02,  5.3397e-02, -6.4730e-02,  ..., -2.3077e-02,\n",
       "            5.7568e-05,  2.0528e-02],\n",
       "          [ 1.6396e+00, -4.8811e-01, -9.9514e-03,  ...,  1.7677e-01,\n",
       "           -4.7795e-01, -1.0079e+00],\n",
       "          [ 2.4722e-01,  3.7060e-01, -1.0250e+00,  ...,  2.0807e-01,\n",
       "           -4.6059e-01, -1.7041e-01],\n",
       "          [-5.4641e-01, -2.4882e-01, -7.4858e-01,  ...,  1.2140e-01,\n",
       "           -5.4370e-01, -3.7479e-01],\n",
       "          [-3.5210e-01, -2.4205e-01, -3.3063e-01,  ...,  2.7070e-01,\n",
       "           -2.3697e-01, -8.6972e-02],\n",
       "          [-5.0034e-01,  6.8837e-01, -4.0678e-01,  ..., -5.3676e-01,\n",
       "            2.5868e-01, -5.0757e-01]],\n",
       "\n",
       "         [[ 1.1421e-02, -2.3286e-02,  2.4031e-02,  ...,  1.9458e-02,\n",
       "           -5.3157e-02,  1.4022e-02],\n",
       "          [-3.8006e-01, -9.7725e-01, -1.2923e+00,  ...,  7.0721e-01,\n",
       "            3.2788e-01, -4.0794e-01],\n",
       "          [-6.9351e-01, -5.1183e-01, -1.2015e+00,  ..., -5.0793e-01,\n",
       "           -5.8926e-01, -1.0330e+00],\n",
       "          [ 6.9966e-01, -4.6243e-01, -6.3425e-02,  ..., -4.2045e-01,\n",
       "            3.1818e-01, -1.4097e+00],\n",
       "          [ 2.3885e-01, -1.1491e+00, -2.2676e-01,  ..., -1.1752e+00,\n",
       "           -6.6198e-01,  4.1947e-01],\n",
       "          [ 8.2530e-01, -4.4802e-01, -9.8024e-01,  ..., -2.0318e-02,\n",
       "            9.3066e-01,  6.1676e-01]],\n",
       "\n",
       "         [[ 4.1737e-02, -2.4639e-02,  5.5960e-02,  ...,  2.9685e-02,\n",
       "           -5.1147e-03, -2.1416e-03],\n",
       "          [-3.6082e-01,  1.1219e+00,  2.5839e-01,  ..., -1.1847e+00,\n",
       "            4.4165e-01, -3.9130e-01],\n",
       "          [ 8.1087e-01, -1.2216e+00, -1.8857e-01,  ...,  1.3553e+00,\n",
       "            8.4686e-01,  7.6864e-01],\n",
       "          [ 9.1158e-01, -9.2438e-01,  1.0119e+00,  ..., -1.0911e-01,\n",
       "           -1.0549e+00,  6.2259e-01],\n",
       "          [ 2.8784e-01, -3.6428e-01,  1.2719e+00,  ..., -1.4337e+00,\n",
       "            8.4332e-01, -4.5081e-01],\n",
       "          [-1.2517e-01, -4.5092e-01,  1.3617e+00,  ..., -8.1732e-01,\n",
       "           -1.6278e-01,  5.1874e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8734e-01,  7.9658e-02,  5.4667e-02,  ...,  4.3881e-02,\n",
       "            4.9607e-02, -1.2179e-01],\n",
       "          [-2.4291e-01, -2.3118e-01, -1.4433e-01,  ..., -2.6103e-01,\n",
       "            8.7648e-01,  1.4179e+00],\n",
       "          [-1.0027e-01, -4.3283e-03, -8.8811e-01,  ...,  1.8947e+00,\n",
       "            8.8203e-01, -5.5436e-01],\n",
       "          [ 1.0702e+00,  3.2932e-01, -9.9974e-01,  ...,  6.7583e-01,\n",
       "            1.0051e+00,  4.7331e-01],\n",
       "          [ 4.6893e-01, -5.2948e-01, -7.6411e-01,  ...,  3.7149e-01,\n",
       "           -2.9935e-02,  5.3914e-01],\n",
       "          [ 1.6174e-01,  4.8627e-01, -8.2387e-01,  ..., -5.8658e-01,\n",
       "            9.8113e-01,  1.2179e+00]],\n",
       "\n",
       "         [[-5.9185e-01, -1.7438e-05,  4.2870e-02,  ..., -1.6821e-02,\n",
       "            1.4827e-02, -6.8937e-03],\n",
       "          [-1.4579e+00, -1.5723e-01, -5.7404e-01,  ..., -1.8376e-01,\n",
       "            9.4737e-01,  4.8910e-01],\n",
       "          [-1.1836e+00, -5.4174e-01, -1.7196e+00,  ..., -1.4367e-01,\n",
       "           -1.5331e-02, -4.1854e-01],\n",
       "          [-1.5517e+00,  8.1629e-01, -3.3577e-01,  ...,  6.7102e-01,\n",
       "            4.5030e-01, -1.3986e+00],\n",
       "          [-9.2511e-01, -6.1454e-01, -8.1048e-01,  ...,  5.1572e-01,\n",
       "           -1.4547e+00,  8.0358e-01],\n",
       "          [-2.4591e+00,  5.9060e-01, -7.1154e-01,  ...,  2.3370e-01,\n",
       "            3.6587e-01,  8.9528e-01]],\n",
       "\n",
       "         [[-2.5289e-03,  8.2326e-02, -4.4994e-02,  ...,  5.7166e-02,\n",
       "            3.3988e-02, -4.4381e-02],\n",
       "          [-1.8215e+00,  6.7447e-01,  1.4626e-01,  ..., -2.0834e-01,\n",
       "           -6.8011e-01, -9.1083e-02],\n",
       "          [-4.6562e-01, -5.8764e-01,  4.3874e-01,  ...,  5.7693e-01,\n",
       "           -1.6715e+00, -9.1715e-01],\n",
       "          [-3.8917e-01, -1.2701e-01, -1.6339e-01,  ..., -9.0823e-01,\n",
       "           -5.5632e-01, -4.7635e-01],\n",
       "          [-1.0684e-01,  9.0816e-01,  5.1393e-02,  ..., -1.3657e+00,\n",
       "           -1.0962e+00, -6.7484e-01],\n",
       "          [-7.4926e-01, -1.6506e-01,  8.1514e-01,  ..., -7.9515e-01,\n",
       "            7.2082e-01,  1.8173e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.0372, -2.3316,  0.1670,  ..., -0.2257, -0.1957,  0.0710],\n",
       "          [-1.1617,  5.7819,  0.4649,  ...,  0.8789, -0.4382,  0.4594],\n",
       "          [ 0.2971,  4.6504, -0.2275,  ..., -1.1504,  0.3292, -0.0356],\n",
       "          [-0.0761,  4.3353, -0.3059,  ...,  0.0864, -0.6932, -0.5892],\n",
       "          [-0.3985,  4.5602, -0.1537,  ...,  0.2325, -2.0483, -0.1456],\n",
       "          [-2.0378,  4.0326,  0.7890,  ..., -0.7293, -0.1312, -0.4225]],\n",
       "\n",
       "         [[-0.7982,  0.2271,  0.4667,  ..., -0.5218,  1.0731,  1.1169],\n",
       "          [ 0.6056,  0.5853,  0.7010,  ..., -0.0171,  1.8413, -2.3594],\n",
       "          [ 0.2314, -1.2332,  1.8281,  ..., -0.1113,  1.2637, -0.1922],\n",
       "          [ 0.9562, -0.0725,  2.9488,  ..., -0.4942,  1.0327,  0.9051],\n",
       "          [ 1.3426,  1.1462,  1.4113,  ...,  1.1072,  1.6665, -0.3191],\n",
       "          [ 1.3724,  0.4694,  1.5526,  ...,  0.3311,  2.3320, -0.0982]],\n",
       "\n",
       "         [[-0.8517,  0.4646,  0.0205,  ...,  0.4969, -0.2337,  1.1502],\n",
       "          [ 0.4491, -2.2600, -0.1684,  ..., -0.7217,  0.4886,  0.7755],\n",
       "          [ 1.5638,  0.0630,  0.1468,  ...,  0.5792,  0.7467,  0.4655],\n",
       "          [ 1.4294, -0.4668,  0.1714,  ...,  0.5820,  0.8931,  0.8655],\n",
       "          [ 1.5642,  0.1228, -0.5454,  ...,  0.6193,  0.7867, -0.3255],\n",
       "          [ 0.2327, -0.7478, -0.1269,  ...,  0.3944,  0.4645,  0.0317]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3062, -0.1479,  0.1367,  ...,  0.1887,  1.7284, -2.8581],\n",
       "          [-0.3847, -2.4209, -2.0211,  ..., -0.0423, -5.5258,  6.0159],\n",
       "          [ 0.5601, -0.4120,  0.2471,  ..., -0.1924, -3.6420,  4.3287],\n",
       "          [-0.4400,  0.5435, -0.8990,  ..., -0.5892, -3.4134,  4.1278],\n",
       "          [-0.5743, -0.2087, -0.9569,  ..., -0.4020, -4.3810,  4.4570],\n",
       "          [-1.1415, -1.4433, -0.6136,  ..., -0.2260, -5.4817,  6.2460]],\n",
       "\n",
       "         [[ 0.1831,  0.3681,  0.2261,  ..., -0.2125,  0.0277, -0.1478],\n",
       "          [-1.0061,  0.1369, -0.7097,  ...,  1.4483,  0.3864,  0.1570],\n",
       "          [-1.3645,  0.2322, -0.6130,  ..., -0.1178,  1.1438,  1.0369],\n",
       "          [-0.6504, -0.0382, -0.3818,  ...,  0.8291,  0.9432,  0.2812],\n",
       "          [-1.0431, -0.5983, -1.1729,  ...,  1.1380,  1.0083, -0.0688],\n",
       "          [ 0.0469, -0.1255, -0.1479,  ...,  1.9392,  0.2393,  0.5683]],\n",
       "\n",
       "         [[ 0.3660,  0.1106,  0.6124,  ...,  0.5269,  0.5722, -0.3355],\n",
       "          [-0.6914, -1.7580, -1.0766,  ..., -0.9415, -3.9056,  0.5220],\n",
       "          [ 2.0799, -0.8335, -1.0468,  ..., -0.7635, -3.2339, -0.2528],\n",
       "          [ 2.5334, -1.2922, -0.2246,  ..., -0.0655, -3.7061, -0.1005],\n",
       "          [ 1.6189, -0.7401, -0.2353,  ..., -0.4852, -4.4108, -0.9910],\n",
       "          [ 0.4560, -0.7769,  0.2636,  ..., -0.8372, -4.1783,  0.1772]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 5.4477e-02, -1.0079e-02, -7.1997e-03,  ...,  1.2584e-01,\n",
       "           -6.7115e-02, -3.4897e-02],\n",
       "          [-1.3886e+00,  1.2505e-01,  1.5337e+00,  ...,  2.8175e-02,\n",
       "            1.3031e+00,  8.0599e-01],\n",
       "          [-1.3310e+00,  5.5521e-01,  1.8017e+00,  ..., -3.2921e-01,\n",
       "            1.1204e+00, -2.4966e-01],\n",
       "          [-1.2341e+00, -4.2294e-01,  8.5823e-01,  ..., -4.3371e-01,\n",
       "            4.3917e-01,  3.1447e-01],\n",
       "          [-1.9331e+00,  1.2241e-01, -2.8035e-01,  ..., -3.9943e-01,\n",
       "           -9.3256e-01,  2.0032e-01],\n",
       "          [-7.7111e-01,  2.2833e-01,  6.7183e-03,  ...,  5.9045e-01,\n",
       "           -6.7925e-01, -1.1975e-01]],\n",
       "\n",
       "         [[-6.2819e-04,  3.5647e-02,  4.8210e-02,  ..., -7.1052e-03,\n",
       "           -7.6824e-03,  1.6443e-03],\n",
       "          [-6.4987e-01, -7.6724e-01,  9.8571e-01,  ..., -1.4795e+00,\n",
       "           -9.2898e-01, -9.5714e-01],\n",
       "          [-6.5020e-01,  8.8941e-01,  5.7052e-01,  ..., -1.2096e+00,\n",
       "           -3.8296e-01,  3.8895e-01],\n",
       "          [ 6.1265e-02, -1.2388e+00,  1.4729e-01,  ..., -3.4133e-01,\n",
       "            8.3819e-01, -4.7657e-01],\n",
       "          [ 9.6014e-01, -7.0762e-02,  2.8882e-01,  ..., -8.7494e-01,\n",
       "            4.9408e-01, -9.4027e-01],\n",
       "          [-1.1471e+00,  1.0903e-01,  1.2771e-01,  ..., -7.6890e-01,\n",
       "           -3.1069e-01, -3.3888e-01]],\n",
       "\n",
       "         [[ 5.2694e-02, -4.0038e-02,  6.1282e-02,  ...,  4.4516e-02,\n",
       "           -6.2781e-02, -6.8190e-02],\n",
       "          [-4.9701e-01, -1.0709e-01,  4.2327e-01,  ...,  2.6530e-01,\n",
       "            1.9708e-01,  9.3207e-01],\n",
       "          [-2.2900e-01,  4.7560e-01,  1.1217e-01,  ...,  8.1871e-01,\n",
       "           -1.0781e+00, -2.2687e-01],\n",
       "          [-1.0416e+00,  8.5794e-02,  4.5858e-01,  ...,  2.5331e-01,\n",
       "           -6.7923e-01, -6.5123e-01],\n",
       "          [-1.4401e+00,  9.5121e-01,  9.3474e-01,  ...,  4.6582e-01,\n",
       "           -3.3549e-01, -6.1994e-01],\n",
       "          [-7.2515e-01,  7.1498e-01,  4.4392e-01,  ..., -9.4843e-01,\n",
       "            1.4603e-01,  2.3363e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.5199e-02, -4.4339e-02,  4.9220e-02,  ..., -8.3539e-02,\n",
       "            5.0388e-02,  7.1123e-03],\n",
       "          [-1.0189e-01, -8.8846e-01,  2.4153e+00,  ..., -4.0559e-02,\n",
       "            1.2145e+00, -5.5087e-01],\n",
       "          [-4.2768e-01,  9.2921e-01,  1.4781e-01,  ...,  1.0282e+00,\n",
       "            6.0841e-01,  2.2331e-01],\n",
       "          [ 4.2375e-01,  6.8117e-01, -7.8469e-01,  ...,  8.1687e-02,\n",
       "            1.7815e-01,  3.8728e-01],\n",
       "          [-1.1625e+00,  1.1358e+00, -5.9096e-01,  ..., -6.4872e-01,\n",
       "            1.0011e+00,  1.9870e-01],\n",
       "          [-6.8181e-01,  9.0723e-01,  3.8521e-01,  ..., -9.2200e-01,\n",
       "           -3.3153e-01,  2.9967e-01]],\n",
       "\n",
       "         [[ 1.3284e-01, -7.5167e-02,  1.2040e-01,  ...,  5.7788e-02,\n",
       "            2.8678e-02, -1.3395e-01],\n",
       "          [-1.6283e+00,  2.6347e-01, -5.2707e-01,  ...,  7.6813e-02,\n",
       "            5.4005e-01,  2.7726e-02],\n",
       "          [ 7.2956e-01,  3.8698e-01, -5.6285e-01,  ..., -3.7095e-01,\n",
       "           -2.0524e+00,  1.1800e+00],\n",
       "          [-5.1925e-01, -5.1365e-02, -5.4354e-01,  ..., -1.4309e+00,\n",
       "           -2.6063e+00,  3.4166e-01],\n",
       "          [-1.7410e+00,  2.3181e-01, -4.1315e-01,  ..., -6.1196e-01,\n",
       "           -1.4037e+00,  4.6274e-01],\n",
       "          [-1.1821e+00, -2.4767e-02, -1.6050e+00,  ..., -2.2606e+00,\n",
       "            4.9634e-01,  3.8726e-01]],\n",
       "\n",
       "         [[ 2.1119e-01, -4.4821e-02, -5.3789e-02,  ...,  4.3649e-02,\n",
       "            5.0222e-02,  2.0442e-02],\n",
       "          [-3.4639e-01, -1.1319e+00,  3.8547e-01,  ...,  6.3546e-03,\n",
       "           -9.0637e-01,  9.0627e-02],\n",
       "          [ 1.3015e+00,  8.2373e-01, -1.0245e+00,  ...,  1.1055e+00,\n",
       "           -5.2696e-01, -1.2284e+00],\n",
       "          [ 3.6545e-01,  4.1934e-01,  7.8836e-01,  ..., -1.2589e-01,\n",
       "           -2.7153e-01,  2.7081e-01],\n",
       "          [-1.0286e+00, -1.2389e+00, -5.0139e-02,  ...,  6.8383e-01,\n",
       "            1.3844e+00,  4.6353e-02],\n",
       "          [ 3.0799e-01,  1.5385e+00,  9.8440e-01,  ...,  1.4835e-01,\n",
       "           -2.8485e-01,  1.2651e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.0048e-02, -2.4519e-01, -4.4249e-01,  ...,  3.2120e-01,\n",
       "            3.1733e-01,  3.7760e-01],\n",
       "          [-4.7956e-01,  2.0112e-01, -6.2867e-01,  ...,  1.5845e+00,\n",
       "           -7.7267e-01, -1.5101e+00],\n",
       "          [ 1.5139e+00, -5.7630e-01, -1.7139e+00,  ..., -7.9521e-01,\n",
       "            1.1497e-01,  1.8011e+00],\n",
       "          [ 5.5933e-01, -1.1989e+00, -1.2590e+00,  ..., -1.1134e+00,\n",
       "            3.2362e-01,  7.3537e-01],\n",
       "          [ 5.0461e-01, -1.4871e+00, -1.2781e+00,  ..., -3.0407e-01,\n",
       "            8.9045e-01,  8.5736e-01],\n",
       "          [ 9.0054e-01, -8.9217e-01,  4.1878e-01,  ...,  2.8741e-01,\n",
       "            3.7797e-02,  3.5587e-01]],\n",
       "\n",
       "         [[-2.8365e-01,  1.6541e-01,  1.1532e-01,  ...,  4.8897e-02,\n",
       "           -1.1488e+00, -1.5094e-01],\n",
       "          [ 4.8387e-01, -1.4985e+00, -3.7533e-01,  ...,  9.8088e-01,\n",
       "            4.3543e-02, -2.4499e-01],\n",
       "          [-4.5029e-01,  3.4685e-01,  1.0547e+00,  ...,  7.1425e-01,\n",
       "           -1.5553e+00,  2.5462e+00],\n",
       "          [-6.8651e-01,  1.6106e-01,  1.1263e+00,  ...,  1.2565e+00,\n",
       "           -1.2150e+00,  1.9787e+00],\n",
       "          [-7.8180e-01,  6.8352e-01,  2.0586e-01,  ...,  4.7411e-01,\n",
       "           -1.4140e+00,  1.1738e+00],\n",
       "          [-2.4834e+00,  7.3927e-01,  3.5901e-01,  ..., -4.0280e-01,\n",
       "            2.0415e+00,  4.7683e-02]],\n",
       "\n",
       "         [[-1.2462e+00, -1.0842e-01,  5.5454e-01,  ..., -6.5962e-01,\n",
       "            4.7463e-01, -2.7957e-01],\n",
       "          [ 1.2219e+00, -1.3565e-02, -1.8268e-01,  ..., -1.0927e-01,\n",
       "            6.8062e-01, -3.8454e-01],\n",
       "          [ 2.2572e+00,  1.8302e+00,  4.9307e-01,  ...,  6.4262e-01,\n",
       "            2.9754e-01,  1.6313e-02],\n",
       "          [ 1.4665e+00,  1.1976e+00, -6.6109e-01,  ..., -6.4455e-01,\n",
       "            1.0975e+00,  5.7698e-01],\n",
       "          [ 8.6282e-01,  8.4190e-01, -3.1785e-01,  ..., -2.8177e-01,\n",
       "            1.3350e+00,  8.2584e-01],\n",
       "          [ 1.0206e+00,  9.6810e-01,  4.4090e-01,  ..., -1.0202e-01,\n",
       "            2.1476e-01,  9.2081e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.8925e-01, -9.1099e-01, -3.9517e-01,  ..., -1.0467e+00,\n",
       "           -4.1457e-01,  4.9478e-01],\n",
       "          [ 1.1100e+00, -1.0900e+00, -1.2293e+00,  ...,  2.4607e-02,\n",
       "            1.3240e+00, -9.5905e-01],\n",
       "          [ 5.0972e-01, -1.0040e+00, -2.2193e+00,  ...,  2.9366e-02,\n",
       "           -9.8022e-01, -1.5648e+00],\n",
       "          [ 8.4555e-01, -1.3755e+00, -1.0326e+00,  ..., -1.6109e+00,\n",
       "           -1.3422e+00, -6.9174e-01],\n",
       "          [-7.1119e-03, -8.8284e-01,  4.9570e-02,  ..., -6.5686e-01,\n",
       "           -2.3739e+00, -2.5799e-01],\n",
       "          [ 7.4398e-01, -1.2061e-01, -8.0183e-01,  ...,  1.6934e+00,\n",
       "           -9.3868e-01,  3.1875e-01]],\n",
       "\n",
       "         [[-9.1097e-01,  2.5492e+00,  3.2162e-01,  ...,  3.5468e-01,\n",
       "            1.9495e+00, -5.2428e-01],\n",
       "          [-6.2096e-04, -4.8603e+00, -6.1537e-02,  ...,  8.3505e-01,\n",
       "           -3.5077e+00,  2.9900e+00],\n",
       "          [ 3.5843e-02, -2.6148e+00,  1.2643e+00,  ...,  4.9090e-01,\n",
       "           -3.0041e+00,  1.0766e+00],\n",
       "          [-8.3419e-01, -3.0239e+00,  3.1975e-01,  ...,  5.6425e-02,\n",
       "           -2.9024e+00,  8.0498e-01],\n",
       "          [-1.5556e+00, -3.7139e+00,  6.3307e-01,  ...,  8.5211e-01,\n",
       "           -3.7726e+00,  1.5816e-01],\n",
       "          [-2.7506e-02, -3.1176e+00,  3.5587e-01,  ...,  3.5617e-01,\n",
       "           -4.3815e+00,  1.2116e+00]],\n",
       "\n",
       "         [[-2.0122e+00, -3.6749e-01, -1.1128e+00,  ..., -3.8888e-01,\n",
       "            5.8251e-02,  2.5243e-01],\n",
       "          [ 3.7357e+00,  4.8627e-01,  2.6720e+00,  ...,  2.7053e-01,\n",
       "            9.4404e-01,  1.5985e+00],\n",
       "          [ 1.8944e+00, -1.2611e+00,  4.0851e-01,  ..., -4.5994e-01,\n",
       "           -1.1304e-01,  7.7695e-01],\n",
       "          [ 2.7895e+00, -1.0498e+00,  9.3460e-01,  ..., -1.4733e+00,\n",
       "            1.8911e-01, -4.0969e-01],\n",
       "          [ 2.9585e+00, -5.4059e-01,  1.1896e+00,  ..., -1.3306e+00,\n",
       "            6.2093e-02, -5.2613e-01],\n",
       "          [ 3.5868e+00,  3.4205e-01,  2.5797e+00,  ..., -9.4570e-01,\n",
       "           -1.3436e-01, -4.2768e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-5.5811e-02, -8.1160e-02,  3.5665e-02,  ...,  1.0902e-01,\n",
       "           -2.7879e-02,  2.5281e-02],\n",
       "          [-9.0960e-01, -3.8609e-01,  1.2446e-01,  ...,  3.0909e-01,\n",
       "            1.1499e+00,  3.5506e-01],\n",
       "          [ 4.1554e-01, -1.5314e+00, -2.7755e-01,  ..., -2.4036e-01,\n",
       "            1.1545e+00,  9.3819e-01],\n",
       "          [ 3.0706e-01, -5.9753e-01, -6.0489e-01,  ..., -1.0958e-01,\n",
       "            5.6616e-01,  1.4927e-01],\n",
       "          [-5.4296e-01, -4.2924e-01, -3.3765e-01,  ...,  8.5746e-01,\n",
       "            2.1383e+00, -5.9858e-01],\n",
       "          [-2.6504e-01, -5.4263e-01, -3.3169e-01,  ...,  3.6797e-01,\n",
       "           -2.4309e-01, -2.5749e-01]],\n",
       "\n",
       "         [[ 1.6063e-02,  7.9706e-03, -3.7657e-02,  ...,  2.3601e-02,\n",
       "            1.4850e-02,  4.8284e-02],\n",
       "          [-6.0880e-02, -1.3131e+00, -6.0873e-01,  ...,  1.8584e-01,\n",
       "            3.6074e-01, -3.8274e-01],\n",
       "          [ 9.3150e-01,  9.5213e-01, -7.8978e-01,  ...,  1.7790e+00,\n",
       "            7.8821e-01, -4.9302e-01],\n",
       "          [ 3.0214e-01,  3.1645e-01, -1.3880e+00,  ...,  3.0687e-01,\n",
       "            4.8124e-02, -1.1341e+00],\n",
       "          [-6.2257e-01, -2.2575e-01, -9.5893e-01,  ...,  6.6574e-01,\n",
       "            7.3010e-02, -1.8797e+00],\n",
       "          [-8.2747e-02, -8.1084e-02,  3.8091e-01,  ...,  6.2206e-01,\n",
       "            1.1608e+00, -3.7564e-01]],\n",
       "\n",
       "         [[ 4.5020e-02,  4.0703e-02, -8.5913e-02,  ..., -2.0362e-03,\n",
       "           -1.8462e-02,  1.6402e-04],\n",
       "          [ 1.6720e-01,  4.8971e-01,  3.9579e-01,  ..., -2.8611e-03,\n",
       "           -6.2666e-01, -1.1864e+00],\n",
       "          [-5.8589e-01,  4.1150e-01, -4.4853e-01,  ...,  3.7865e-01,\n",
       "            9.8872e-01,  2.0892e-01],\n",
       "          [ 4.4636e-01, -7.6701e-01,  5.6126e-02,  ..., -1.2943e-01,\n",
       "           -3.9370e-01, -1.1772e+00],\n",
       "          [ 1.0689e+00, -4.2474e-02,  3.8979e-01,  ..., -7.6389e-01,\n",
       "           -5.5940e-01, -1.0808e+00],\n",
       "          [ 1.4033e+00,  3.9555e-01, -6.6537e-01,  ...,  7.5821e-02,\n",
       "            2.1800e-01, -7.7442e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.0052e-03,  3.0031e-02, -6.7098e-03,  ..., -2.2071e-02,\n",
       "           -2.5694e-02,  3.6145e-02],\n",
       "          [ 5.0416e-01,  1.1350e+00,  3.9957e-01,  ...,  5.8379e-02,\n",
       "           -1.8627e-01,  5.7473e-01],\n",
       "          [ 9.8570e-01, -1.4251e+00, -2.0447e+00,  ..., -1.0428e+00,\n",
       "           -1.7868e-01, -1.2913e+00],\n",
       "          [ 6.4359e-01,  4.9231e-02, -1.1532e+00,  ..., -3.0770e-02,\n",
       "            9.4991e-01, -1.4975e-01],\n",
       "          [-1.8275e+00,  5.3913e-01, -3.8070e-01,  ...,  8.4557e-02,\n",
       "           -8.7357e-02, -1.8106e-01],\n",
       "          [ 3.3765e-01, -1.3169e-01,  3.7519e-01,  ..., -6.3592e-01,\n",
       "            4.0733e-01, -8.7381e-01]],\n",
       "\n",
       "         [[-6.5377e-02, -4.5243e-02,  2.9972e-02,  ...,  1.7289e-02,\n",
       "           -4.0501e-02, -1.0150e-01],\n",
       "          [ 3.3697e-01,  1.1677e-01, -8.0096e-01,  ...,  7.6040e-01,\n",
       "            1.3982e-01, -1.2177e-01],\n",
       "          [ 7.5692e-02, -1.9914e-01,  5.1068e-01,  ..., -8.3964e-01,\n",
       "           -6.9331e-01, -5.3009e-01],\n",
       "          [ 9.8713e-01,  1.0488e+00, -5.8275e-01,  ...,  4.4845e-01,\n",
       "            3.5765e-01,  1.8527e-01],\n",
       "          [ 7.1748e-01, -1.1359e-01, -7.5193e-01,  ...,  5.4481e-01,\n",
       "            1.1482e-01,  4.7524e-01],\n",
       "          [-9.1947e-01,  5.8532e-01,  2.4564e-01,  ...,  1.9438e+00,\n",
       "            7.9026e-02,  1.4179e+00]],\n",
       "\n",
       "         [[-1.1829e-02,  3.3011e-02, -4.7851e-02,  ..., -7.7909e-03,\n",
       "            1.3503e-02,  1.1945e-02],\n",
       "          [ 9.6119e-02, -3.9244e-01, -6.0998e-01,  ..., -1.6956e-01,\n",
       "            1.0601e+00, -3.7701e-01],\n",
       "          [ 5.1655e-01, -1.1518e+00, -3.1415e-01,  ...,  2.4800e+00,\n",
       "           -2.3997e-01, -1.6880e-01],\n",
       "          [-6.0519e-01,  1.9115e-01, -8.0738e-01,  ...,  8.2899e-01,\n",
       "           -2.0062e-01,  4.9299e-01],\n",
       "          [-6.4246e-01, -3.4727e-01, -3.0784e-01,  ...,  4.9420e-01,\n",
       "           -1.7693e-01,  1.9945e-01],\n",
       "          [-3.6404e-02, -1.5328e+00, -6.7074e-01,  ...,  1.9174e-01,\n",
       "            1.0277e+00, -5.5909e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5105,  0.4888, -0.8554,  ..., -1.0267, -1.3161,  0.2115],\n",
       "          [ 0.2877,  1.1726,  0.3502,  ...,  1.8822,  0.3735, -1.5771],\n",
       "          [-0.4424,  1.2662,  1.0167,  ...,  0.1037,  0.4777, -0.9544],\n",
       "          [-0.5534,  0.3274,  0.4123,  ...,  2.1440,  0.4541, -0.2127],\n",
       "          [-1.3232,  0.9199, -0.6839,  ...,  0.7245,  0.9266, -0.4656],\n",
       "          [ 0.0630,  0.9778,  0.5695,  ...,  1.3222,  0.5134, -0.1806]],\n",
       "\n",
       "         [[ 0.8641, -2.0632,  0.1507,  ...,  0.2358, -2.4628, -0.4365],\n",
       "          [ 0.9688,  2.2874,  0.0156,  ...,  0.7563,  1.3243,  0.4979],\n",
       "          [ 0.7073,  0.1989,  0.7418,  ...,  2.2389, -2.1909, -0.4385],\n",
       "          [ 0.4631,  1.2719,  1.6549,  ...,  1.0307,  0.1528, -0.6475],\n",
       "          [ 0.7245,  2.4511,  0.4049,  ...,  0.8818,  1.9068, -0.5133],\n",
       "          [ 1.6705,  1.3079, -0.5537,  ...,  0.4417,  1.5376, -0.9358]],\n",
       "\n",
       "         [[ 1.0047,  0.3677, -0.1759,  ..., -0.8085, -1.3894, -0.3705],\n",
       "          [-1.7129, -0.7999, -0.7384,  ...,  1.3115, -0.2055,  0.0450],\n",
       "          [-1.7283,  1.2728, -0.1777,  ...,  1.1961, -0.2905, -0.0705],\n",
       "          [-0.6036,  0.1837,  0.0374,  ...,  1.8050,  0.0517,  0.0720],\n",
       "          [-1.0830,  0.1748,  0.1691,  ...,  0.5526, -0.5439,  0.3037],\n",
       "          [-0.0446, -0.7160,  0.4211,  ...,  0.8922, -0.1115, -0.3300]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2270, -0.5682,  0.4831,  ..., -0.6640,  1.0591,  0.2766],\n",
       "          [-0.4265,  1.3822, -1.1272,  ..., -2.1121, -1.1342, -0.3716],\n",
       "          [-0.3630,  1.8137, -1.5860,  ..., -1.2103, -2.0567,  2.0846],\n",
       "          [-1.4367,  2.7929, -1.9303,  ..., -1.8125, -2.4616,  1.7783],\n",
       "          [-1.3206,  3.0734, -2.0488,  ..., -1.2306, -2.4862,  1.5271],\n",
       "          [-0.4272,  0.6264, -0.5921,  ..., -3.3172, -0.3580,  0.8858]],\n",
       "\n",
       "         [[ 0.2557,  0.5486,  0.5427,  ...,  0.7337,  0.0735,  0.8409],\n",
       "          [-0.2630,  3.1089,  0.0968,  ...,  1.1111, -1.2343,  0.9462],\n",
       "          [-1.7111,  1.0336, -1.1075,  ..., -0.1216,  1.0755,  0.5210],\n",
       "          [-0.7910,  0.4252, -0.2779,  ...,  0.1207, -0.0782,  1.1446],\n",
       "          [-1.3934,  0.4443,  0.1709,  ..., -0.0378, -0.3023,  1.7022],\n",
       "          [ 0.5238,  0.7138,  0.8931,  ..., -0.7931, -0.5208,  1.2751]],\n",
       "\n",
       "         [[-0.7236,  0.3119, -1.5837,  ..., -0.3841,  0.2253, -1.3224],\n",
       "          [-1.1716, -0.2108, -0.5625,  ..., -1.0099,  0.2504,  0.5097],\n",
       "          [ 0.9571,  0.1792,  0.4043,  ..., -0.4605,  0.2877, -1.1319],\n",
       "          [ 1.9032, -0.1706,  0.9789,  ..., -0.3471, -0.3716,  0.6636],\n",
       "          [ 1.1902,  0.4560,  0.2871,  ..., -0.6818, -0.8303,  2.2144],\n",
       "          [ 0.1459,  0.2501,  0.7950,  ...,  1.1997,  1.3652, -0.9925]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.8845e-02,  4.6500e-02, -6.7226e-02,  ...,  6.0936e-02,\n",
       "           -2.3287e-02, -9.3406e-02],\n",
       "          [-9.4938e-01, -7.0013e-01,  1.5670e+00,  ..., -7.2562e-01,\n",
       "           -3.4044e-01,  7.6453e-02],\n",
       "          [-2.6040e-01, -1.5089e+00, -9.5186e-01,  ..., -9.7730e-01,\n",
       "            1.1327e+00, -1.1046e+00],\n",
       "          [-8.0018e-01, -1.2850e-01, -7.2774e-01,  ..., -6.6072e-01,\n",
       "            2.7515e-01,  6.7423e-02],\n",
       "          [-8.8089e-01,  3.2091e-01, -2.8150e-02,  ...,  1.3322e-01,\n",
       "            8.0330e-01,  4.5479e-01],\n",
       "          [ 7.0085e-02,  1.3141e-01,  8.7364e-01,  ..., -4.7532e-01,\n",
       "            7.3325e-01, -6.2032e-01]],\n",
       "\n",
       "         [[ 5.9533e-02,  7.3243e-04,  3.6023e-02,  ..., -3.3076e-02,\n",
       "           -2.6139e-02, -2.8997e-03],\n",
       "          [ 7.9582e-01, -4.8593e-02, -4.5047e-02,  ...,  2.7073e-01,\n",
       "            3.3371e-01,  7.6484e-01],\n",
       "          [-5.7428e-01, -2.5242e+00,  6.9282e-01,  ...,  7.8759e-01,\n",
       "            9.8245e-01,  1.1367e+00],\n",
       "          [-8.0777e-01, -4.2712e-01,  4.6215e-01,  ...,  2.1518e+00,\n",
       "            6.8225e-02, -6.4745e-02],\n",
       "          [-1.0937e+00, -6.1398e-01, -7.5256e-01,  ...,  4.2730e-01,\n",
       "            2.1543e-01,  8.8168e-01],\n",
       "          [ 3.4092e-01,  9.3562e-01, -1.7594e-01,  ...,  3.1151e-01,\n",
       "           -3.0346e-01,  1.1093e+00]],\n",
       "\n",
       "         [[-1.0822e-02,  1.1039e-02, -2.9605e-02,  ...,  1.4061e-02,\n",
       "            3.5705e-02,  5.5029e-02],\n",
       "          [ 2.8251e-02,  1.3149e-01,  6.0443e-01,  ...,  2.2485e-01,\n",
       "           -9.9927e-01, -2.6869e-01],\n",
       "          [-1.4289e+00, -4.7748e-01, -5.8321e-01,  ...,  9.8525e-01,\n",
       "           -2.1011e-01, -1.4174e+00],\n",
       "          [ 1.8501e-01,  7.1037e-01, -3.3176e-01,  ...,  2.4591e-01,\n",
       "           -8.8258e-02, -1.2994e-01],\n",
       "          [ 1.0368e-01,  8.2482e-02, -6.6023e-01,  ...,  1.0176e+00,\n",
       "            7.4973e-01, -1.8706e-01],\n",
       "          [-2.2340e-02, -7.8211e-01,  1.0343e+00,  ..., -7.2567e-02,\n",
       "            2.4552e-01, -9.6137e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.8938e-02,  3.7375e-02, -1.7196e-03,  ..., -4.7671e-02,\n",
       "           -2.1496e-02,  2.7531e-02],\n",
       "          [ 1.6518e-01,  7.4135e-01, -7.2497e-01,  ...,  2.1972e-01,\n",
       "           -4.6066e-01,  1.2030e-01],\n",
       "          [-6.2015e-02, -3.8734e-01,  4.3369e-01,  ..., -4.1152e-02,\n",
       "           -3.0381e-01, -5.8787e-01],\n",
       "          [ 1.0416e+00,  1.3798e-02, -3.4068e-01,  ...,  7.4623e-01,\n",
       "            7.5903e-01,  3.7144e-01],\n",
       "          [ 7.1661e-01,  9.7824e-01, -1.9671e-01,  ...,  3.1868e-01,\n",
       "            3.7599e-01,  3.4114e-01],\n",
       "          [-7.9523e-01, -9.1268e-01, -6.1249e-01,  ...,  6.1174e-01,\n",
       "           -8.3486e-01,  6.1979e-01]],\n",
       "\n",
       "         [[ 8.4550e-02,  2.7683e-02,  6.4263e-02,  ...,  2.7156e-02,\n",
       "            4.6119e-02,  9.7694e-03],\n",
       "          [-6.1208e-01,  3.6774e-01,  5.9525e-01,  ..., -2.3135e+00,\n",
       "            6.9887e-01, -1.3180e+00],\n",
       "          [ 1.2746e+00,  8.3069e-02,  3.0892e-01,  ...,  8.3676e-01,\n",
       "           -8.6151e-01,  1.3476e-01],\n",
       "          [ 8.7542e-02, -3.0672e-02, -4.3851e-01,  ...,  2.8738e-01,\n",
       "           -3.8462e-01,  4.2904e-01],\n",
       "          [-1.0170e+00, -1.5753e-01,  1.4123e-01,  ...,  9.2197e-02,\n",
       "           -1.1485e+00,  3.3970e-01],\n",
       "          [ 6.5623e-01,  3.1281e-01,  2.0340e+00,  ...,  2.2594e-01,\n",
       "           -1.0366e-01, -1.2434e+00]],\n",
       "\n",
       "         [[-1.1082e-01,  2.3628e-02, -5.7385e-02,  ..., -9.5812e-02,\n",
       "            5.7014e-02, -1.9716e-02],\n",
       "          [ 1.0451e+00,  1.6790e-01, -2.4514e-01,  ...,  7.5939e-01,\n",
       "            1.0304e+00,  7.7789e-01],\n",
       "          [ 4.2581e-01,  6.2875e-01,  7.7112e-01,  ..., -4.0835e-01,\n",
       "            1.7963e+00,  1.3712e-01],\n",
       "          [ 3.6507e-01, -1.6319e-01,  1.4042e+00,  ...,  3.2552e-01,\n",
       "           -6.1401e-01,  1.2750e+00],\n",
       "          [ 6.8312e-01,  1.3630e-01,  7.0274e-01,  ..., -4.7928e-01,\n",
       "            1.4291e+00, -1.8211e-01],\n",
       "          [ 7.1933e-02,  1.7649e-01,  1.7782e-01,  ..., -2.1952e-01,\n",
       "            4.8524e-01,  2.1737e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7216, -0.3013, -0.2952,  ...,  0.1865,  0.3256, -0.5023],\n",
       "          [-0.1957, -0.1786, -0.9112,  ...,  1.1172, -0.7191, -0.0284],\n",
       "          [ 0.7248,  0.5007, -0.2639,  ...,  2.6659, -1.0404, -0.4159],\n",
       "          [ 0.2425,  0.5207, -0.9356,  ...,  1.7922, -1.3903, -0.6294],\n",
       "          [ 0.4712,  0.1115, -0.0415,  ...,  1.5392, -1.8158, -0.5334],\n",
       "          [ 0.9631,  0.6546,  0.1408,  ...,  0.3217, -2.6272, -1.7244]],\n",
       "\n",
       "         [[ 0.1073, -0.0800,  2.2955,  ...,  0.2432,  0.0857, -0.1948],\n",
       "          [ 0.9808, -0.6278, -0.4527,  ...,  0.3213,  0.1807, -0.0346],\n",
       "          [ 1.3845, -1.0304, -0.8928,  ...,  0.1578, -0.2128, -0.1027],\n",
       "          [ 0.7095, -1.5666, -0.4112,  ...,  0.0956, -0.4379, -0.4129],\n",
       "          [ 0.6082, -1.7625, -0.4600,  ...,  0.3516,  0.7293, -0.1603],\n",
       "          [ 0.4703, -0.9785, -1.5268,  ...,  0.0105,  0.8630,  0.0979]],\n",
       "\n",
       "         [[-0.1928,  1.0482,  0.4753,  ..., -0.5411,  0.3269, -0.1044],\n",
       "          [-0.1273,  0.0276, -0.0614,  ...,  1.1025,  0.4218, -0.4889],\n",
       "          [-1.3032,  0.1003,  1.0335,  ..., -0.6467, -0.2764,  0.0563],\n",
       "          [-0.4066,  1.1694,  0.2483,  ...,  0.1185, -0.1882, -0.5604],\n",
       "          [-0.0616,  1.2375,  0.9535,  ...,  0.2151,  0.4834, -0.7051],\n",
       "          [ 0.3426,  1.5684, -0.9191,  ...,  0.7581,  0.3132, -0.3588]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5510,  0.9702, -0.8676,  ..., -0.7088,  0.6934,  0.8508],\n",
       "          [-0.1047,  0.5283, -1.5482,  ..., -0.6607, -0.7164,  0.1849],\n",
       "          [ 1.3639,  1.6962,  0.3004,  ..., -0.7982, -0.2042,  0.7423],\n",
       "          [-0.2901,  1.9786,  0.9151,  ...,  0.1559, -0.0602,  0.1783],\n",
       "          [ 0.4083,  2.2105, -1.4128,  ...,  0.6916,  0.7264, -0.7940],\n",
       "          [ 0.0444, -0.7728,  0.0199,  ...,  0.1415, -0.1845, -0.7000]],\n",
       "\n",
       "         [[-0.4068,  0.3957,  0.3603,  ...,  0.6988,  0.0296, -0.0826],\n",
       "          [-1.4076,  1.4544, -1.0819,  ...,  0.3654, -0.2819, -0.1160],\n",
       "          [-0.0828,  1.9985, -1.0212,  ...,  0.4110, -0.3995, -0.9426],\n",
       "          [-0.4889,  1.6810,  0.3998,  ..., -0.3870,  0.0167, -0.7570],\n",
       "          [-0.8669,  1.0761, -0.8223,  ...,  0.1428,  0.0400, -0.6887],\n",
       "          [-1.0468,  1.2639, -1.1295,  ..., -0.6211,  0.4471, -1.1706]],\n",
       "\n",
       "         [[-0.7434, -0.0046,  0.4467,  ..., -0.1008,  0.0175, -0.0680],\n",
       "          [ 0.7038, -0.6238,  0.3741,  ..., -0.4137, -1.0309,  1.0379],\n",
       "          [ 0.4315, -0.1153,  1.1448,  ...,  0.2494, -1.8136,  1.1379],\n",
       "          [-0.4450,  0.3297,  1.1764,  ..., -0.8390, -0.5959,  1.7231],\n",
       "          [-0.7175, -0.0457,  0.0688,  ..., -0.6439,  0.2235,  1.9636],\n",
       "          [ 0.2345, -0.2968,  2.0395,  ...,  0.7276,  0.4434,  0.8050]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1065, -0.1222, -0.1562,  ..., -0.2670,  0.2449, -0.1406],\n",
       "          [ 1.4053,  0.1191,  0.3313,  ...,  3.0563, -1.4882,  1.4536],\n",
       "          [-0.1104, -0.1435,  1.0196,  ..., -0.5375, -0.2656,  1.1927],\n",
       "          [-0.4413,  0.5893,  2.4394,  ...,  1.9857, -1.0965,  0.6146],\n",
       "          [ 0.8952,  0.1504,  1.4734,  ...,  1.4747, -0.4538,  0.7150],\n",
       "          [-1.3370,  0.4981,  0.8157,  ...,  2.1881, -3.0789, -0.0739]],\n",
       "\n",
       "         [[ 0.1237, -0.0431,  0.0194,  ..., -0.0261, -0.0917,  0.1659],\n",
       "          [ 0.0162, -0.0761, -0.3063,  ...,  0.3631,  0.3114, -0.7951],\n",
       "          [-1.6995, -0.8372,  0.2502,  ...,  1.4552,  1.3108, -0.3305],\n",
       "          [-0.5330, -0.1822, -0.8781,  ...,  0.2110,  0.1250,  0.6951],\n",
       "          [ 0.1053, -0.4270, -1.6958,  ..., -0.9060, -0.0839,  0.4039],\n",
       "          [ 1.2649,  0.7761,  0.8251,  ...,  0.1777, -0.2636,  0.0638]],\n",
       "\n",
       "         [[-0.0164,  0.0347, -0.0670,  ...,  0.0290,  0.0150,  0.0685],\n",
       "          [ 1.5384,  1.3917,  0.9124,  ..., -0.3786,  0.0426,  0.4816],\n",
       "          [ 0.8778,  1.2207, -0.1252,  ..., -1.5114, -0.1039,  0.0132],\n",
       "          [ 1.2860, -0.2252, -0.0363,  ..., -0.8516, -0.1274, -1.0064],\n",
       "          [ 0.5278,  0.4644,  1.0936,  ..., -1.1357, -0.1402,  1.6427],\n",
       "          [ 1.2849, -0.2924, -0.6708,  ..., -0.8477,  1.3436,  0.2537]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0089, -0.0111,  0.0988,  ...,  0.0838, -0.0212,  0.0478],\n",
       "          [-0.4070,  0.7785,  0.7239,  ...,  0.9680, -0.0801,  0.2763],\n",
       "          [ 0.1637,  2.3015,  1.5086,  ..., -0.2331, -3.4151,  0.1851],\n",
       "          [ 0.4336, -0.3510,  0.5281,  ...,  0.5033, -0.8239,  1.2575],\n",
       "          [-0.7558, -0.3276, -0.0444,  ...,  0.4986, -0.3727,  0.6526],\n",
       "          [-0.3191, -0.2910,  0.1045,  ...,  0.3256,  1.0175,  0.6764]],\n",
       "\n",
       "         [[-0.1965, -0.0647,  0.0663,  ..., -0.0456,  0.0493, -0.0815],\n",
       "          [-0.8762,  0.2020, -0.3455,  ...,  0.2209,  0.7596, -0.1498],\n",
       "          [-0.2981, -0.3068, -0.9621,  ...,  0.8002, -0.1841, -0.0861],\n",
       "          [-0.3308,  0.7246, -1.6807,  ..., -0.7604, -0.0177, -0.3216],\n",
       "          [ 0.4499,  0.6016, -2.4589,  ..., -0.8163, -0.1960, -0.5063],\n",
       "          [-1.1827, -0.1458,  1.1744,  ..., -0.6851,  0.4782,  0.1486]],\n",
       "\n",
       "         [[ 0.1040, -0.1469,  0.1752,  ..., -0.1528,  0.0050, -0.1816],\n",
       "          [-0.5424, -0.4902,  1.4413,  ..., -0.7550,  0.1687, -0.0194],\n",
       "          [ 1.4206, -1.2850,  1.0580,  ...,  0.3877, -0.1089, -1.4067],\n",
       "          [ 1.0933, -1.4002,  0.8399,  ..., -0.7289, -0.0227,  0.0401],\n",
       "          [ 1.7574, -0.0187,  1.0648,  ..., -1.2873,  0.1817,  0.8371],\n",
       "          [-0.1270, -0.8639, -0.7590,  ...,  0.0093, -0.3957, -0.1514]]]],\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model(**encoded_text_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab9f5df-9f3e-45a3-baf5-0bc9054d7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = bert_model(**encoded_text)\n",
    "gpt_output = gpt_model(**encoded_text_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b86a0e-b21f-4fdc-921a-689574999c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65c9206123645d195e6d8bfa8a303ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46988beaeed49cd8ecfb8f1f2c5ccdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#내가 원하는 모델 찾은 이후에 설정법 \n",
    "model_robert = AutoModel.from_pretrained(\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2325dc1-11c3-42c3-ab87-5bd1647e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#바디와 헤드 설정 \n",
    "#헤드가 포함된 모델 설정\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2f7952-e196-47ad-95d5-8801d3455948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df0d2e606849c19c33df73773ba968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc4b885de9c408aab7eb621a2f07fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e63b73be8448408fa94930a11eff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fa938ddc4e466395ebfaa77351290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2046bad8630443b183748a1b22b0c04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f604e4ed6444f790f2878dc8730fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5419e51a569840de911ac487a1c1a8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'disappointment', 'score': 0.4666953980922699}, {'label': 'sadness', 'score': 0.3984946608543396}, {'label': 'annoyance', 'score': 0.06806597113609314}, {'label': 'neutral', 'score': 0.057030290365219116}, {'label': 'disapproval', 'score': 0.04423936828970909}, {'label': 'nervousness', 'score': 0.014850731007754803}, {'label': 'realization', 'score': 0.014059912413358688}, {'label': 'approval', 'score': 0.011267471127212048}, {'label': 'joy', 'score': 0.006303390488028526}, {'label': 'remorse', 'score': 0.006221487186849117}, {'label': 'caring', 'score': 0.006029403302818537}, {'label': 'embarrassment', 'score': 0.0052654859609901905}, {'label': 'anger', 'score': 0.004981426056474447}, {'label': 'disgust', 'score': 0.004259033594280481}, {'label': 'grief', 'score': 0.004002132453024387}, {'label': 'confusion', 'score': 0.003382918192073703}, {'label': 'relief', 'score': 0.003140497487038374}, {'label': 'desire', 'score': 0.0028274687938392162}, {'label': 'admiration', 'score': 0.0028157976921647787}, {'label': 'fear', 'score': 0.002707518870010972}, {'label': 'optimism', 'score': 0.0026164923328906298}, {'label': 'love', 'score': 0.0024883910082280636}, {'label': 'excitement', 'score': 0.0024494801182299852}, {'label': 'curiosity', 'score': 0.0023743596393615007}, {'label': 'amusement', 'score': 0.0017466946737840772}, {'label': 'surprise', 'score': 0.0014529851032420993}, {'label': 'gratitude', 'score': 0.0006464761681854725}, {'label': 'pride', 'score': 0.0005542500875890255}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "\n",
    "sentences = [\"I am not having a great day\"]\n",
    "\n",
    "model_outputs = classifier(sentences)\n",
    "print(model_outputs[0])\n",
    "# produces a list of dicts for each of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ff33f3-7878-4fec-a651-4f6a155e3bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110,618,112\n"
     ]
    }
   ],
   "source": [
    "#한국어 평가 모델 \n",
    "model_token = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "model = AutoModel.from_pretrained('klue/roberta-base')\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0535754f-5e25-4b08-ab06-4bd148854f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "a = np.array([-0.1, 0.8, 1, 1])\n",
    "b = np.array([0.8, -0.9, 0.9,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ff68cf-474b-458d-96d2-abd4ca4f8b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'토크나이저는 텍스트를 토큰 단위로 나눈다'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token = model_token(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "#ids값을 token 값으로 \n",
    "model_token.convert_ids_to_tokens(text_token['input_ids'])\n",
    "\n",
    "model_token.decode(text_token['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db61381e-cac7-4bfb-b975-e7ebd364377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 5891, 2205, 5971, 2], [0, 3822, 2073, 13262, 12190, 2]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_token(['안녕하세요', '오늘은 목요일입니다'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c930d50b-b238-48e1-a6d0-bf38c9fec692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 3776, 2259, 5382, 8665, 2115, 12190, 2, 1, 1, 1, 1, 1], [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_token([\"여기는 플레이데이터입니다\", '토크나이저는 텍스트를 토큰 단위로 나눈다'], padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fbe6c5c-9170-4383-bd72-a66790570ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752e5fb0c9fe4ac79ca576b87c45b24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23dc238442b4a08a43139023852949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3768a8182e3409789c7d36603b41fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f8d6202781447c96f2e5d67e1b011a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15593372668438b9c34cc82c49e816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fdda0612b94dd1a05f3335e79accf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152748e69d6c44768d353b02ed776f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/847k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a327e95ededa4aa0af8280354bb46d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeeeb8264164cd09ba8f5365b6540b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "klue_mrc = load_dataset('klue', 'mrc')\n",
    "\n",
    "\n",
    "klue_train = klue_mrc = load_dataset('klue', 'ynat', split='train')\n",
    "klue_eval = klue_mrc = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_train.features['label']\n",
    "klue_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b99803a-4f59-4b3e-b377-a3f75e32514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_train = klue_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_eval = klue_eval.remove_columns(['guid', 'url', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "841f7c90-4889-4f05-9f9b-a8251f73dd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT과학'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_train.features['label'].int2str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92431ffe-21d5-4b5b-a25a-88ef7f93489d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'경제'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_train.features['label'].int2str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f68d446-095c-4ffe-b0ab-2426a8270b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = klue_train.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54bb40dc-bb34-42a9-8bde-566d2f4e71e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cae24d6-5b10-4c0a-9c9c-6de929f6cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label(batch):\n",
    "    batch['label_str'] = label.int2str(batch['label'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfb951da-cad1-4946-98a3-41ef3f60a18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df91aeee55b942bc9539ff6c7517e194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "klue_tc_train = klue_train.map(make_label, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ccdebc6-e1da-4e2e-a759-9b484f2c1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06088b44-1731-4868-882e-ea9e4945c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93bf65d5-4992-4e84-b6d9-60105dcaae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = klue_eval.train_test_split(test_size=1000, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52978c63-c51f-43c8-9a76-14a7d3018dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'label'],\n",
       "        num_rows: 8107\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01c851af-00ee-4789-b042-e2f3085d0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#각각 천개씩 데이터 분할하기 \n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ada4289-1a6f-4869-8b9b-7e210616ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습시키기 \n",
    "from transformers import Trainer, TrainingArguments\n",
    "#토큰 변환 \n",
    "def tokenize_func(data):\n",
    "    return model_token(data['title'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2fc0aae-a970-46f4-bf43-723fd03b390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f218ba27f5487b9038b007b2797178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9871b4cf77344f758e7d0355cbbf9103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70215adce6b5424aa394f97ce2962c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_func, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_func, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98baada3-9367-46de-92cb-eb290a79694a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '신동엽 50주기 맞아 산문 전집·앤솔로지 출간',\n",
       " 'label': 3,\n",
       " 'label_str': '생활문화',\n",
       " 'input_ids': [0,\n",
       "  29104,\n",
       "  3956,\n",
       "  2223,\n",
       "  2015,\n",
       "  18556,\n",
       "  16342,\n",
       "  15146,\n",
       "  100,\n",
       "  1391,\n",
       "  2437,\n",
       "  7683,\n",
       "  8189,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "971b26bf-7103-4b95-8a9a-7036fed82d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#헤더있고, 헤더없고 따라서 down-stream tesk를 시키라는 오류가 뜸 \n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d8d54f2-e298-4737-9eb9-69ab94d689fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#해당 모델을 훈련시키기 / 문서 분류 작업 \n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\", \n",
    "    num_train_epochs=1, \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy='epoch',\n",
    "    #학습률 \n",
    "    learning_rate=5e-5,\n",
    "    #모델을 바로 허깅페이스에 적재할 것인지? \n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88b04af3-4771-4617-bbeb-032143d03027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy' : (pred == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fa6dedb-5da4-43d8-a75c-4bcedfe9a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20330/920701915.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer =Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer =Trainer(\n",
    "    model = model, \n",
    "    args = training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=model_token,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3f68c2b-f304-4bf0-9a00-84932535292a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 4 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3660\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3708\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3707\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3710\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1352\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1351\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1352\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1354\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 4 is out of bounds."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a89607-b445-452e-bde4-bd51cd1e15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "model_token = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "klue_train = klue_mrc = load_dataset('klue', 'ynat', split='train')\n",
    "klue_eval = klue_mrc = load_dataset('klue', 'ynat', split='validation')\n",
    "\n",
    "klue_train = klue_train.remove_columns(['guid', 'url', 'date'])\n",
    "\n",
    "klue_eval = klue_eval.remove_columns(['guid', 'url', 'date'])\n",
    "\n",
    "label = klue_train.features['label']\n",
    "\n",
    "def make_label(batch):\n",
    "    batch['label_str'] = label.int2str(batch['label'])\n",
    "    return batch \n",
    "def tokenize_func(data):\n",
    "    return model_token(data['title'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "klue_tc_train = klue_train.map(make_label, batched=True, batch_size=1000)\n",
    "\n",
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']\n",
    "dataset = klue_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_func, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_func, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_func, batched=True)\n",
    "\n",
    "model  = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base', num_labels=len(train_dataset.features['label'].names))\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=model_token,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset) # 정확도 0.84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
