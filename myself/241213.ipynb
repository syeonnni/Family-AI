{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b7e6ce-9705-45ca-9bc9-1c397269a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38224e81-409f-4c1e-a8a2-560e623b029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex=True)\n",
    "train_data = train_data[~(train_data.document == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092881aa-26da-4137-94eb-25115417143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex=True)\n",
    "\n",
    "test_data = test_data[~(test_data.document == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d76696-45de-4341-a5d7-73cc764c7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "komoran.pos(train_data.document[1])\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', \n",
    "             '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '아', '나','네요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72113396-0a2a-4237-84b8-df5d94be54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "\n",
    "for sentence in train_data.document:\n",
    "    #print(sentence)\n",
    "    tokenized_sentence = komoran.morphs(sentence)\n",
    "    X_train.append([word for word in tokenized_sentence if not word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a3b607-f522-4469-af20-6f850e782064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 48852/48852 [00:14<00:00, 3446.66it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = komoran.morphs(sentence) # 토큰화\n",
    "    X_test.append([word for word in tokenized_sentence if not word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82f878f-2e19-4880-ab3e-7c5799a06bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5220a8db-d9a0-4dc2-968b-e61fe0ae0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)\n",
    "np.unique(y_train, return_counts=True)\n",
    "\n",
    "word_list = []\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "\n",
    "word_counts = Counter(word_list)\n",
    "\n",
    "threshold = 3\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "rare_cnt = 0\n",
    "\n",
    "for key, value in word_counts.items():\n",
    "    #총 누적 값 \n",
    "    total_freq += value \n",
    "    if (value < threshold): \n",
    "        rare_cnt += 1\n",
    "        rare_freq += value\n",
    "\n",
    "vocab = sorted(word_counts.items(), key=lambda x : x[1], reverse=True)\n",
    "\n",
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e616ed15-68f5-4d2d-a164-fc06a7d91fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도수가 3개 이하인 것들 / len(vocab) - rare_cnt\n",
    "vocab = vocab[:15955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a206b920-f4e6-47dd-84b3-3fed75df3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 2\n",
    "for word, _ in vocab:\n",
    "    word_to_index[word] = cnt\n",
    "    cnt += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab8e3b7-4c73-42c7-b875-caa458c9f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['야']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e239c177-6ddd-4bd1-b896-2c584da3f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코딩 - 단어에 해당하는 개수 별로 인코딩 실시 \n",
    "def texts_to_sequences(token_data, word_to_index):\n",
    "    encode_data = []\n",
    "    for sent in token_data:\n",
    "        index_seq = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_seq.append(word_to_index[word])\n",
    "            except:\n",
    "                index_seq.append(word_to_index['<UNK>'])\n",
    "        encode_data.append(index_seq)\n",
    "    return encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dca451-deeb-49f9-bbb8-755728c50598",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_X_train = texts_to_sequences(X_train, word_to_index)\n",
    "en_X_val = texts_to_sequences(X_valid, word_to_index)\n",
    "en_X_test = texts_to_sequences(X_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fefbafb-4e3f-4a26-9a89-e4bef87c8c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 1820, 85, 83, 6, 40]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ecdb94-449c-4bdd-b9a3-2b916d3c10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for key, val in word_to_index.items():\n",
    "    index_to_word[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77a04df-9ccb-4d69-86f9-f67d1e06da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for sentence in nested_list:\n",
    "        if sentence <= max_len:\n",
    "            cnt += 1\n",
    "    print(f\"{max_len} 단어가 차지하는 비율 : {cnt / len(nested_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a92e05-d361-46ff-b388-38754b290214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 단어가 차지하는 비율 : 0.7473047096652166\n",
      "20 단어가 차지하는 비율 : 0.8449025912615851\n",
      "25 단어가 차지하는 비율 : 0.8881132107914782\n",
      "30 단어가 차지하는 비율 : 0.9173874168199873\n",
      "35 단어가 차지하는 비율 : 0.9385800505528139\n"
     ]
    }
   ],
   "source": [
    "for x in range(15,40,5):\n",
    "    check(x, [len(x) for x in en_X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e0101f1-4153-466d-b115-394bfdcfd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sentences, max_len):\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "          features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81c97d6a-f434-4da0-902d-2fb698f7db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "test_label_tensor = torch.tensor(np.array(y_test))\n",
    "print(train_label_tensor[:5])\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        # LSTM은 (hidden state, cell state)의 튜플을 반환합니다\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # lstm_out: (batch_size, seq_length, hidden_dim), hidden: (1, batch_size, hidden_dim)\n",
    "\n",
    "        last_hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
    "        logits = self.fc(last_hidden)  # (batch_size, output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64502320-b1b2-4155-b576-e8e243a30d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
